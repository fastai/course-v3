{
"00:01": "welcome to lesson seven the last lesson",
"00:04": "of part one this will be a pretty",
"00:10": "intense lesson and so don't let that",
"00:14": "bother you because partly what I want to",
"00:15": "do is to kind of give you enough things",
"00:19": "to think about to keep you busy until",
"00:21": "platt - and so in fact some of the",
"00:25": "things we cover today I'm not going to",
"00:28": "tell you about some of the details I've",
"00:29": "just point out a few things where I'll",
"00:31": "say like okay that we're not talking",
"00:32": "about yet that not and we're not talking",
"00:33": "about that",
"00:34": "and so then come back in part two to get",
"00:36": "the details on some of these extra extra",
"00:39": "pieces right so well you know today will",
"00:41": "be a lot of material pretty quickly you",
"00:45": "might require a few viewings to fully",
"00:48": "understand at all a few experiments and",
"00:50": "so forth and that's kind of intentional",
"00:52": "and trying to give you stuff to to keep",
"00:54": "you amused for a couple of months wanted",
"00:59": "to start by showing some core work done",
"01:02": "by a couple of students Reshma and in",
"01:05": "patters Eero one who have developed an",
"01:08": "Android and an iOS app and so check out",
"01:12": "Reshma's post on the forum about that",
"01:16": "because they have a demonstration of how",
"01:18": "to create a both Android and iOS apps",
"01:21": "that are actually on the Play Store and",
"01:22": "on the Apple App Store so that's pretty",
"01:26": "cool first first ones I know of that are",
"01:28": "on the App Store's that are using first",
"01:29": "AI and let me also say a huge thank you",
"01:33": "to Rushmore for all of the work she does",
"01:35": "both for the fast AI community and the",
"01:38": "machine learning community or generally",
"01:40": "and also the women in machine learning",
"01:41": "community in particular she does a lot",
"01:44": "of fantastic work including providing",
"01:47": "lots of fantastic documentation and",
"01:49": "tutorials and community organizing and",
"01:51": "so many other things so thank you rush",
"01:54": "me and congrats on getting this app out",
"01:56": "there we have lots of less than",
"02:05": "seven-minute bucks today as you see and",
"02:08": "we're going to start with the one",
"02:13": "so the first notebook we're going to",
"02:14": "look at is lesson seven ResNet amnesty",
"02:17": "and what I want to do is look at some of",
"02:21": "the stuff we started talking about last",
"02:22": "week around convolutions and",
"02:24": "convolutional neural networks and start",
"02:26": "building on top of them to create a",
"02:28": "fairly modern deep learning architecture",
"02:31": "largely from scratch when I say from",
"02:34": "scratch I'm not going to re-implement",
"02:35": "things we already know how to implement",
"02:37": "but kind of use the pre-existing ply",
"02:40": "torch bits of those so we're going to",
"02:43": "use the EM list data set which so URLs",
"02:47": "that amnesty has the whole emne status",
"02:49": "set often we've done stuff with a subset",
"02:51": "of it",
"02:52": "so in there there's a training folder",
"02:54": "and a testing folder and as I read this",
"02:58": "in I'm going to show some more details",
"03:00": "about pieces of the data blocks API so",
"03:02": "that you see how to kind of see what's",
"03:04": "going on normally with the date betablox",
"03:06": "API we've kind of said bla bla bla bla",
"03:09": "and done it all in one cell but let's do",
"03:10": "them one cell at a time so first thing",
"03:14": "you say is what kind of item list do you",
"03:16": "have so in this case it's an item list",
"03:18": "of images and then where are you getting",
"03:21": "the list of file names from in this case",
"03:23": "by looking in a folder recursively and",
"03:26": "that's where it's coming from you can",
"03:29": "pass in arguments that end up going to",
"03:31": "pillow because pillow or PIL is the",
"03:33": "thing that actually opens that for us",
"03:34": "and in this case these are black and",
"03:37": "white rather than RGB so you have to use",
"03:40": "pillows convert mode equals L for more",
"03:42": "details refer to the Python imaging",
"03:45": "library documentation to see what",
"03:47": "they're convert modes are but this one",
"03:49": "is going to be a grayscale which is what",
"03:53": "M lists is so inside an item list is an",
"03:57": "item's attribute and the items attribute",
"04:00": "is kind of the thing that you gave it",
"04:02": "it's the thing that it's going to use to",
"04:04": "create your items in this case the thing",
"04:06": "you gave it really is a list of file",
"04:07": "names that's what it got from the folder",
"04:11": "okay when you show images normally it",
"04:15": "shows them in RGB and so in this case we",
"04:18": "want to use a binary color map so in",
"04:20": "first AI you can set a default color map",
"04:22": "for more information about C map and",
"04:24": "color maps",
"04:25": "to the matplotlib documentation and so",
"04:28": "this will set the default color map for",
"04:30": "a faster I okay so our image item list",
"04:34": "contains 70,000 items and it's a bunch",
"04:36": "of images that are 1 by 28 by 28",
"04:40": "remember that pipe torch puts channel",
"04:41": "first so there one channel 28 for 28 you",
"04:45": "might think we're way out there just 28",
"04:46": "by 28 matrices rather than a 1 by 28 by",
"04:50": "28 rank 3 tensor it's just easier that",
"04:54": "way all the comp 2d stuff and so forth",
"04:57": "works on rank 3 tensors so you want to",
"05:00": "you want to include that unit axis at",
"05:03": "the start and so first day I will do",
"05:05": "that for you even when it's reading one",
"05:07": "channel images so the dot items",
"05:12": "attribute contains the things that's",
"05:14": "kind of red to build the image which in",
"05:16": "this case is the file name but if you",
"05:19": "just index into an item list directly",
"05:20": "you'll get the actual image object okay",
"05:23": "and so the actual image object has a",
"05:24": "show method and so there's there's the",
"05:26": "image so once you've got an image item",
"05:29": "list you then split it into training",
"05:31": "versus validation you nearly always want",
"05:34": "validation if you don't you can actually",
"05:36": "use the dot nose split method to create",
"05:39": "a kind of empty validation set you can't",
"05:43": "skip it entirely you have to say how to",
"05:45": "split and one of the options is no split",
"05:47": "right and so remember that's always the",
"05:49": "order first create your item list then",
"05:52": "decide how to split in this case we're",
"05:54": "gonna do it based on folders in this",
"05:58": "case the the validation folder for EM",
"06:01": "list is called testing so in kind of",
"06:04": "fast AI parlance we use the same kind of",
"06:07": "parlance that kaggle does which is the",
"06:09": "training set is what you train on the",
"06:11": "validation set has labels and you do it",
"06:14": "for testing that your models working the",
"06:16": "test set doesn't have labels and you use",
"06:20": "it for doing inference or submitting to",
"06:22": "a competition or sending it off to",
"06:24": "somebody who's held out those labels for",
"06:26": "you know event or testing or whatever",
"06:28": "okay so just because a folder in your",
"06:31": "data set is called testing doesn't mean",
"06:33": "it's a test set right this one has",
"06:35": "labels so it's a validation set",
"06:38": "okay so if you want to do inference on",
"06:40": "lots you know lots of things at a time",
"06:42": "rather than one thing at a time you want",
"06:44": "to use the test equals in in fast AI to",
"06:49": "say this is stuff which has no labels",
"06:51": "and I'm just using for inference okay so",
"06:55": "my split data is a training set and a",
"06:59": "validation set as you can see so inside",
"07:03": "the training set there's a folder for",
"07:06": "each image for each class so now we can",
"07:09": "take that split data and say label from",
"07:13": "folder so first you create the Animus",
"07:15": "then you spit it then you label it and",
"07:18": "so you can see now we have an X and a Y",
"07:21": "and the Y are category objects category",
"07:26": "object is just a class basically so if",
"07:30": "you index into a label list such as lol",
"07:35": "trained as a label list you will get",
"07:37": "back an independent variable independent",
"07:40": "variable X & Y so this case the X will",
"07:43": "be an image object which I can show and",
"07:46": "the Y will be a category object which I",
"07:48": "can read that's the number it's the",
"07:52": "number 8 category and there's the 8 next",
"07:56": "thing we can do is to add transforms in",
"07:59": "this case we're not going to use the",
"08:01": "normal get transforms function because",
"08:04": "we're doing digit recognition and digit",
"08:07": "recognition like you wouldn't want to",
"08:08": "flip it left right",
"08:09": "that would change the meaning of it you",
"08:11": "wouldn't want to rotate it too much that",
"08:13": "would change the meaning of it also",
"08:14": "because these images are so small kind",
"08:16": "of doing zooms and stuff is going to",
"08:18": "make them so fuzzy used to be unreadable",
"08:19": "so normally for small images of digits",
"08:23": "like this you just add a bit of random",
"08:25": "padding so I'll use the random padding",
"08:27": "function which actually returns two",
"08:30": "transforms so a bit that does the",
"08:32": "padding and the D that does the random",
"08:34": "crop so you have to use star to say put",
"08:36": "both these transforms in this list so",
"08:39": "now we can call transform this empty",
"08:42": "array here is referring to the",
"08:43": "validation set transforms so no",
"08:45": "transforms for the validation set now",
"08:49": "we've got a transformed",
"08:52": "list we can pick a batch size and choose",
"08:55": "data bunch we can choose normalize in",
"08:59": "this case we're not using a pre-trained",
"09:00": "model so there's no reason to use",
"09:03": "imagenet stats here and so if you call",
"09:06": "normalize like this without passing in",
"09:10": "stats it will all grab a batch of data",
"09:13": "at random and use that to decide what",
"09:16": "normalization stats to use that's a good",
"09:18": "idea",
"09:19": "if you're not using a pre-trained model",
"09:23": "okay so we've got a dead data bunch and",
"09:25": "so in that data bunch is a data set",
"09:29": "which we've seen already but what is",
"09:34": "interesting is that the training data",
"09:35": "set now has data augmentation because",
"09:37": "you've got transforms so plot multi is a",
"09:40": "faster Oh function that we're all plot",
"09:42": "the result of calling some function for",
"09:45": "each of this row by column grid so in",
"09:48": "this case my function is just grab and",
"09:50": "grab the first image from the training",
"09:51": "set and because each time you grab",
"09:54": "something from the training set it's",
"09:55": "going to load it from disk and it's",
"09:57": "going to transform it on the fly right",
"10:01": "so people sometimes ask like how many",
"10:04": "transformed versions of the image do you",
"10:06": "create and the answer is kind of",
"10:08": "infinite each time we grab one thing",
"10:10": "from the data set we do a random",
"10:12": "transform on the fly",
"10:14": "okay so potentially you everyone will",
"10:16": "look a little bit different so you can",
"10:19": "see here if we plot the result of that",
"10:21": "lots of times we get",
"10:22": "eights in slightly different positions",
"10:25": "because we did random padding you can",
"10:28": "always grab a batch of data then from",
"10:31": "the data bunch because remember a data",
"10:33": "bunch has data loaders and data loaders",
"10:36": "things that you grab a batch at a time",
"10:38": "and so you can then grab our X patch and",
"10:41": "a Y batch look at their shape batch size",
"10:44": "by channel by row by column all fast a a",
"10:48": "data bunches have a show batch which",
"10:50": "will show you what's in it in some",
"10:54": "sensible way okay so that's a quick walk",
"10:58": "through with the data block API stuff to",
"10:59": "grab our data so let's start out",
"11:02": "creating a simple CNN",
"11:06": "simple confident so the input is 28 by",
"11:10": "28 so let's define I like to define when",
"11:16": "I'm creating architectures a function",
"11:17": "which kind of does the things that I do",
"11:19": "again and again and again I don't want",
"11:21": "to call it with the same arguments",
"11:22": "because I'll forget I'll make a mistake",
"11:23": "so in this case all of my convolutions",
"11:26": "are going to be kernel size 3 stride 2",
"11:29": "padding 1 so let's just create a simple",
"11:32": "function to do a cons with those",
"11:34": "parameters so you try to have a",
"11:36": "convolution it's skipping over one pixel",
"11:41": "so it's doing jumping jumping two steps",
"11:43": "each time so that means that each time",
"11:46": "we have a convolution it's going to have",
"11:48": "the grid size so I've put a comment here",
"11:51": "showing what the new grid size is after",
"11:53": "each one so after the first convolution",
"11:56": "we have one channel coming in because",
"11:58": "it's remember it's a grayscale image",
"12:00": "with one channel and then how many",
"12:02": "channels coming out whatever you like",
"12:05": "right so remember you always get to pick",
"12:07": "how many filters you create regardless",
"12:11": "of whether it's a fully connected layer",
"12:13": "in which case it's just the the width of",
"12:15": "the matrix you're multiplying by or in",
"12:17": "this case with a 2d cons it's just how",
"12:20": "many how many filters do you want so I",
"12:23": "picked 8 and so after this it's dried 2",
"12:26": "so the 28 by 28 image is now a 14 by 14",
"12:30": "feature map with 8 channels so",
"12:33": "specifically therefore it's an 8 by 14",
"12:35": "by 14 tensor of activations then we'll",
"12:41": "do batch norm then would devalue so the",
"12:43": "number of input filters to the next con",
"12:46": "has to equal the number of output",
"12:47": "filters from the previous conf and we",
"12:50": "can just keep increasing the number of",
"12:51": "channels because we're doing stride to",
"12:54": "it's going to keep decreasing the grid",
"12:56": "size notice here it goes from 7 to 4",
"12:59": "because if you're doing a stride to",
"13:02": "convey over 7 it's going to be kind of",
"13:05": "math dot ceiling of 7/2 patch norm rail",
"13:12": "you confer now down to 2 by 2 personal",
"13:15": "really akan we're now down to 1 by 1",
"13:17": "right so after this",
"13:20": "we have a batch side of the picture map",
"13:25": "of let's say ten by one by one does that",
"13:34": "make sense we've got a grid size of one",
"13:35": "now so it's not a vector of length 10",
"13:39": "its a rank 3 tensor of 10 by 1 by 1 so",
"13:46": "our loss functions expect generally a",
"13:49": "vector not a rank 3 tensor so you can",
"13:52": "check flatten at the end and flatten",
"13:54": "just means remove any unit axes so that",
"13:59": "will make it now just a vector of length",
"14:02": "10 which is what we always expect so",
"14:06": "that's how we can create a CNN so then",
"14:10": "we can return that into a learner by",
"14:11": "passing in the data and the model and",
"14:14": "the loss function and if optionally some",
"14:17": "metrics so we're going to use",
"14:19": "cross-entropy as usual so we can then",
"14:22": "call own dot summary and confirm after",
"14:24": "that first cond we're down to 14 by 14",
"14:27": "and after the second column 7 by 7 and 4",
"14:31": "by 4 2 by 2 1 by 1 the flatten comes out",
"14:37": "calling it a lambda but that as you can",
"14:39": "see it gets rid of the 1 by 1 then it's",
"14:41": "now just a length 10 vector for each",
"14:45": "item in the bench so 128 by 10 matrix of",
"14:48": "the whole mini batch so just to confirm",
"14:52": "that this is working ok we can grab that",
"14:55": "mini batch of X that we created earlier",
"14:58": "there's so many vetch of X pop it onto",
"15:02": "the GPU and call the model directly",
"15:04": "remember any PI torch module we can",
"15:07": "pretend as a function and that gives us",
"15:10": "back as we hoped a 128 by 10 resolved ok",
"15:14": "so that's how you can directly get some",
"15:16": "predictions out now find fit one cycle",
"15:21": "and bang we already have a 98.6%",
"15:24": "accurate confident and this is trained",
"15:29": "from scratch of course it's not",
"15:31": "pre-trained we literally",
"15:32": "created our own architecture about the",
"15:34": "simplest possible architecture you can",
"15:35": "imagine 18 seconds to train so that's",
"15:37": "how easy it is to create a pretty",
"15:39": "accurate digit detector",
"15:42": "so let's refactor that a little rather",
"15:46": "than saying clowns metronome really all",
"15:49": "the time",
"15:51": "first day I already has something called",
"15:52": "con underscore lair which lets you",
"15:55": "create cons batch normal you",
"15:58": "accommodations and it has various other",
"16:00": "options to do other tweaks to it but the",
"16:03": "basic version is just exactly what I",
"16:05": "just showed you so we can refactor that",
"16:07": "like so so that's exactly the same",
"16:10": "euronet and so you know let's just try",
"16:15": "it a little bit longer and it's actually",
"16:17": "99.1 percent accurate if we train it for",
"16:20": "all over minute so that's cool so how",
"16:26": "can we improve this well what we really",
"16:28": "want to do is create a deeper Network",
"16:33": "and it's a very easy way to create a",
"16:35": "deeper Network would be after every",
"16:38": "stride to cons add a stride one cons",
"16:41": "because the straight one comes doesn't",
"16:43": "change the feature map size at all so",
"16:46": "you can add as many as you like right",
"16:48": "but there's a problem there's a problem",
"16:53": "and the problem was pointed out in this",
"16:55": "paper very very very influential paper",
"16:58": "called deep learning deep residual",
"17:00": "learning for image recognition by coming",
"17:02": "her and colleagues at then at Microsoft",
"17:05": "Research and they did something",
"17:07": "interesting they said let's look at the",
"17:08": "training error",
"17:09": "so forget generalization even let's just",
"17:11": "look at the training error of a network",
"17:14": "train on so far 10 and let's try one",
"17:18": "network of 20 layers just basic 3x3",
"17:21": "funds it's just basically the same",
"17:23": "network I just showed you but without",
"17:26": "batch norm let's try train to 20 layer",
"17:30": "one and a 56 layer one on the training",
"17:33": "set so the 56 layer one has a lot more",
"17:36": "parameters it's got a lot more of these",
"17:37": "trade one comes in the middle so the one",
"17:40": "with more parameters should seriously",
"17:42": "over fit right so you would expect",
"17:46": "the 56 layer one to zip down to zero ish",
"17:49": "training error pretty quickly and that",
"17:51": "is not what happens it is worse than the",
"17:54": "shallower network so when you see",
"17:56": "something weird happen really good",
"17:59": "researchers don't go oh no it's not",
"18:01": "working",
"18:02": "they go that's interesting so cutting",
"18:05": "her said that's interesting what's going",
"18:09": "on and he said I don't know but what I",
"18:14": "do know is this I could take this 56",
"18:18": "layer Network and make a new version of",
"18:22": "it which is identical but has to be at",
"18:25": "least as good as the 20 layer network",
"18:27": "and here's how every two convolutions",
"18:31": "I'm going to add together the input to",
"18:37": "those two convolutions add it together",
"18:39": "with the result of those two",
"18:42": "convolutions so in other words he's",
"18:46": "saying instead of saying output equals",
"18:51": "con two of cons one of X instead he's",
"18:59": "saying output equals x plus con of two",
"19:04": "of cons one of s so that fifty six",
"19:14": "layers worth of convolutions in in that",
"19:17": "his theory was has to be at least as",
"19:20": "good as the twenty layer version because",
"19:23": "it could always just set com2 and cons",
"19:27": "one to a bunch of zero waits for",
"19:29": "everything except for the first 20",
"19:31": "layers because because they're X the",
"19:34": "input could just go straight through so",
"19:38": "this thing here is as you see called an",
"19:41": "identity connection it's the identity",
"19:45": "function nothing happens at all it's",
"19:47": "also known as a skip connection so that",
"19:50": "was a theory right that's what the paper",
"19:51": "describes as the intuition behind this",
"19:54": "is what would happen if we created",
"19:57": "something",
"19:58": "which has to train at least as well as a",
"20:01": "20 layer neural network because it kind",
"20:03": "of contains that 28 layer neural network",
"20:05": "is literally a path you can just skip",
"20:07": "over all the convolutions and so what",
"20:11": "happens and what happened was he won",
"20:16": "imagenet that year he easily won",
"20:18": "imagenet that year and in fact you know",
"20:21": "even today you know we had that",
"20:25": "record-breaking result on image net",
"20:28": "speed training ourselves you know in the",
"20:29": "last year we used this to you know res",
"20:33": "net has been revolutionary and anytime",
"20:38": "here's a trick if you're interested in",
"20:40": "doing some research in the whole",
"20:41": "research anytime you find some model for",
"20:46": "anything with recite medical image",
"20:47": "segmentation or you know some kind of",
"20:51": "gain or whatever you know and it was",
"20:54": "written a couple of years ago they might",
"20:58": "have forgotten to put rest nets in res",
"21:00": "block res blocks this is what we",
"21:02": "normally call a res block they might",
"21:04": "have forgotten what res blocks in so",
"21:06": "replace their convolutional path with a",
"21:09": "bunch of res blocks and you'll almost",
"21:11": "always get better results faster it's a",
"21:14": "good trick",
"21:16": "so at Europe's which Rachel and I and",
"21:19": "David or just came back from and Sylvia",
"21:22": "we saw a new presentation where they",
"21:27": "actually figured out how to visualize",
"21:29": "the loss surface of a neural net which",
"21:33": "is really cool this is a fantastic paper",
"21:35": "and anybody who's watching this lesson 7",
"21:39": "is at a point where they will understand",
"21:41": "most of the most important concepts in",
"21:43": "this paper you could read this now you",
"21:45": "won't necessarily get all of it but I'm",
"21:47": "sure you'll find it again enough to find",
"21:49": "an interesting and so the the big",
"21:52": "picture was this one here's what happens",
"21:54": "if you if you draw a picture we're kind",
"21:57": "of X&Y",
"21:58": "here are two projections of the of the",
"22:01": "white space and Z is the loss and so as",
"22:05": "you move through the white space rest at",
"22:07": "a 56 layer neural network without skip",
"22:10": "connections is very very bumpy",
"22:12": "and that's why this got nowhere because",
"22:17": "it just got stuck in all these hills and",
"22:19": "bellies the exact same network with",
"22:23": "identity connections with skip",
"22:25": "connections has this lost landscape",
"22:28": "right so that's it's kind of interesting",
"22:31": "how how how her recognized back in 2015",
"22:36": "you know this shouldn't happen here's a",
"22:39": "way that must fix it and it took three",
"22:41": "years before people were able to say oh",
"22:44": "this is kind of why it fixed it it kind",
"22:47": "of reminds me of the batch normal",
"22:49": "discussion we had a couple of weeks ago",
"22:50": "people realizing a little bit after the",
"22:53": "fact sometimes what's what's going on",
"22:55": "and why it helps so in our code we can",
"23:06": "create a res block in just the way I",
"23:09": "described we create a tenant module we",
"23:12": "create two con Flair's remember a con",
"23:14": "flair is kind of 2d that's normal you so",
"23:21": "I come to derail your veteran or so",
"23:23": "creator of those and then in forward we",
"23:25": "go conf one of ex-cons two of that and",
"23:29": "then add X there's a res Bach function",
"23:34": "already in fast AI so you can just call",
"23:37": "res block instead and you just pass in",
"23:41": "something saying how many filters do you",
"23:44": "want so yeah so there's the wrist block",
"23:47": "that I defined in a notebook and so with",
"23:51": "that look with that res block we can now",
"23:53": "take every one of those I've just copied",
"23:56": "the previous CNN and after every con to",
"24:00": "except the last one I added a res block",
"24:02": "so this is great now got three times as",
"24:05": "many layers so it should be able to do",
"24:07": "more compute right but it shouldn't be",
"24:09": "any harder to optimize so what happens",
"24:13": "well let's just refactor it one more",
"24:16": "time since I go come to res block so",
"24:18": "many times let's just pop that into a",
"24:20": "little mini sequential model here and so",
"24:24": "I can refactor that like so",
"24:25": "like keep refactoring your architectures",
"24:28": "if you're trying novel architectures",
"24:29": "because you'll make less mistakes very",
"24:31": "few people do this most research codes",
"24:33": "you look at is is clunky as all hell and",
"24:37": "people often make mistakes in that way",
"24:39": "so don't don't do that be you know",
"24:40": "you're all coders so use your coding",
"24:43": "skills to make life easier",
"24:47": "okay so there's my ResNet ish",
"24:51": "architecture and ela find as usual fit",
"24:57": "for a while and I get ninety nine point",
"25:02": "five four so that's interesting because",
"25:07": "we've trained this literally from",
"25:09": "scratch with an architecture we built",
"25:11": "from scratch I didn't look at this",
"25:13": "architecture anywhere it's just the",
"25:14": "first thing that came to mind but in",
"25:17": "terms of where that puts us point four",
"25:19": "five percent error is around about the",
"25:23": "state of the art for this data set as of",
"25:26": "three or four years ago now you know",
"25:30": "today M just is considered a kind of",
"25:31": "trivially easy data set so I'm not",
"25:34": "saying like well we've broken some",
"25:36": "records here people have got beyond",
"25:38": "0.45% error but what I'm saying is that",
"25:41": "you know we can't you know this kind of",
"25:45": "resonate is a genuinely extremely useful",
"25:50": "network still today and this is this is",
"25:53": "really all we use in our first image net",
"25:56": "training still and one of the reasons as",
"25:58": "well is that it's so popular so the the",
"26:01": "vendors of the library spend a lot of",
"26:03": "time optimizing it so things tend to",
"26:05": "work fast where are some more modern",
"26:09": "style architectures using things like",
"26:12": "separable or group convolutions tend not",
"26:14": "to actually train very quickly in",
"26:16": "practice if you look at the definition",
"26:21": "of res block in the FASTA code you'll",
"26:24": "see it looks a little bit different to",
"26:25": "this and that's because i've created",
"26:28": "something called a merge layer and a",
"26:30": "merge layer is something which in the",
"26:33": "forward just keeps dense for a moment",
"26:35": "the forward says X plus X",
"26:39": "a ridge so you can see that some coming",
"26:42": "ResNet ish going on here what is X dot",
"26:44": "orig well if you create a special kind",
"26:47": "of sequential model called a sequential",
"26:49": "e^x so this is like the fastes",
"26:52": "sequential extended it's just like a",
"26:54": "normal sequential model but we store the",
"26:57": "input in X dot orig right and so this",
"27:02": "this here is the quench really ex-con",
"27:05": "flour corn flour merge layer will do",
"27:09": "exactly the same that's this okay so you",
"27:12": "can create your own variations of ResNet",
"27:15": "blocks very easily with just sequential",
"27:18": "ax and merge layer so there's something",
"27:22": "else here which is when you create your",
"27:24": "merge layer you can optionally set dense",
"27:26": "equals true what happens if you do well",
"27:29": "if you do it doesn't go X plus X dot",
"27:32": "image that goes cat X comma X dot orig",
"27:35": "in other words rather than putting a",
"27:39": "plus in this connection it does a",
"27:41": "concatenate so that's pretty interesting",
"27:45": "because what happens is that you have",
"27:47": "your your input coming into your res",
"27:53": "block and once you use concatenate",
"27:55": "instead of plus it's not called a res",
"27:57": "block anymore it's called a dense block",
"27:58": "and it's not quite a ResNet any what",
"28:01": "more is called a dense net so the dense",
"28:03": "net was invented about a year after the",
"28:06": "ResNet and if you read the dense net",
"28:08": "paper it can sound incredibly complex",
"28:10": "and different but actually it's",
"28:11": "literally identical but plus here is",
"28:14": "replaced with with cat so you have your",
"28:17": "input coming into your dense block right",
"28:19": "and you've got a kind of few",
"28:21": "convolutions in here and then you've got",
"28:23": "some output coming out and then you've",
"28:26": "got your identity connection and",
"28:28": "remember it doesn't plus it con cats so",
"28:31": "this is the channel access it gets a",
"28:33": "little bit bigger all right and then so",
"28:36": "we do another dense block right and at",
"28:38": "the end of that we have you know all of",
"28:42": "this coming in drop sorry we have okay",
"28:47": "so at the end of that we have you know",
"28:49": "the result of the convolution as per",
"28:50": "usual but this time",
"28:52": "identity bloc is that big right so you",
"28:57": "can see that what happens is that with",
"28:59": "dense blocks it's getting bigger and",
"29:01": "bigger and bigger and kind of",
"29:03": "interestingly the exact input is still",
"29:07": "here right so that actually no matter",
"29:10": "how deep you get the original input",
"29:12": "pixels are still there and the original",
"29:14": "layer 1 features are still there in the",
"29:15": "original layer of two features are still",
"29:17": "there so as you can imagine dense Nets",
"29:21": "are very memory intensive there are ways",
"29:24": "to manage this the best from time to",
"29:26": "time you can have a regular convolution",
"29:28": "that squishes your channels back down",
"29:30": "but they are memory intensive but they",
"29:33": "have very few parameters so for dealing",
"29:38": "with small datasets",
"29:39": "you should definitely experiment with",
"29:41": "dense blocks and dense Nets they tend to",
"29:45": "work really well on small datasets also",
"29:49": "because it's possible to kind of keep",
"29:51": "those original input pixels all the way",
"29:53": "down the path they work really well for",
"29:55": "segmentation right because for",
"29:57": "segmentation you know you kind of want",
"29:59": "to be able to reconstruct the original",
"30:02": "resolution of your picture so having all",
"30:04": "of those original pixels still there is",
"30:06": "super helpful so so that's that's",
"30:18": "residents and the main one of the main",
"30:20": "reasons other than fact that rez nets",
"30:21": "are awesome to tell you about them is",
"30:23": "that these skipped connections are",
"30:25": "useful in other places as well and there",
"30:28": "it's particularly useful in other places",
"30:30": "and other ways of designing",
"30:31": "architectures for segmentation so in",
"30:35": "building this lesson I always kind of I",
"30:38": "keep trying to take old papers and",
"30:41": "saying like imagining like what would",
"30:44": "that person have done if they had access",
"30:46": "to all the modern techniques we have now",
"30:48": "and I try to kind of rebuild them in a",
"30:50": "more modern style so I've been really",
"30:52": "rebuilding this next architecture going",
"30:54": "to look at called a unit in a more",
"30:56": "modern style recently and got to the",
"30:59": "point now I keep showing you this",
"31:02": "semantic segmentation paper with the",
"31:06": "state of the art for camford which was",
"31:09": "91.5 this week I got it up to 94.1 using",
"31:15": "the architecture I'm about to show you",
"31:17": "so we just we keep pushing this further",
"31:19": "and further and further and it's really",
"31:21": "was all about you know adding all of the",
"31:26": "modern tricks many of which I'll show",
"31:30": "you today some of which we will see in",
"31:32": "part two so what we're going to do to",
"31:37": "get there is we're going to use this",
"31:38": "unit so we've used a unit before I've",
"31:42": "improved it a bit since then so we've",
"31:45": "used a unit before we used it when we",
"31:47": "did the camp fit segmentation but we",
"31:48": "didn't understand what it was doing so",
"31:51": "we're now in a position where we can",
"31:52": "understand what I was doing and so the",
"31:59": "first thing we need to do is kind of",
"32:00": "understand the basic idea of how you can",
"32:03": "do segmentation so if we go back to our",
"32:09": "canvas notebook in our camping notebook",
"32:15": "you'll remember that basically what we",
"32:17": "were doing is we were taking these",
"32:18": "photos and adding a class to every",
"32:22": "single pixel and somebody go data touch",
"32:24": "show batch for something which is a",
"32:26": "segmentation item list it will",
"32:30": "automatically show you these color-coded",
"32:33": "pixels so here's the thing like in order",
"32:39": "to color code this as a pedestrian you",
"32:43": "know but this as a bicyclist it needs to",
"32:47": "know what it is there needs to actually",
"32:49": "know that's what a pedestrian looks like",
"32:51": "and it needs to know that's exactly",
"32:52": "where the pedestrian is and this is the",
"32:53": "arm of the pedestrian and not part of",
"32:55": "their shopping basket it needs to really",
"32:57": "understand a lot about this picture to",
"33:00": "do this task and it really does through",
"33:02": "this task like when you looked at the",
"33:04": "results of our top model it's it's you",
"33:09": "know I can't see a single pixel by",
"33:12": "looking at it by eye I know there's a",
"33:14": "few wrong but I can't see",
"33:15": "ones that are wrong is that accurate so",
"33:18": "how does it do that so the way that",
"33:20": "we're doing it to get these really",
"33:22": "really good results is not surprisingly",
"33:26": "using pre-training so we start with a",
"33:30": "ResNet 34 and you can see that here unit",
"33:36": "learner data comma model start ResNet 34",
"33:40": "and if you don't say pre-trained equals",
"33:42": "false by default you get pre-trained",
"33:44": "equals true because why not so we start",
"33:49": "with a resonate 34 which starts with a",
"33:55": "big image so in this case this is from",
"33:58": "the unit paper now they're images they",
"34:01": "started with one channel by 572 by 572",
"34:04": "this is for medical imaging segmentation",
"34:07": "so after your stride to cons you and",
"34:12": "they're doubling the number of channels",
"34:13": "to 128 and they're having the size so",
"34:16": "they're now down to 280 by 280 in this",
"34:19": "original unit paper they didn't add any",
"34:22": "padding so they lost the pixel on each",
"34:24": "side each time they did a con that's why",
"34:26": "you're losing these two but so basically",
"34:28": "half the size and then half the size and",
"34:31": "then half the size and then half the",
"34:33": "size until they're down to 28 by 28 with",
"34:37": "1024 channels right so that's that's",
"34:40": "what the unit's down the sampling path",
"34:45": "this is called the down sampling path",
"34:46": "look like ours is just a ResNet 34 so",
"34:50": "you can see it here learn dot summary",
"34:54": "right this is literally a resinate 34 so",
"35:01": "you can see that the size keeps having",
"35:03": "channels keep going up and so forth",
"35:07": "okay so eventually you've got down to a",
"35:10": "point where if you use a unit",
"35:12": "architecture it's 28 by 28 with 1,024",
"35:15": "channels with the resonant architecture",
"35:17": "with a 224 pixel input it would be 512",
"35:22": "channels by 7 by 7 so it's a pretty",
"35:25": "small grid size on this feature map",
"35:28": "somehow we've got to end up with",
"35:31": "something which is the same size as our",
"35:34": "original picture so how do we do that",
"35:38": "how do you do computation which",
"35:42": "increases the grid size well we don't we",
"35:46": "don't have a way to do that in our",
"35:47": "current bag of tricks",
"35:48": "we can use a stride one conf to do",
"35:51": "computation and keeps grid size or a",
"35:53": "stride to con to do computation and half",
"35:57": "the grid size so how do we double the",
"35:59": "grid size we do a stride 1/2 conf also",
"36:04": "known as a deconvolution also known as a",
"36:08": "transposed convolution there is a",
"36:11": "fantastic paper called a guide to",
"36:14": "convolution arithmetic fatigue learning",
"36:15": "that shows a great picture of exactly",
"36:18": "what does a 3x3 kernel stride 1/2 cons",
"36:21": "look like and it's literally this if you",
"36:24": "have a 2x2 input so the blue squares are",
"36:27": "the 2x2 input you add not only 2 pixels",
"36:32": "of padding all around the outside but",
"36:35": "you also add a pixel of padding between",
"36:39": "every pixel and so now if we put this",
"36:44": "3x3 kernel here and then here and then",
"36:48": "here usually other 3x3 kernels just",
"36:50": "moving across it in the usual way you",
"36:52": "will end up going from a 2x2 output to a",
"36:56": "5x5 output so if you only added one",
"36:59": "pixel of padding around the outside you",
"37:02": "would add up end up with a 3x3 output",
"37:05": "right so that's very 4x4 so this is how",
"37:11": "you can increase the resolution this was",
"37:19": "the way people did it until maybe a year",
"37:24": "or two ago that's another trick for",
"37:28": "improving things you find online because",
"37:30": "this is actually a dumb way to do it and",
"37:33": "it's kind of obvious it's a dumb way to",
"37:34": "do it for a couple of reasons one is",
"37:35": "that like hello look at this nearly all",
"37:38": "of those pixels are white they're nearly",
"37:40": "all zeros",
"37:42": "so like what a waste what a waste of",
"37:44": "time what a waste of computation",
"37:46": "there's just nothing going on there I'm",
"37:48": "also this one when you get down to like",
"37:54": "that 3x3 area two out of the nine pixels",
"37:58": "are non-white but this one one out of",
"38:02": "the nine at lone white so they're kind",
"38:03": "of like there's different amounts of",
"38:05": "information going into different parts",
"38:07": "of your convolution so like this it just",
"38:10": "doesn't make any sense to kind of throw",
"38:13": "away information like this and they're",
"38:15": "going to do all this unnecessary",
"38:15": "computation and have different parts of",
"38:17": "the convolution having access to",
"38:19": "different amounts of information so what",
"38:23": "people generally do nowadays is",
"38:26": "something really simple which is if you",
"38:28": "have a let's say a two by two input with",
"38:32": "these are your pixel values a a B C and",
"38:36": "D and you want to create a four by four",
"38:44": "why not just do this a a a a b b b b CC",
"38:51": "CC d D D D so I've now upscaled from two",
"38:57": "by two to four by four I haven't done",
"38:59": "any interesting computation but now on",
"39:01": "top of that I could just do a Strad one",
"39:05": "convolution and now I have done some",
"39:08": "computation right so an up sample this",
"39:11": "is called nearest neighbor interpolation",
"39:13": "nearest neighbor interpolation so you",
"39:20": "can just do that's super fast just",
"39:22": "that's taken to a nearest neighbor",
"39:24": "interpolation and then a stride one conf",
"39:27": "and now you've got some computation",
"39:29": "which is actually kind of using you know",
"39:32": "there's no zeros here this is kind of",
"39:34": "nice because it gets a mixture of A's",
"39:36": "and B's which is kind of what you would",
"39:37": "want and so forth another approach is",
"39:41": "instead of using nearest neighbor",
"39:42": "interpolation you can use bilinear",
"39:44": "interpolation which basically means",
"39:47": "instead of copying a to all those",
"39:49": "different cells you take a kind of a",
"39:51": "weighted average",
"39:51": "if the cells around it so for exam",
"39:54": "if you were you know looking at what",
"39:58": "should go here he would kind of go like",
"40:00": "oh it's about 3/8 2 C's 1d and 2 B's and",
"40:06": "you could have taken the average not",
"40:07": "exactly but roughly just a weighted",
"40:09": "average by linear interpolation you'll",
"40:11": "find in any you know all over the place",
"40:13": "it's is pretty standard technique",
"40:15": "anytime you look at a picture on your",
"40:18": "computer screen and change its size it's",
"40:20": "doing bilinear interpolation so you can",
"40:22": "do that and then a strike one conf so",
"40:26": "that was what people were using worst",
"40:28": "what people still tend to use that's as",
"40:31": "much as are going to teach you this part",
"40:33": "in part two will actually learn what the",
"40:36": "first day I library is actually doing",
"40:38": "behind the scenes which is something",
"40:40": "called a pixel shuffle also known as sub",
"40:43": "pixel convolutions it's got not",
"40:45": "dramatically more complex but complex",
"40:47": "enough that I won't cover it today",
"40:48": "there's a same basic idea all of these",
"40:50": "things is something which is basically",
"40:52": "letting us do a convolution that ends up",
"40:55": "with something that's twice the size and",
"40:57": "so that gives us our up sampling path",
"41:01": "right so that lets us go from 28 by 28",
"41:05": "to 54 by 54 and keep on doubling the",
"41:10": "size so that's good and that was that",
"41:16": "was it until unit came along that's what",
"41:19": "people did and it didn't work real well",
"41:21": "which is not surprising because like in",
"41:24": "this 28 by 28 feature map how the hell",
"41:28": "is it going to have enough information",
"41:29": "to reconstruct a 572 by 572 output space",
"41:34": "you know that's a really tough ask so",
"41:37": "you tended to end up with these things",
"41:39": "that lack fine detail so what dollar for",
"41:48": "on a burger and a towel did was they",
"41:52": "said hey let's add a skip connection an",
"41:55": "identity connection and amazingly enough",
"41:59": "this was before res Nets existed so this",
"42:03": "was like a really big leap really",
"42:06": "impressive",
"42:07": "and so but rather than adding a skip",
"42:09": "connection that skipped every two",
"42:12": "convolutions they added skip connections",
"42:15": "where these gray lines are in other",
"42:17": "words they added a skip connection from",
"42:19": "the same part of the downsampling path",
"42:21": "to the same-sized bit in the up sampling",
"42:25": "path and they didn't add that's why you",
"42:28": "can see the white and the blue next to",
"42:30": "each other they didn't add they",
"42:31": "concatenated so basically these are like",
"42:34": "dense blocks right but the Skip",
"42:37": "connections are skipping over larger and",
"42:39": "larger amounts of the architecture so",
"42:43": "that over here you've literally got",
"42:48": "nearly the input pixels themselves",
"42:51": "coming into the computation of these",
"42:54": "last couple of layers and so that's",
"42:56": "going to make it super handy through",
"42:58": "resolving the fine details in these",
"43:00": "segmentation tasks because you've",
"43:01": "literally got all of the fine details on",
"43:04": "the downside you don't have very many",
"43:07": "layers of computation going on here just",
"43:09": "for right so you better hope that by",
"43:12": "that stage you've done all the",
"43:14": "computation necessary to figure out is",
"43:16": "this the bicyclist or is this a",
"43:17": "pedestrian but you can then add on top",
"43:20": "of that something saying like is this",
"43:21": "you know is this exact pixel where their",
"43:23": "nose finishes or is that the start of",
"43:25": "the tree so that works out really well",
"43:30": "and that's a unit so this is the unit",
"43:35": "code from fast AI and the key thing that",
"43:40": "comes in is the encoder the encoder",
"43:44": "refers to that part in other words in",
"43:52": "our case a ResNet 34 in most cases they",
"43:57": "have this specific older style",
"44:00": "architecture but like I said replace any",
"44:02": "older style architecture bits where the",
"44:04": "ResNet bits and life improves",
"44:06": "particularly if they're pre trained so",
"44:08": "that certainly happened for us so we",
"44:10": "start with our encoder so our layers of",
"44:11": "our unit is an encoder then batch norm",
"44:14": "then rally and then middle con which is",
"44:18": "just con flare comma con flare member",
"44:21": "Khan",
"44:21": "is a conf rally veteran on in faster go",
"44:25": "and so that middle con is these two",
"44:30": "extra steps here at the bottom okay just",
"44:32": "doing a little bit of computation you",
"44:34": "know it's kind of nice to add more",
"44:36": "layers of computation where you can so",
"44:39": "encode a batch or Lu and then to",
"44:40": "convolutions and then we enumerate",
"44:43": "through these indexes what are these",
"44:46": "indexes I haven't included the code but",
"44:48": "these are basically we figure out what",
"44:50": "is the layer number where each of these",
"44:54": "stripe to comes occurs and we just store",
"44:57": "it in an array of indexes so then we can",
"45:00": "loop through that and we can basically",
"45:02": "say for each one of those points create",
"45:04": "a unit block telling us how many",
"45:08": "upsampling channels that are and how",
"45:09": "many cross connection these these things",
"45:12": "here are called cross connections at",
"45:14": "least that's what I call them so that's",
"45:17": "really the main works going on in the in",
"45:20": "the unit block as I said there's quite a",
"45:23": "few tweaks we do as well as the fact we",
"45:25": "use a much better encoder we also use",
"45:27": "some tweaks in all of our app sampling",
"45:29": "using this pixel shuffle we use another",
"45:31": "tweak called ICN our and then and then",
"45:34": "another tweak which I just did in the",
"45:35": "last week is to not just take the result",
"45:38": "of the convolutions and pass it across",
"45:40": "but we actually grab the input pixels",
"45:42": "and make them another cross connection",
"45:45": "that's what this last cross is here you",
"45:48": "can see we're literally appending a res",
"45:50": "block with the original inputs so you",
"45:53": "can see I merge layer so really all the",
"45:57": "works going on a new net block and unit",
"46:00": "block is it has to store the the",
"46:04": "activations at each of these",
"46:06": "downsampling points and the way to do",
"46:09": "that as we learn in the last lesson is",
"46:11": "with hooks so we we put hooks into the",
"46:15": "resinate 34 to store the activations",
"46:18": "each time there's a strata to cons and",
"46:21": "so that's you can see here we we grab",
"46:23": "the hook okay and we grab the result of",
"46:27": "the stored value in that hook and we",
"46:30": "literally just go torch doc hat so we",
"46:32": "concatenate",
"46:35": "the upsampled convolution with the",
"46:42": "result of the hook which we Chuck",
"46:44": "through batch norm and then we do two",
"46:46": "convolutions to it and actually you know",
"46:49": "something you could play with at home is",
"46:52": "pretty obvious here anytime you see two",
"46:54": "convolutions like this there's an",
"46:55": "obvious question is what if we used a",
"46:57": "resident block instead so you could try",
"46:59": "replacing those two comes with a",
"47:01": "resinate block you might find you get",
"47:03": "even better results they're the kind of",
"47:05": "things I look for when I look at an",
"47:07": "architecture is like oh two columns in a",
"47:09": "row probably should be a ResNet block",
"47:14": "okay so that's that's unit and you know",
"47:22": "it's amazing to think you know it",
"47:24": "preceded ResNet to preceded dense net",
"47:27": "it's been it wasn't even published in a",
"47:30": "major machine learning venue it was",
"47:33": "actually published in Mekhi which is a",
"47:35": "specialized medical image computing",
"47:37": "conference for years actually you know",
"47:41": "it was largely unknown outside of the",
"47:43": "medical imaging community and actually",
"47:45": "what happened was tackle competitions",
"47:48": "for segmentation kept on being easily",
"47:50": "won by people using units and that was",
"47:52": "the first time I saw it getting noticed",
"47:54": "outside the medical imaging community",
"47:56": "and then gradually a few people in the",
"47:57": "academic machine learning community",
"47:59": "started noticing and now everybody loves",
"48:02": "unit which I'm glad because it's just",
"48:05": "it's just awesome so yeah so identity",
"48:12": "connections regardless of whether",
"48:14": "they're a plus style or a concat style",
"48:17": "we're incredibly useful they can",
"48:20": "basically get us close to the state of",
"48:22": "the art on lots of important tasks so I",
"48:27": "want to use them on another task now and",
"48:30": "so the next task I want to look at is",
"48:33": "image restoration so image restoration",
"48:36": "refers to starting with an image at this",
"48:40": "time we're not going to create a",
"48:41": "segmentation mask but we're going to try",
"48:43": "and create a a better image",
"48:47": "and there's lots of kind of versions of",
"48:48": "better there could be different image so",
"48:50": "the kind of things we can do with this",
"48:52": "kind of image generation would be take a",
"48:54": "low res image make it high res take a",
"48:57": "black-and-white image make a color take",
"48:59": "an image where something's being cut out",
"49:02": "of it and trying to replace the cutout",
"49:04": "thing take a photo and try and turn it",
"49:07": "into what looks like a line drawing take",
"49:09": "a photo and try and make it look like a",
"49:10": "Monet painting these are all examples of",
"49:13": "kind of image to image generation tasks",
"49:15": "which you all know how to do after this",
"49:17": "part of class so in our case we're going",
"49:22": "to try to do image restoration which is",
"49:26": "going to start with low resolution poor",
"49:29": "quality JPEGs with writing written over",
"49:33": "the top of them and get them to replace",
"49:36": "them with high resolution good quality",
"49:38": "pictures in which the the text has been",
"49:41": "removed two questions okay let's go",
"49:50": "why do you compat before calling comp to",
"49:54": "comp one not after because if you did",
"50:01": "kind of one Conte you know if you did",
"50:03": "your combs before you concat then",
"50:05": "there's no way for the channels of the",
"50:08": "two parts to interact with each other",
"50:10": "you don't get any you know so remember",
"50:13": "in a 2d conf it's really 3d right it's",
"50:18": "moving across two dimensions but in each",
"50:21": "case it's doing a dot product of all",
"50:25": "three dimensions of a rank three tensor",
"50:27": "row by column by Channel so generally",
"50:32": "speaking we want as much interaction as",
"50:34": "possible we want to say you know this",
"50:37": "part of the down sampling path and this",
"50:39": "part of the up sampling path if you look",
"50:40": "at the combination of them you find",
"50:42": "these interesting things so generally",
"50:44": "you know you you want to have as many",
"50:47": "interactions going on as possible in",
"50:50": "each computation that you do",
"50:55": "is concatenating every layer together in",
"50:57": "a dense Network when the size of the",
"50:59": "image feature Maps is changing through",
"51:01": "the layers that's a great question so if",
"51:08": "you have a stride to cons you can't keep",
"51:11": "dense knitting right so that's what",
"51:15": "actually happens in a dense net is you",
"51:16": "kind of go like dense block growing",
"51:18": "dense block growing dense block growing",
"51:20": "so you getting more and more channels",
"51:21": "and then you do a stride to cons without",
"51:25": "a dense block and so now it's kind of",
"51:29": "gone and then you just do a few more",
"51:30": "dense blocks and then it's gone so in",
"51:33": "practice a dense block doesn't actually",
"51:36": "keep all the information all the way",
"51:37": "through but just every up into every one",
"51:40": "of these stride to comes and there's",
"51:45": "kind of various ways of doing these",
"51:46": "bottlenecking layers where you're",
"51:48": "basically saying hey let's let's reset",
"51:51": "it also helps us keep memory under",
"51:53": "control because at that point we can",
"51:54": "decide how many channels we actually",
"51:56": "want good questions thank you back so in",
"52:04": "order to create something which can turn",
"52:06": "crappy images into nice images we need a",
"52:10": "data set containing nice versions of",
"52:12": "images and crappy versions of the same",
"52:14": "images so the easiest way to do that is",
"52:16": "to start with some nice images and crap",
"52:19": "fi them and so the way to crap fi them",
"52:21": "is to create a function called crap fi",
"52:23": "which contains your krappa fication",
"52:25": "logic so Mike ratification logic you can",
"52:30": "pick your own is that I open up my nice",
"52:32": "image I resize it to be really small 96",
"52:35": "by 96 pixels with bilinear interpolation",
"52:40": "I then pick a random number between 10",
"52:43": "and 70 I draw that number into my image",
"52:48": "at some random location and then I save",
"52:52": "that image with a JPEG quality of that",
"52:55": "random number and a JPEG quality of 10",
"52:58": "is like absolute rubbish a JPEG quality",
"53:02": "of 70 is not bad at all okay so I end up",
"53:07": "where",
"53:09": "if high quality images low quality",
"53:11": "images that look something like these",
"53:14": "and so you can see this one you know",
"53:17": "there's the image and this is after",
"53:19": "transformation so that's why it's been",
"53:20": "flipped and you won't always see the",
"53:23": "image because we're zooming into them so",
"53:26": "a lot of the time the image is cropped",
"53:28": "out",
"53:29": "so yeah it's trying to figure out how to",
"53:31": "take this incredibly jpg artifactory",
"53:33": "thing with with text written over the",
"53:35": "top and turn it into into this so I'm",
"53:38": "using the Oxford pets data set again the",
"53:40": "same one we used in lesson 1 so there's",
"53:43": "nothing more high qualities and pictures",
"53:45": "of dogs and cats I think we can all",
"53:46": "agree with that",
"53:48": "the krappa fication process can take a",
"53:51": "while but fast AI has a function called",
"53:54": "parallel and if you pass parallel a",
"53:57": "function name and a list of things to",
"53:59": "run that function on it will run that",
"54:02": "function on them all in parallel so this",
"54:05": "actually can run pretty quickly",
"54:11": "the way you write this function is where",
"54:14": "you get to do all the interesting stuff",
"54:15": "in this assignment try and think of an",
"54:19": "interesting krappa fication which does",
"54:21": "something that you want to do right so",
"54:23": "if you want to you know colorize",
"54:25": "black-and-white images you would replace",
"54:27": "it with black-and-white if you want",
"54:29": "something which can you know take like",
"54:31": "large cutout blocks of image and replace",
"54:34": "them with kind of hallucinatin image you",
"54:37": "know add a big black box to these if you",
"54:40": "want something which can kind of take",
"54:42": "old families photos scans that have been",
"54:44": "like folded up and have crinkles in try",
"54:46": "and find a way of like adding dust",
"54:48": "prints and crinkles and so forth right",
"54:50": "and anything that you don't include in",
"54:54": "crap fi your model won't learn to fix",
"54:57": "because every time it sees that in your",
"55:00": "photos the input and output will be the",
"55:01": "same so it won't consider that to be",
"55:03": "something worthy of fixing okay",
"55:06": "so so we now want to create a model",
"55:10": "which can take an input photo that looks",
"55:15": "like that and outputs something that",
"55:18": "looks like that so obviously what we",
"55:20": "want to do is",
"55:21": "is a unit because we already know that",
"55:23": "units can do exactly that kind of thing",
"55:25": "and we just need to pass the unit that",
"55:28": "data okay so our data is just literally",
"55:32": "the file names of each of those from",
"55:35": "each of those two folders do some",
"55:37": "transforms data bunch normalize or use",
"55:41": "imagenet stats because we're going to",
"55:43": "use a pre trained model why are we using",
"55:45": "a pre trained model well because like if",
"55:47": "you're going to get rid of this 46 you",
"55:50": "need to know what probably was there and",
"55:52": "to know what probably was there you need",
"55:54": "to know what this is a picture of back",
"55:56": "because otherwise how can you possibly",
"55:57": "know what it ought to look like so you",
"55:59": "know let's use a pre trained model that",
"56:01": "knows about these kinds of things so we",
"56:04": "create our unit with that data the",
"56:07": "architecture is ResNet 34 these three",
"56:13": "things are important and interesting and",
"56:15": "useful but I'm going to leave them to",
"56:17": "part two okay for now you should always",
"56:19": "include them when you use a unit for",
"56:22": "this kind of problem and so now we're",
"56:27": "going to add this whole thing I'm",
"56:29": "calling a generator okay it's going to",
"56:30": "generate this is clarity of modeling",
"56:32": "they're kind of there's not a really",
"56:34": "formal definition but it's basically",
"56:36": "something where the thing we're",
"56:37": "outputting is like a real object in this",
"56:40": "case an image it's not just a number so",
"56:44": "we're going to create a generator",
"56:46": "learner which is this unit learner and",
"56:49": "then we can fit we're using MSC loss",
"56:52": "right so in other words what's the mean",
"56:54": "squared error between the actual pixel",
"56:56": "value that it should be in the pixel",
"56:58": "value that we predicted MSE lost",
"57:00": "normally expects two vectors in our case",
"57:04": "we have two images so we have a version",
"57:07": "called MSC loss flat which simply",
"57:09": "flattens out those images into a big",
"57:11": "long vector there's there's never any",
"57:14": "reason not to use this even if you do",
"57:16": "have a vector it works fine if you don't",
"57:17": "have a work vector it'll also work fine",
"57:19": "so we're already you know down to 0.05",
"57:22": "mean squared error on the pixel values",
"57:25": "which is not bad after one minute 35",
"57:28": "like all things in fast day I pretty",
"57:31": "much because we're doing transfer",
"57:33": "learning by default when you create the",
"57:35": "it'll freeze the the pre-trained part",
"57:39": "and the pre-trained part of a unit is",
"57:42": "this putt the downsampling part that's",
"57:46": "where the resident is so let's unfreeze",
"57:48": "that and train a little more and look at",
"57:54": "that so with you know three minutes of",
"57:59": "four minutes of training we've got",
"58:01": "something which is basically doing a",
"58:03": "perfect job of removing numbers it's",
"58:07": "certainly not doing a good job of up",
"58:09": "sampling but it's definitely doing a",
"58:14": "nice you know sometimes when it removes",
"58:15": "a number it maybe leaves a little bit of",
"58:17": "JPEG artifact but you're certainly doing",
"58:20": "something pretty useful and so if all we",
"58:21": "wanted to do was kind of watermark",
"58:24": "removal would be finished we're not",
"58:28": "finished because we actually want this",
"58:31": "thing to look more like this thing so",
"58:35": "how we got to do that the problem the",
"58:39": "reason that we're not making as much",
"58:41": "progress with that as we'd like is that",
"58:43": "our loss function doesn't really",
"58:45": "describe what we want because actually",
"58:47": "the the mean squared error between the",
"58:50": "pixels of this and this is actually very",
"58:53": "small right if you actually think about",
"58:55": "it most of the pixels are very nearly",
"58:58": "the right color but we're missing the",
"59:00": "texture of the pillow and we're missing",
"59:02": "the eyeballs entirely pretty much right",
"59:05": "and we're missing the texture of the fur",
"59:07": "right so we want we want some loss",
"59:11": "function that does a better job than",
"59:14": "pixel mean squared error loss of saying",
"59:17": "like is this a good quality picture of",
"59:20": "this thing so there's a fairly general",
"59:25": "way of answering that question and it's",
"59:30": "something called a generative",
"59:32": "adversarial Network or can and again",
"59:37": "tries to solve this problem by using a",
"59:40": "loss function which actually calls",
"59:43": "another model and let me describe it to",
"59:47": "you",
"59:51": "so we've got our crappy image right and",
"59:55": "we've already created a generator it's",
"59:57": "not a great one but it's not terrible",
"59:58": "right and that's creating predictions",
"60:01": "like like this we have a high-res image",
"60:09": "like that and we can compare the",
"60:12": "high-res image to the prediction with",
"60:15": "with pixel MSE okay",
"60:19": "we could also train another model which",
"60:22": "we would variably call variously call",
"60:25": "either the discriminator or The Critic",
"60:27": "they both mean the same thing I'll call",
"60:29": "it a critic we could try and build a",
"60:31": "binary classification model that takes",
"60:35": "all the pairs of the generated image and",
"60:38": "the real high-res image and tries to",
"60:41": "classify learn to classify which is",
"60:44": "which you know so look at some picture",
"60:47": "and say like hey what do you think is",
"60:50": "that a high-res cat or is that a",
"60:51": "generated cat how about this one is that",
"60:53": "a high-res cat or a generated cat so",
"60:55": "just a regular standard binary",
"60:58": "cross-entropy e classified so we know",
"61:01": "how to do that already so if we had one",
"61:05": "of those we could now train we could",
"61:08": "fine tune the generator and rather than",
"61:12": "using pixel MSE is the loss the loss",
"61:15": "could be how good are we at fooling the",
"61:18": "critic so can we create generated images",
"61:23": "that the critic thinks are real so that",
"61:28": "would be a very good plan right because",
"61:30": "if it can do that if if the loss",
"61:32": "function is am I fooling the critic that",
"61:35": "then it's going to learn to create",
"61:37": "images which the critic can't tell",
"61:40": "whether they're real or fake so we could",
"61:44": "do that for a while train a few batches",
"61:48": "but the critic isn't that great the",
"61:52": "reason the critic is that isn't that",
"61:53": "great is because it wasn't that hard",
"61:54": "like these images are really shitty so",
"61:56": "it's really easy to tell the difference",
"61:57": "alright so after we train the generator",
"62:00": "a little bit more using the",
"62:02": "critic as the loss function the",
"62:05": "generators going to get really good",
"62:06": "they're falling the critic so now we're",
"62:10": "going to stop training the generator and",
"62:11": "we'll drain the critic some more on",
"62:13": "these newly generated images so now that",
"62:17": "the generators better it's now a tougher",
"62:19": "task for the critic to the side which is",
"62:21": "real and which is fake so again so we're",
"62:23": "trained that a little bit more and then",
"62:26": "once we've done that and the critics now",
"62:27": "pretty good at recognizing the",
"62:28": "difference between the better generated",
"62:30": "images and the originals well we'll go",
"62:33": "back and we'll fine-tune the generator",
"62:35": "some more using the better discriminator",
"62:37": "the better critic as the loss function",
"62:39": "and so we'll just go ping pong ping pong",
"62:41": "backwards and forwards that's again well",
"62:47": "that's our version of again I don't know",
"62:49": "if anybody's written this before we've",
"62:52": "we've created a new version of again",
"62:54": "which is kind of a lot like the original",
"62:56": "Gans but we have this this neat trick",
"62:59": "where we pre train the generator and we",
"63:01": "pre train the critic",
"63:02": "I mean games have been kind of in the",
"63:05": "news a lot they're pretty fashionable",
"63:08": "tall and if you've seen them you may",
"63:10": "have heard that they're a real pain to",
"63:12": "Train but it turns out we realized that",
"63:16": "really most of the pain of training them",
"63:18": "was at the start if you don't have a pre",
"63:20": "trained generator and you don't have a",
"63:22": "pre trained critic then it's basically",
"63:25": "the blind leading the blind right you're",
"63:27": "basically like the critics well the",
"63:29": "generators trying to generate something",
"63:31": "which falls a critic but the critic",
"63:32": "doesn't know anything at all so it's",
"63:33": "basically got nothing to do and then the",
"63:35": "critics kind of try to decide whether",
"63:37": "the generated images are real or not and",
"63:38": "that gets really obvious so it does does",
"63:40": "it and so they kind of like don't go",
"63:43": "anywhere for ages and then once they",
"63:46": "finally start picking up steam they go",
"63:49": "along pretty quickly so if you can find",
"63:52": "a way to generate things without using",
"63:55": "again like means quit there are pixel",
"63:57": "loss and discriminate things without",
"63:59": "using a can like predict on that first",
"64:02": "generator you can make a lot of progress",
"64:04": "so let's create the critic so to create",
"64:08": "just a totally standard fast AI binary",
"64:12": "classification model we need two folders",
"64:15": "one folders contain",
"64:16": "high-res images one folder containing",
"64:18": "generated images we already have the",
"64:21": "folder with high-res images so we just",
"64:23": "have to save our generated images so",
"64:26": "here's a teeny tiny bit of code that",
"64:28": "does that we're going to create a",
"64:30": "directory called image gen pop it into a",
"64:34": "variable called path gen we're good a",
"64:37": "little function called save Preds that",
"64:39": "takes a data loader and we're going to",
"64:42": "grab all of the file names because",
"64:43": "remember that in an item list the dot",
"64:45": "items contains the file names if it's an",
"64:48": "image item list so here's the final file",
"64:51": "names in that data loaders data set and",
"64:54": "so now let's go through each batch of",
"64:56": "the data loader and let's grab a batch",
"65:00": "of predictions for that batch and then",
"65:04": "reconstruct akil's true means it's",
"65:05": "actually going to create fast AI image",
"65:07": "objects for each of those each thing in",
"65:10": "the in the batch and so then we'll go",
"65:12": "through each of those predictions and",
"65:14": "save them and the name will save it with",
"65:16": "is the name of the original file but",
"65:20": "we're going to pop it into our new",
"65:21": "directory so that's it that's how you",
"65:27": "save predictions and so you can see I'm",
"65:29": "kind of increasingly not just using",
"65:32": "stuff that's already in the first day I",
"65:33": "library but try to show you how to write",
"65:35": "stuff yourself right and generally",
"65:38": "doesn't require heaps of code to do that",
"65:41": "and so if you come back for part two",
"65:43": "this is what you know platen matza part",
"65:46": "two were kind of like here's how you use",
"65:48": "things inside the library and of course",
"65:50": "here's how we wrote the library so",
"65:52": "increasingly writing our own code okay",
"65:57": "so save those predictions and lend this",
"65:59": "just to a PIL dot image to open on the",
"66:02": "first one and yep there it is okay so",
"66:05": "there's an example of a generated image",
"66:07": "so now I can train a critic in the usual",
"66:11": "way it's really annoying to have to",
"66:14": "restart TripIt a notebook to refresh",
"66:16": "your reclaim GPU memory so one easy way",
"66:19": "to handle this is if you just set",
"66:21": "something that you knew was using a lot",
"66:22": "of GPU to none like this learner and",
"66:25": "then just go GC collect that tells",
"66:28": "Python to do",
"66:30": "a memory garbage collection and after",
"66:33": "that you'll generally be fine you'll be",
"66:37": "able to use all of your GPU remember",
"66:39": "again if you're using nvidia SMI to",
"66:41": "actually look at your GPU memory you",
"66:44": "won't see it clear because plate watch",
"66:46": "still has a kind of allocated cache but",
"66:50": "it makes it available so you should find",
"66:52": "this is how you can avoid restarting",
"66:54": "your notebook okay",
"66:56": "so we're going to create a critic it's",
"66:57": "just an image item list from folder in",
"67:00": "the totally usual way and the classes",
"67:03": "will be the image gen and images will do",
"67:08": "a random split because we want to know",
"67:09": "how well we're doing with a critic to",
"67:11": "have a validation set",
"67:12": "we just label it from folder in the",
"67:14": "usual way",
"67:15": "that's and transforms data bunch",
"67:17": "normalized so it's a totally standard",
"67:19": "object classifier okay so we've got a",
"67:26": "totally standard classifier so here's",
"67:30": "what some of it looks like so here's one",
"67:32": "from the real images real images",
"67:34": "generated images generated images okay",
"67:37": "so that's it's going to try and figure",
"67:39": "out which class is which okay so we're",
"67:45": "going to use Brian airy cross HB as",
"67:48": "usual however we're not going to use a",
"67:55": "res net here and the reason we'll get",
"68:00": "into in more detail in part two but",
"68:02": "basically when you're doing again you",
"68:07": "need to be particularly careful that the",
"68:11": "the generator and the critic can't kind",
"68:14": "of both push in the same direction and",
"68:16": "like increase the weights out of control",
"68:19": "so we have to use them in called",
"68:20": "spectral normalization to make Gans work",
"68:24": "nowadays well learn about that in part",
"68:26": "two so anyway if you say Gann critic",
"68:29": "that will give you first a a will give",
"68:31": "you a binary classifier suitable begins",
"68:34": "I strongly suspect we probably can use a",
"68:37": "resonate here we just have to create a",
"68:38": "pre trained ResNet with spectral norm",
"68:40": "hope to do that pretty soon",
"68:42": "we'll see how we go but as of now this",
"68:45": "is kind of the best approach there's",
"68:48": "this thing called game critic again",
"68:52": "critic uses a slightly different way of",
"68:57": "averaging the the different parts of the",
"69:01": "image when it does the loss so anytime",
"69:03": "you're doing again at the moment you",
"69:05": "have to wrap your loss function with",
"69:06": "adaptive loss again we'll look at the",
"69:09": "details in part two for now just know",
"69:11": "this is what you have to do and it'll",
"69:12": "work so other than that slightly odd",
"69:16": "loss function and that slightly odd",
"69:18": "architecture everything else is the same",
"69:19": "we can call that to create our critic",
"69:23": "because we have this slightly different",
"69:25": "architecture and slightly different loss",
"69:27": "function we did a slightly different",
"69:28": "metric this is the equivalent gain",
"69:31": "version of accuracy the critics and then",
"69:34": "we can train it and you can see it's 98%",
"69:37": "accurate at recognizing that kind of",
"69:42": "crappy thing from that kind of nice",
"69:43": "thing but of course we don't see the",
"69:45": "numbers here anymore right because these",
"69:47": "are the generated images that generate",
"69:48": "already knows how to get rid of those",
"69:50": "numbers that are written on top okay so",
"69:55": "let's finish up this game now that we",
"69:58": "have pre trained the generator and",
"70:01": "pre-trained the critic we now need to",
"70:04": "get it to kind of ping pong between",
"70:06": "training a little bit of each and the",
"70:09": "amount of time you spend on each of",
"70:11": "those things and the learning rates you",
"70:13": "use is still a little bit on the fuzzy",
"70:16": "side so we've created a again learner",
"70:21": "for you which you just pass in your",
"70:24": "generator and your critic which we've",
"70:27": "just just simply loaded here from the",
"70:29": "ones we just trained and it will go",
"70:33": "ahead and when you go learned up fit it",
"70:36": "will do that for you it'll figure out",
"70:38": "how much time to train the generator and",
"70:39": "then when to switch to training the",
"70:40": "discriminator the critic it'll go back",
"70:42": "on and forward these weights here is",
"70:46": "that what we actually do is we don't",
"70:49": "only use the critic as the loss function",
"70:51": "if we only use the critic as a loss",
"70:53": "function the game could get very good at",
"70:56": "creating",
"70:57": "pictures that look like real pictures",
"71:00": "but they actually have nothing to do",
"71:02": "with the original picture the original",
"71:04": "photo at all so we actually add together",
"71:06": "the pixel loss and the critic loss and",
"71:10": "so those two losses are kind of on",
"71:14": "different scales so we multiply the",
"71:16": "pixel loss by something between about 50",
"71:19": "and about 300 again something in that",
"71:22": "range generally works pretty well",
"71:26": "something else with cans cans hate",
"71:30": "momentum when you're training them it",
"71:33": "kind of doesn't make sense to train them",
"71:34": "with her mentum because you keep",
"71:35": "switching between generator and critic",
"71:37": "so it's kind of tough maybe there are",
"71:39": "ways to use momentum but I'm not sure",
"71:40": "anybody's figured it out so this number",
"71:43": "here when you create an atom optimizer",
"71:45": "is where the momentum goes so you should",
"71:47": "set that to zero so anyway if you're",
"71:49": "doing games use these hyper parameters",
"71:51": "it should work okay so so that's what",
"72:00": "can learner does and so then you can go",
"72:02": "fit and it trains for a while and one of",
"72:05": "the tough things about ganz is that",
"72:08": "these loss numbers they're meaningless",
"72:13": "you can't expect them to go down right",
"72:16": "because as the generator gets better it",
"72:19": "gets harder for the discriminator the",
"72:21": "critic and then as the credit gets",
"72:23": "better it's harder for the generator so",
"72:25": "the numbers should stay about the same",
"72:28": "okay so that's one of the tough things",
"72:32": "about training ganz is it's kind of hard",
"72:34": "to know how are they doing so the only",
"72:36": "way to know how are they doing is to",
"72:39": "actually take a look at the results from",
"72:41": "time to time I haven't and so if you put",
"72:44": "show image equals true here it'll",
"72:47": "actually print out a sample after every",
"72:49": "epoch I haven't put that in the notebook",
"72:51": "because it makes it too big for the repo",
"72:53": "but you can try that so I've just put",
"72:56": "the results at the bottom and here it is",
"73:00": "so pretty beautiful I would say we",
"73:05": "already knew how to get rid of the",
"73:07": "numbers but we're now don't really have",
"73:09": "that kind of",
"73:10": "back to where it used to be and it's",
"73:12": "it's definitely sharpening up this",
"73:15": "little kitty cat quite nicely it's not",
"73:22": "great always like there's some weird",
"73:24": "kind of noise going on here certainly a",
"73:30": "lot better than the horrible original",
"73:32": "like this is a tough job to turn that",
"73:34": "into that but there are some really",
"73:38": "obvious problems like here these things",
"73:41": "ought to be eyeballs and they're not so",
"73:46": "why aren't they well our critic doesn't",
"73:50": "know anything about eyeballs and even if",
"73:52": "it did it wouldn't know that eyeballs",
"73:55": "are particularly important you know we",
"73:56": "care about eyes like when we see a cat",
"73:58": "with that eyes it's a lot less cute I",
"74:05": "mean I'm more of a dog person but you",
"74:07": "know it's it just doesn't know that this",
"74:14": "is a feature that that matters",
"74:17": "particularly because the critic remember",
"74:19": "is not a pre trained network so I kind",
"74:22": "of suspect that if we replace the critic",
"74:24": "with a pre trained network that's been",
"74:26": "pre trained on imagenet but is also",
"74:27": "compatible with gans it might do a",
"74:29": "better job here but it's definitely a",
"74:33": "shortcoming of this approach so I'm",
"74:36": "going to have a break Oh question first",
"74:39": "and then we'll have a break and then",
"74:42": "after the break I will show you how to",
"74:44": "find the cat's",
"74:45": "eyeballs again for what kind of problems",
"74:49": "do you not want to use units",
"74:55": "well unit so for when the the size of",
"75:00": "your output you know is is similar to",
"75:04": "the size of your input and kind of",
"75:06": "aligned with it",
"75:08": "there's no point kind of having cross",
"75:10": "connections if that level of spatial",
"75:12": "resolution in the output isn't necessary",
"75:15": "or useful so yeah any kind of generative",
"75:19": "modeling and you know segmentation is",
"75:21": "kind of generative modeling right it's",
"75:23": "it's Jen",
"75:23": "rating a picture which is a mask of the",
"75:26": "original objects yeah so probably",
"75:30": "anything where you want that kind of",
"75:31": "that kind of resolution of the output to",
"75:35": "be of the same kind of fidelity as a",
"75:37": "resolution of the input obviously",
"75:39": "something like a classifier makes no",
"75:41": "sense right you you're in a classifier",
"75:44": "you just want the down sampling path",
"75:46": "because at the end you just want a",
"75:48": "single number which is like is it a dog",
"75:49": "or a cat or what kind of pet is it or",
"75:52": "whatever great okay so let's get back",
"75:56": "together at five past eight just before",
"76:00": "we leave Dan's I just mentioned there's",
"76:01": "another notebook you might be interested",
"76:03": "in looking at which is less than 7w",
"76:07": "again when games started a few years ago",
"76:12": "people generally use them to kind of",
"76:16": "create images out of thin air which I",
"76:20": "personally don't think is a particularly",
"76:22": "useful or interesting thing to do but",
"76:27": "it's kind of a good I don't know it's a",
"76:28": "good research exercise I guess so",
"76:30": "we implemented this this w again paper",
"76:33": "which is kind of really the first one to",
"76:35": "do a somewhat adequate job somewhat",
"76:37": "easily and so you can see how to do that",
"76:40": "with the first AI library it's kind of",
"76:43": "interesting because the data set we use",
"76:47": "is this else on bedrooms data set which",
"76:51": "we've provided in our URLs which just as",
"76:54": "you can see has bedrooms lots and lots",
"76:57": "and lots of bedrooms and the approach",
"77:03": "you'll see in the pros here that Silva",
"77:06": "wrote the the the approach that we use",
"77:08": "in this case is to just say can we",
"77:12": "create a bedroom and so what we actually",
"77:14": "do is that the the input to the",
"77:20": "generator isn't an image that we clean",
"77:22": "up we actually feed to the generator",
"77:25": "random noise and so then the generators",
"77:29": "task is can you turn random noise into",
"77:32": "something which the critic can't tell",
"77:34": "the difference between that output and a",
"77:35": "real bedroom",
"77:37": "and so we're not doing any pre training",
"77:40": "here or any of the stuff that makes this",
"77:42": "kind of fast and easy so this is a very",
"77:49": "traditional approach but you can still",
"77:50": "see you still just go you know again",
"77:52": "learner and there's actually a double",
"77:53": "you can version which is you know this",
"77:55": "kind of older style approach but you",
"77:58": "just passing the data in the generator",
"77:59": "and the critic in the usual way",
"78:01": "and you call fit and you'll see in this",
"78:06": "case we have show image on you know",
"78:08": "after epoch one it's not creating great",
"78:11": "bedrooms or two or three and you can",
"78:12": "really see that in the early days of",
"78:14": "these kinds of games it doesn't do a",
"78:15": "great job of anything but eventually",
"78:18": "after you know a couple of hours of",
"78:22": "training producing somewhat like bedroom",
"78:27": "ish things you know so anyway it's a",
"78:30": "notebook you can never play with and",
"78:31": "it's a bit of fun so I was very excited",
"78:40": "when we got faster I to the point in the",
"78:43": "last week or so that we had Gans working",
"78:48": "in a way we're kind of API wise they're",
"78:51": "far more concise and more flexible than",
"78:54": "any other library that exists but also",
"78:57": "kind of disappointed with them they take",
"78:59": "a long time to train and the outputs sto",
"79:01": "like so so and so the next step was like",
"79:04": "well can we get rid of cans entirely so",
"79:07": "the first step with with that I mean",
"79:09": "obviously the thing we really want to do",
"79:11": "is come up with a better loss function",
"79:13": "we want a loss function that does a good",
"79:15": "job of saying this is a high-quality",
"79:16": "image without having to go all the over",
"79:20": "game trouble and preferably it also",
"79:22": "doesn't just say it's a high-quality",
"79:23": "image but it's an image which actually",
"79:26": "looks like the thing it's meant to so",
"79:29": "the real trick here comes back to this",
"79:30": "paper from a couple of years ago",
"79:32": "perceptual losses of real-time style",
"79:34": "transfer and super resolution Justin",
"79:37": "Johnson at our curve this thing they",
"79:40": "call perceptual losses it's a nice paper",
"79:42": "but I hate this term because they're",
"79:45": "nothing particularly perceptual about",
"79:46": "them I would call them feature losses so",
"79:49": "in the fast AI library you'll see",
"79:50": "this referred to as feature losses and",
"79:53": "it shares something with Ganz which is",
"79:55": "that after we go through our generator",
"79:59": "which they call the image transform net",
"80:01": "and you can see it's got this kind of",
"80:02": "unit shaped thing they didn't actually",
"80:04": "use new nets because at the time this",
"80:06": "came out nobody in the machine learning",
"80:08": "world knew about units nowadays of",
"80:11": "course we use units about anyway",
"80:13": "something unit ish i should mention like",
"80:16": "in these kind of these architectures",
"80:19": "where you have a down sampling path",
"80:20": "followed by table sampling path the down",
"80:22": "sampling path is very often called the",
"80:25": "encoder as you saw in our code actually",
"80:28": "we call that the encoder and the up",
"80:30": "sampling path is very often called the",
"80:32": "decoder in generative models more you",
"80:36": "know generally including generative text",
"80:39": "models neural translation stuff like",
"80:41": "that they didn't be called the encoder",
"80:43": "and the decoder two pieces right so we",
"80:46": "have this um this generator and we want",
"80:49": "a loss function that says you know is",
"80:52": "the thing that it's created like the",
"80:54": "thing that we want and so the way they",
"80:56": "do that is they take the prediction",
"80:58": "remember Y hat is what we normally use",
"81:00": "for a prediction from a model we take",
"81:02": "the prediction and we put it through a",
"81:04": "pre trained image net network so at the",
"81:09": "time that this came out the pre-training",
"81:11": "network at work they were using was vgg",
"81:14": "people still take that's a kind of old",
"81:16": "now but people still tend to use it",
"81:18": "because it works fine for this process",
"81:22": "so they take the prediction and they put",
"81:24": "it through vgg the pre trained imagenet",
"81:27": "network it doesn't matter too much which",
"81:29": "one it is and so normally the output of",
"81:31": "that would tell you hey is this",
"81:33": "generated thing you know a dog or a cat",
"81:37": "or an airplane or a or a fire engine or",
"81:40": "whatever right but in the process of",
"81:43": "getting to that final classification it",
"81:45": "goes through lots of different layers",
"81:46": "and in this case they've color-coded all",
"81:49": "the layers with the same grid size in",
"81:51": "the feature map with the same color so",
"81:53": "every time we switch colors we're",
"81:55": "switching grid size so there's a stride",
"81:57": "to convey or in VG's case they still",
"81:59": "used to use some Mac spalling layers",
"82:01": "which kind of similar idea",
"82:04": "and so what we could do is say hey let's",
"82:06": "not take the final output of the vgg",
"82:09": "model on this generated image but let's",
"82:13": "take home is something in the middle",
"82:16": "let's take the activations of some layer",
"82:19": "in the middle",
"82:20": "so those activations you know it might",
"82:23": "be a feature map of like 256 channels by",
"82:27": "28 by 28 say and so those kind of 28 by",
"82:31": "28 grid cells will kind of roughly",
"82:33": "semantically say things like hey in this",
"82:35": "in this part of that 28 by 28 grid is",
"82:38": "there something that looks kind of furry",
"82:39": "or is there something that looks kind of",
"82:41": "shiny or is there something that was",
"82:42": "kind of circular is there something that",
"82:44": "kind of looks like an eyeball or",
"82:45": "whatever so what we do is that we then",
"82:48": "take the target so that the actual Y",
"82:52": "value and we put it through the same",
"82:53": "pre-trained vgg network now we can pull",
"82:55": "out the activations of the same layer",
"82:57": "and then we do a mean square error",
"82:59": "comparison so it'll say like okay in the",
"83:02": "real image grid cell 1 1 of that 28 by",
"83:06": "28 feature map you know is is furry and",
"83:12": "mu and round shaped and in the generated",
"83:16": "image it's very and blue and not round",
"83:19": "shaped so it's kind of like an okay",
"83:21": "match so that ought to go a long way",
"83:25": "towards fixing our eyeball problem",
"83:28": "because in this case the feature Maps",
"83:30": "going to say this eyeballs here it's",
"83:33": "right here but there isn't here so do a",
"83:36": "better job of that please make better",
"83:38": "idols so that's the idea okay and so",
"83:41": "that's what we call feature losses or",
"83:43": "Johnson a tower called perceptual losses",
"83:47": "so so to do that we're going to use the",
"83:59": "since seven super res notebook and this",
"84:03": "time the task we're going to do is kind",
"84:05": "of the same as the previous task but I",
"84:08": "wrote this notebook a little bit before",
"84:09": "the game notebook before I came up with",
"84:12": "the idea of like putting text honours",
"84:14": "from having a random JPEG quality so the",
"84:16": "JPEG quality is always 60 there's no",
"84:18": "text written on top and it's 96 by 96 so",
"84:22": "and it's before I realized what a great",
"84:25": "word crap a PHY is so it's called resize",
"84:28": "so here's our crappy images and our",
"84:32": "original images kind of a similar task",
"84:35": "to what we had before so I'm going to",
"84:40": "try and create a loss function which",
"84:43": "does this so the first thing i do is i",
"84:49": "define a base loss function which is",
"84:53": "basically an account i got to compare",
"84:55": "the pixels and the features you know and",
"84:58": "the choices mainly like MSC or l1",
"85:01": "doesn't matter too much which you choose",
"85:03": "I tend to like l1 better than MSC",
"85:05": "actually so I picked l1 right so anytime",
"85:08": "you see base loss we mean l1 loss that",
"85:12": "you could use MSE loss as well so let's",
"85:15": "create a vgg model right so just using",
"85:17": "the pre-trained model in vgg there's a",
"85:21": "attribute called dot features which",
"85:23": "contains the convolutional part of the",
"85:26": "model so here's the convolutional part",
"85:29": "of the vgg model because we don't need",
"85:31": "the head because we only want the",
"85:33": "intermediate activations so then we'll",
"85:36": "check that on the GPU",
"85:37": "we'll put it into eval mode because",
"85:39": "we're not training it and we'll turn off",
"85:42": "requires grad because we don't want to",
"85:44": "update the weights of this model we're",
"85:46": "just using it for inference right for",
"85:49": "the loss so then let's enumerate through",
"85:52": "all the children of that model and find",
"85:54": "all of the max pooling layers because in",
"85:56": "this in the vgg model that's where the",
"85:58": "grid size changes and as you can see",
"86:01": "from this picture we kind of want to",
"86:03": "grab features from every time just",
"86:07": "before the grid size changes so we grab",
"86:09": "layer I minus 1 that's the layer before",
"86:12": "it changes",
"86:13": "so there's our list of layer numbers",
"86:16": "just before the max Pauling lives and so",
"86:21": "all of those are values not surprisingly",
"86:26": "so those are where we want to grab some",
"86:29": "features from and so we put that in",
"86:32": "blocks it's just a list of ID's so",
"86:34": "here's our feature loss plus which is",
"86:37": "going to implement this idea so",
"86:40": "basically we when we call the feature",
"86:42": "loss class we're going to pass it some",
"86:46": "pre trained model and so that's going to",
"86:48": "be called M feet that's the model which",
"86:51": "contains the features which we want to",
"86:53": "generate for what our feature loss on so",
"86:56": "we can go ahead and grab all of the",
"86:58": "layers from that network that we want",
"87:02": "the losses for that we left or that we",
"87:04": "want the features for to create the",
"87:05": "losses so we're going to need to hawk",
"87:08": "all of those outputs because remember",
"87:10": "that's how we grab intermediate layers",
"87:13": "in pi torch is by hooking them so this",
"87:15": "is going to contain our our hooked",
"87:18": "outputs so now in the forward of feature",
"87:24": "loss we're going to call make features",
"87:27": "passing in the target so this is our",
"87:29": "actual Y which is just going to call",
"87:31": "that vgg model and go through all of the",
"87:34": "stored activations and just grab a copy",
"87:37": "of them and so we're going to do that",
"87:39": "both for the target call that out feet",
"87:42": "and for the input so that's the output",
"87:45": "of our generator in feet and so now",
"87:50": "let's calculate the l1 loss between the",
"87:54": "pixels because we still want the pixel",
"87:56": "lost a little bit and then let's also go",
"87:59": "through all of those layers features and",
"88:05": "get the l1 loss on them right so we're",
"88:08": "basically going through every one of",
"88:09": "these end of each block and grabbing the",
"88:14": "activations and getting the l1 on each",
"88:17": "one so that's going to end up in this",
"88:23": "list called feature losses which are",
"88:26": "then",
"88:26": "sum them all up okay and you know by the",
"88:29": "way the reason to do it as a list is",
"88:30": "because we've got this nice little",
"88:32": "callback that if you put them into a",
"88:34": "thing called metrics in your loss",
"88:36": "function it'll print out all of the",
"88:38": "separate layer loss amounts for you",
"88:43": "which is super handy so that's it that's",
"88:47": "our perceptual loss or feature loss plus",
"88:50": "and so now we can just go ahead and",
"88:52": "train a unit in the usual way with our",
"88:54": "data and pre-trained architecture which",
"88:56": "is a resonance ID for passing in our",
"88:58": "loss function which is using our pre",
"89:01": "trained vgg model and this is that",
"89:03": "callback I mentioned lost metrics which",
"89:05": "is going to print out all the different",
"89:06": "layers losses for us these are two",
"89:10": "things that we'll learn about in part",
"89:11": "two of the course but you should use",
"89:12": "them allow find I just created a little",
"89:17": "function called do fit that does fit one",
"89:18": "cycle and then saves the model and then",
"89:21": "shows the results so as per usual",
"89:24": "because we're using a pre trained",
"89:26": "network in our unit we start with frozen",
"89:28": "layers for the down sampling path train",
"89:32": "for a while and as you can see we get",
"89:34": "not only the loss but also the pixel",
"89:36": "loss and the loss at each of our feature",
"89:38": "layers and then also something we'll",
"89:40": "learn about in part two called gram loss",
"89:43": "which I don't think anybody's used for",
"89:47": "super ears before as far as I know but",
"89:49": "as you'll see it turns out great so",
"89:51": "that's eight minutes so much faster than",
"89:54": "it can and already as you can see this",
"89:57": "is our output modeled out pretty good so",
"90:01": "then we unfreeze and train some more and",
"90:05": "it's a little bit better and then let's",
"90:08": "switch up to double the size and so we",
"90:11": "need to also have the batch size to",
"90:12": "avoid writing a GPU memory and freeze",
"90:15": "again and train some more so it's now",
"90:17": "taking half an hour even better and then",
"90:21": "unfreeze and train some more so all in",
"90:24": "all we've done about an hour and 20",
"90:26": "minutes of training and look at that",
"90:30": "it's it's it's done it like I mean those",
"90:33": "it's it knows that eyes are important so",
"90:35": "it's really made an effort it knows that",
"90:37": "fur is important so it's really made an",
"90:39": "effort so it's",
"90:40": "with something with like JPEG artifacts",
"90:41": "around the ears and all this mess and",
"90:46": "like eyes that are just kind of vague",
"90:48": "light blue things and it just it really",
"90:51": "created a lot of texture this cat is",
"90:54": "clearly kind of like looking over the",
"90:56": "top of one of those little chlorine",
"90:58": "frames covered in fuzz so it actually",
"91:00": "recognized that this thing is probably",
"91:02": "kind of a capita materials it's created",
"91:05": "a capital e material for us so I mean",
"91:08": "that's just remarkable",
"91:11": "so talking of remarkable we can now so I",
"91:17": "I've never seen outputs like this before",
"91:22": "without again so I was just so excited",
"91:25": "when we were able to generate this and",
"91:27": "so quickly one GPU hour-and-a-half so",
"91:31": "like if you create your own crap",
"91:32": "efficacious functions and train this",
"91:35": "model your build stuff that nobody's",
"91:37": "built before because like nobody else's",
"91:39": "that I know of is doing it this way so",
"91:42": "there are huge opportunities I think so",
"91:44": "check this out what we can now do is we",
"91:47": "can now instead of starting with our low",
"91:51": "res I actually stored another set at",
"91:54": "size 256 which are called medium res so",
"91:57": "let's see what happens if we up size a",
"91:59": "medium res so we're going to grab our",
"92:02": "medium res data and here is here is our",
"92:09": "medium res stored photo and so can we",
"92:13": "improve this so you can see there's",
"92:15": "still a lot of room for improvement like",
"92:16": "you see the the lashes here are very",
"92:20": "pixelated place where there should be",
"92:22": "hair here is just kind of fuzzy so watch",
"92:25": "this area as I hit down on my keyboard",
"92:27": "bump look at that it's done it you know",
"92:31": "it's taken a medium res image and it's",
"92:33": "made a totally clear thing here you know",
"92:37": "the thirds reappeared light look at the",
"92:39": "eyeball let's go back the eyeball here",
"92:42": "is just kind of a general blue thing",
"92:45": "here it's added all the right texture",
"92:48": "you know so I just think this is super",
"92:53": "exciting you know",
"92:53": "here's a model I trained in an hour and",
"92:56": "a half using standard stuff that you've",
"92:59": "all learnt about a you NetApp retrain",
"93:01": "model feature loss function and we've",
"93:04": "got something which can turn that into",
"93:07": "that or you know this absolute mess into",
"93:13": "this and like it's really exciting to",
"93:15": "think what what could you do with that",
"93:18": "right so one of the inspirations here",
"93:21": "has been a guy called Jason integer and",
"93:26": "Jason was a student in the course last",
"93:31": "year and what he did very sensibly was",
"93:39": "decided to focus basically nearly quit",
"93:43": "his job and work four days a week for",
"93:45": "really six days a week on studying deep",
"93:46": "learning and as you should do he created",
"93:49": "a kind of capstone project and his",
"93:51": "project was to combine Ganz and feature",
"93:55": "losses together and his krappa fication",
"93:58": "approach was to take color pictures and",
"94:03": "make the black and white so he took the",
"94:05": "whole of image net created a black and",
"94:07": "white image net and then trained a model",
"94:09": "to recolor eyes it and he's put this up",
"94:11": "as do defy and now he's got these actual",
"94:14": "old photos from the 19th century",
"94:17": "that he's turning into color and like",
"94:21": "what this is doing is incredible like",
"94:24": "like look at this the model thought oh",
"94:26": "that's probably some kind of copper",
"94:27": "kettle so I'll make it like copper",
"94:29": "colored and oh these pictures are on the",
"94:31": "wall they're probably let different",
"94:33": "colors to the wall and maybe that looks",
"94:35": "a bit like a mirror maybe it would be",
"94:38": "reflecting stuff outside you know",
"94:42": "these things might be vegetables",
"94:43": "vegetables are often red you know let's",
"94:45": "make them red it's it's extraordinary",
"94:49": "what it's done and you could totally do",
"94:52": "this too like you can take our feature",
"94:55": "loss and our gam loss and combine them",
"94:58": "so I'm very grateful to Jason because",
"95:00": "he's helped us build this this lesson",
"95:03": "has been really nice because we've been",
"95:04": "able to help him to because he hadn't",
"95:08": "realize that he can use all this",
"95:09": "pre-training and stuff and so hopefully",
"95:10": "you'll see two older phi in the next",
"95:12": "couple of weeks be even better at the",
"95:15": "older fication but hopefully you all can",
"95:17": "now add other kinds of D Kappa fication",
"95:20": "methods as well so I'm you know I like",
"95:26": "every course if possible to show",
"95:30": "something totally new because then every",
"95:33": "student has the chance to basically",
"95:35": "build things that had never been built",
"95:36": "before so this is this is kind of that",
"95:38": "thing you know but between the much",
"95:41": "better segmentation results and these",
"95:42": "much simpler and faster",
"95:44": "d krappa fication results I think you",
"95:46": "can build some really cool stuff do you",
"95:48": "have a question is it possible to use",
"95:56": "similar ideas to unit and ganz for NLP",
"95:59": "for example if I want to tag the verbs",
"96:01": "and nouns in a sentence or create a",
"96:03": "really good Shakespeare generator yeah",
"96:08": "pretty much we don't fully know yet it's",
"96:11": "a pretty new area but there's a lot of",
"96:14": "opportunities there and we'll be looking",
"96:16": "at some in in a moment actually",
"96:23": "so I actually tried training this",
"96:27": "actually tried testing this on this",
"96:29": "remember this picture I showed you with",
"96:31": "a slide last lesson and it's a really",
"96:34": "rubbishy looking picture and I thought",
"96:35": "what would happen if we tried running",
"96:36": "this just through the exact same model",
"96:39": "and it changed it from that to that so I",
"96:43": "thought that was a really good example",
"96:44": "you can see something it didn't do which",
"96:47": "is this weird discoloration it didn't",
"96:49": "fix it because I didn't crap a PHY",
"96:51": "things with weird discoloration right so",
"96:53": "if you want to create really good image",
"96:55": "restoration like I say you did really",
"96:56": "good gratification okay so um here's",
"97:02": "what we've learned so far right in in",
"97:06": "the course some of the main things so",
"97:08": "we've learnt that neural nets consist of",
"97:13": "Sandwich layers of are fine functions",
"97:15": "which are basically matrix",
"97:16": "multiplications slightly more general",
"97:18": "version and nonlinearities like rel you",
"97:20": "and we",
"97:21": "that the results of those calculations",
"97:23": "are called activations and the things",
"97:26": "that go into those calculations that we",
"97:28": "learn accord parameters and that the",
"97:30": "parameters are initially randomly",
"97:32": "initialized or we copy them over from a",
"97:35": "pre-trained model and then we train them",
"97:37": "with SGD or faster versions and we",
"97:40": "learned that convolutions are a",
"97:42": "particular affine function that work",
"97:43": "great for auto correlated data so things",
"97:47": "like images and stuff we learn about",
"97:48": "batch norm drop out data orientation and",
"97:51": "weight decay as ways of regularizing",
"97:54": "models and also batch norm helps train",
"97:56": "models more quickly and then today we've",
"97:58": "learned about res slash dense blocks",
"98:01": "we've all of us you learn a lot about",
"98:03": "image classification regression",
"98:05": "embeddings categorical and continuous",
"98:07": "variables collaborative filtering",
"98:10": "language models and NLP classification",
"98:12": "and then kind of segmentation unit and",
"98:14": "gains so go over these things and make",
"98:19": "sure that you feel comfortable with each",
"98:21": "of them if you've only watched this",
"98:22": "series once you definitely won't people",
"98:26": "normally watch it you know three times",
"98:27": "or so to really understand the detail so",
"98:32": "one thing that doesn't that doesn't get",
"98:36": "here is our it ends so that's the last",
"98:39": "thing we're gonna do there it is okay so",
"98:43": "marinades I'm going to introduce a",
"98:45": "little kind of diagrammatic method here",
"98:46": "to explain our ends and the diagrammatic",
"98:49": "method I'll start by showing you a basic",
"98:50": "neural net with a single hidden layer",
"98:52": "square means an input so that'll be",
"98:57": "batch size by number of inputs right so",
"99:01": "kind of you know batch size by number of",
"99:06": "inputs an arrow means a layer broadly",
"99:12": "defined such as matrix product followed",
"99:15": "by value a circle is activations okay so",
"99:21": "in this case we have one set of hidden",
"99:24": "activations and so given that the import",
"99:27": "was number of inputs this here is a",
"99:30": "matrix of number of inputs by number of",
"99:33": "activations so the output will be",
"99:35": "a batch size by a number of activations",
"99:38": "it's really important you know how to",
"99:40": "calculate these shapes right so go learn",
"99:42": "dot summary lots to see all the shapes",
"99:46": "so then here's another arrow so that",
"99:48": "means it's another layer matrix product",
"99:51": "followed by non-linearity in this case",
"99:52": "we go into the output so we use soft Max",
"99:54": "and then triangle means an output okay",
"99:59": "and so this matrix product will be",
"100:01": "number of activations by a number of",
"100:02": "classes so our output is batch size by",
"100:04": "number classes",
"100:05": "okay so let's reuse the that key",
"100:10": "remember triangle Airport circle is",
"100:12": "activations hidden state we also call",
"100:16": "that and rectangle is important so let's",
"100:20": "now imagine that we wanted to create a",
"100:23": "get a big document split it in two sets",
"100:27": "of three words at a time and grab each",
"100:30": "set of three words and then try to",
"100:31": "predict the third word using the first",
"100:35": "two words so if we had the data set in",
"100:38": "place we could grab word one as an",
"100:41": "import chuck it through an embedding",
"100:42": "right create some activations pass that",
"100:46": "through a matrix product and a",
"100:50": "non-linearity grab the second word put",
"100:55": "it through an embedding and then we",
"100:57": "could either add those two things",
"100:59": "together or concatenate them generally",
"101:02": "speaking when you see kind of two sets",
"101:04": "of activations coming together in a",
"101:07": "diagram you normally have a choice of",
"101:09": "concatenate or or add and that's going",
"101:13": "to create the second bunch of",
"101:14": "activations and then you could put it",
"101:16": "through one more fully connected layer",
"101:20": "and softmax to create an output so that",
"101:23": "would be a totally standard fully",
"101:26": "connected neural net with one very minor",
"101:28": "tweak which is concatenated or adding at",
"101:31": "this point which we could use to try to",
"101:33": "predict the third word of every from",
"101:37": "pairs of two words okay so remember",
"101:41": "arrows represent layer operations",
"101:44": "and I removed on this one the specifics",
"101:48": "of what they are because they're always",
"101:49": "an ephah in function followed by a",
"101:51": "non-linearity okay let's go further what",
"101:58": "if we wanted to predict word for using",
"102:01": "words one and two and three it's",
"102:03": "basically the same picture as last time",
"102:04": "except with one extra input and one",
"102:06": "extra circle but I want to point",
"102:08": "something out which is each time we go",
"102:13": "from rectangle to circle we're doing the",
"102:16": "same thing we're doing an embedding",
"102:17": "which is just a particular kind of",
"102:20": "matrix model play where you have a",
"102:22": "one-pot encoded input each time we go",
"102:25": "from circle to circle we're basically",
"102:27": "taking one piece of hidden state run",
"102:31": "through activations and turning it into",
"102:32": "another set of activations by saying",
"102:34": "we're now at the next word and then when",
"102:37": "we go from circle to triangle we're",
"102:39": "doing something else again which is",
"102:41": "we're saying let's convert the hidden",
"102:43": "state these activations into an output",
"102:45": "so it would make sense so you can see",
"102:47": "I've colored each of those errors",
"102:49": "differently so each of those arrows",
"102:51": "should probably use the same weight",
"102:54": "matrix because it's doing the same thing",
"102:57": "so why would you have a different set of",
"102:59": "embeddings for each word or a different",
"103:01": "set of a different matrix to multiply by",
"103:04": "to go from this hidden state to this",
"103:05": "hidden state versus this one okay so",
"103:09": "this is what we're going to build so",
"103:13": "we're now going to jump into human",
"103:21": "numbers which is less than seven human",
"103:23": "numbers and this is a data set that I",
"103:24": "created which literally just contains",
"103:26": "all the numbers from one to nine",
"103:29": "thousand hundred 99 written out in",
"103:31": "English okay and we're going to try and",
"103:33": "create a language model that can predict",
"103:35": "the next word in this document it's just",
"103:37": "a toy example for this purpose so in",
"103:42": "this case we only have one document and",
"103:44": "that one document is the list of numbers",
"103:46": "so we can use a text list to create an",
"103:50": "item list with text in for the training",
"103:51": "and the validation in this case the",
"103:53": "validation set is the numbers from 8,000",
"103:55": "onwards and the training set is 1 to",
"103:57": "8,000",
"103:59": "we can combine them together turn that",
"104:02": "into a data bunch so we only have one",
"104:05": "document so train zero is the document",
"104:07": "grab its dot txt that's how you grab the",
"104:09": "contents of a text list and here are the",
"104:11": "first 80 characters",
"104:14": "it starts with a special token xx POS",
"104:17": "anything starting with xx is a special",
"104:19": "fast a token the OS is the beginning of",
"104:22": "stream token it basically says this is",
"104:24": "the start of a document and it's very",
"104:26": "helpful in NLP to know when documents",
"104:29": "start so that your models can learn to",
"104:31": "recognize them",
"104:33": "the validation set contains 13,000",
"104:35": "tokens so 13 thousand words or",
"104:38": "punctuation marks because everything",
"104:40": "between spaces is a separate token the",
"104:44": "batch size that we asked for was 64 and",
"104:52": "then by default it uses something called",
"104:54": "be PTT of 70 be PTT as we briefly",
"104:57": "mentioned stands for back prop through",
"105:00": "time that's the sequence links so for",
"105:03": "each of our so we're there to our kind",
"105:06": "of 64 document segments we split it up",
"105:10": "into lists of 70 words that we look at",
"105:14": "at one time so what we do is we grab",
"105:17": "this for the validation set entire",
"105:20": "string of 13,000 tokens and then we",
"105:24": "split it into 64 roughly equal sized",
"105:29": "sections okay people very very very",
"105:31": "often think I'm saying something",
"105:33": "different I did not say they are of",
"105:34": "length 64 they're not they're 64 equally",
"105:39": "sized roughly segments so we take the",
"105:42": "first one 64th of the document piece one",
"105:45": "second 64 space - okay and then for each",
"105:51": "of those",
"105:52": "1/64 of the document we then split those",
"105:55": "into pieces of length 70 so each batch",
"106:01": "right so let's now say okay for those",
"106:04": "13,000 tokens how many batches are there",
"106:07": "well divide by batch size and divide by",
"106:10": "70",
"106:11": "there's about 2.9 batches so three",
"106:13": "there's going to be three batches so",
"106:15": "let's grab an iterator for a data loader",
"106:17": "grab one two three batches the X and the",
"106:21": "y and let's add up the number of",
"106:24": "elements and we get back slightly less",
"106:27": "than this because there's a little bit",
"106:29": "left over at the end that doesn't quite",
"106:30": "make up a full batch okay so this is the",
"106:35": "kind of stuff you should play around",
"106:36": "with a lot lots of shapes and sizes and",
"106:38": "stuff and iterators as you can see it's",
"106:41": "95 by 64 I claimed it was going to be 70",
"106:45": "by 64 that's because our data loader for",
"106:49": "language models slightly randomizes the",
"106:52": "PTT just to give you a bit more kind of",
"106:55": "shuffling get bit more randomization it",
"106:57": "helps the model and so here you can see",
"107:01": "the first batch of X yeah remember we've",
"107:07": "numeric alized all these and here's the",
"107:10": "first batch of Y and you'll see here",
"107:12": "this is 2 18 10 11 8 this is 18 10 11 8",
"107:17": "so this one is offset by 1 from here",
"107:21": "because that's what we want to do with a",
"107:23": "language model we want to predict the",
"107:25": "next word so after two should come up 18",
"107:29": "and after 18 should come 10 right you",
"107:34": "can grab the vocab for this data set and",
"107:36": "a vocab has a text defy so if we call it",
"107:39": "exactly the same look at the same thing",
"107:40": "but with text fi that'll just look it up",
"107:42": "in the vocab so here you can see xx POS",
"107:45": "8001 where else in the why there's no xx",
"107:49": "POS is just 8,000 one so after xx POS is",
"107:52": "8 after 8 these thousand after thousand",
"107:54": "is 1 okay",
"107:57": "and so then after we get 8023 comes x 2",
"108:02": "and look at this we're always looking at",
"108:04": "column 0 so this is the first batch the",
"108:07": "first mini batch comes 8024 and then X 3",
"108:11": "all the way up to 8000 40 right and so",
"108:15": "then we can go right back to the start",
"108:18": "but look at batch 1 right so index 1",
"108:22": "which is better number 2 and now we can",
"108:24": "continue",
"108:25": "a slight skip from 8042 8000 46 that's",
"108:28": "because the last mini batch wasn't quite",
"108:30": "complete so what this means is that",
"108:33": "every mini batch so every yeah every",
"108:38": "mini batch joins up with a previous mini",
"108:40": "batch you know so you can go straight",
"108:42": "from x1 0 to X 2 0 to continue 8023 8024",
"108:48": "right and so he took the same thing for",
"108:51": "colon comma 1 you'll also see they join",
"108:54": "up so all the mini batches join up so",
"108:58": "that's the data we can do show better to",
"109:01": "see it and here is our model which is",
"109:09": "doing this right so here is this is just",
"109:18": "the code copied over right so it content",
"109:21": "contains one embedding ie the green",
"109:25": "arrow one hidden to hidden brown arrow",
"109:30": "layer and one hidden to output right so",
"109:34": "each colored arrow has a single matrix",
"109:38": "okay and so that in the forward paths we",
"109:42": "take our first input X 0 and put it",
"109:46": "through input to hidden the green arrow",
"109:49": "okay create our first set of activations",
"109:51": "which we call H assuming that there is",
"109:55": "the second word because like sometimes",
"109:57": "we might be at the end of a batch where",
"109:59": "there isn't a second word assume there",
"110:00": "is a second word then we would add to H",
"110:03": "the result of x1 put through the green",
"110:07": "arrow",
"110:07": "remember that's H and then we would say",
"110:12": "okay our new H is the result of those",
"110:17": "two add it together put through our",
"110:21": "hidden to hidden orange arrow and then",
"110:23": "rel you then batch them and then for the",
"110:25": "second word do exactly the same thing",
"110:27": "and then finally blue arrow put it",
"110:30": "through H oh all right so that's how we",
"110:32": "convert our diagram to code so now",
"110:37": "new here at all so now let's to okay and",
"110:44": "and just you know so we can check that",
"110:45": "in the learner and we can train it 46",
"110:47": "percent okay let's take this code and",
"110:49": "recognize it's pretty awful",
"110:52": "there's a lot of duplicate code and as",
"110:54": "coders when we see duplicate code what",
"110:56": "do we do we refactor so we should",
"110:58": "reflect to this into a loop so here we",
"111:01": "are we've refactored it into a loop so",
"111:04": "now we're going for each X I and X and",
"111:06": "doing it in the loop guess what",
"111:09": "that's an hour and in an hour it in is",
"111:12": "just a refactoring it's not anything new",
"111:18": "this is now a narrative okay and let's",
"111:22": "refactor our diagram from this to this",
"111:26": "this is the same diagram okay but I've",
"111:30": "just replaced it with my loop does the",
"111:34": "same thing so here here it is it's got",
"111:37": "exactly the same unit literally exactly",
"111:38": "the same just popped a loop here before",
"111:41": "I start I just have to make sure that",
"111:42": "I've got some you know a bunch of zeros",
"111:44": "to add to and of course I get exactly",
"111:48": "the same result when I train it okay so",
"111:51": "next thing that you might think then and",
"111:54": "one nice thing about the loop though is",
"111:55": "now this will work even if I'm not",
"111:57": "predicting the fourth word from the",
"111:59": "previous three but the ninth word to the",
"112:01": "previous eight it'll work for any",
"112:02": "arbitrarily length long sequence just",
"112:05": "nice so let's up the BP TT to 20 since",
"112:08": "we can now and let's say I'll say okay",
"112:12": "instead instead of just predicting the",
"112:19": "length word from the previous n minus",
"112:22": "one let's try to predict the second word",
"112:25": "from the first in the third from the",
"112:27": "second and the fourth from the third and",
"112:28": "so forth right because previously like",
"112:30": "look at our loss function previously we",
"112:32": "were comparing the result of our model",
"112:35": "to just the last word of the sequence it",
"112:37": "is very wasteful because there's a lot",
"112:39": "of words in the sequence so let's",
"112:40": "compare every word in X to every word in",
"112:44": "Y so to do that we need to change this",
"112:47": "so it's not just one triangle at the end",
"112:49": "of the loop",
"112:50": "but the triangle is inside this right so",
"112:55": "that in other words after every loop",
"112:58": "predict loop predict loop predict so",
"113:04": "here's this code it's the same as the",
"113:06": "previous code but now I've created an",
"113:08": "array and every time I go through the",
"113:11": "loop I append HOH to the array so now",
"113:15": "for n inputs I create n outputs so I'm",
"113:18": "predicting after every word previously I",
"113:21": "had 46% now I have 40% why is it worse",
"113:26": "well it's worse because now like when",
"113:30": "I'm trying to predict the second word I",
"113:32": "only have one word of state to use okay",
"113:35": "so like and when I'm looking at the",
"113:37": "third word I only have two words of",
"113:39": "state to use so it's a much harder",
"113:40": "problem for it to solve so the obvious",
"113:44": "way to fix this then would you know the",
"113:47": "key problem is here I go H equals torch",
"113:49": "zeros like I reset my state zero every",
"113:52": "time I start another be PTT sequence",
"113:55": "well let's not do that let's keep H",
"113:58": "right and we can because remember each",
"114:01": "batch connects to the previous batch",
"114:03": "it's not shuffled like happens in you",
"114:07": "know image classification so let's take",
"114:09": "this exact model and replicate it again",
"114:11": "but let's move the creation of H into",
"114:13": "the constructor okay there it is so it's",
"114:17": "now self dot H okay and so this is now",
"114:19": "exactly the same code but at the end",
"114:21": "let's put the new H back into self dot H",
"114:24": "okay so it's now doing the same thing",
"114:26": "but it's not throwing away that state",
"114:29": "and so therefore now we actually get",
"114:33": "above the original we get all the way up",
"114:35": "to 54 percent accuracy so this is what a",
"114:39": "real iron tin looks like they you know",
"114:42": "you you always want to keep that state",
"114:45": "right but just keep remembering there's",
"114:47": "nothing different about an RNA and it's",
"114:49": "a totally normal fully connected neural",
"114:51": "net okay it's just that you've got a",
"114:53": "loop you refactored what you could do",
"114:59": "though is at the end of your every loop",
"115:03": "you",
"115:04": "could not just spit out an output but",
"115:05": "you could spit it out into another",
"115:07": "errand in so you have an errand in going",
"115:09": "into an errand and that's nice because",
"115:11": "we've now got more layers of computation",
"115:13": "you would expect that to work better",
"115:16": "well to get there let's do some more",
"115:19": "refactoring so let's take this code and",
"115:23": "replace it with the equivalent built in",
"115:25": "pipe torch code which is you just say",
"115:29": "that so n n dot R and n basically says",
"115:31": "do the loop for me okay we've still got",
"115:34": "the same embedding we've still got the",
"115:36": "same output still got the same batch",
"115:37": "norm we've still got the same",
"115:39": "initialization of H but we just got rid",
"115:41": "of the loop so one of the nice things",
"115:43": "about our a10 is that you can now say",
"115:46": "how many layers you want so this is the",
"115:50": "same accuracy of course so here I'm",
"115:53": "going to do it with two layers but",
"115:56": "here's the thing when you think about",
"115:58": "this right think about it without the",
"116:01": "loop it looks like this right",
"116:04": "it's like keeps on going and we've got a",
"116:06": "BP GT of 20 so there's 20 layers of this",
"116:09": "and we know from that visualizing the",
"116:13": "lost landscapes paper that deep networks",
"116:15": "have awful bumpy lost surfaces so when",
"116:20": "you start creating long timescales and",
"116:24": "multiple layers these things get",
"116:28": "impossible to train so there's a few",
"116:32": "tricks you can do one thing is you can",
"116:33": "add skip connections of course about",
"116:37": "what people normally do is instead they",
"116:39": "put inside instead of just adding these",
"116:43": "together they actually use a little mini",
"116:45": "neural net to decide how much of the",
"116:48": "green arrow to keep and how much of the",
"116:50": "orange arrow to keep and when you do",
"116:52": "that you get something that's either",
"116:53": "called giu or an L STM depending on the",
"116:57": "details of that little neural net and",
"116:59": "we'll learn about the details of those",
"117:00": "neural nets in part 2 they really don't",
"117:02": "matter though frankly so we can now say",
"117:05": "let's create a GI u instead it says just",
"117:07": "like what we had before but it'll handle",
"117:10": "longer sequences in deeper networks",
"117:12": "let's use two layers bump and we're up",
"117:16": "to 75%",
"117:19": "okay so that's how it ends and the main",
"117:27": "reason what it's show it to you was to",
"117:28": "remove the the last remaining piece of",
"117:31": "magic and this is one of like the least",
"117:34": "magical things we have in deep learning",
"117:36": "it's just a refactored fully connected",
"117:39": "Network",
"117:40": "so don't let our own ends ever ever put",
"117:43": "you off and with this approach where you",
"117:47": "basically have a sequence of n inputs",
"117:49": "and a sequence of n outputs which we've",
"117:51": "been using for language modeling you can",
"117:53": "use that for other tasks right for",
"117:55": "example the sequence of outputs could be",
"117:57": "for every word there could be something",
"117:59": "saying is there something that I is",
"118:01": "sensitive and I want to anonymize or not",
"118:02": "you know so like is this private private",
"118:06": "data or not or it could be a part of",
"118:08": "speech tag for that word or it could be",
"118:12": "something saying you know how should",
"118:15": "that word be formatted or whatever and",
"118:18": "so these are called sequence labeling",
"118:20": "tasks and so you can use this same",
"118:22": "approach for pretty much any sequence",
"118:24": "labeling task or you can do what I did",
"118:27": "in the earlier lesson which is once you",
"118:29": "finish building your language model you",
"118:33": "can throw away their kind of this h0 bit",
"118:38": "and instead pop their a standard",
"118:41": "classification head and then you can now",
"118:44": "do NLP classification which as you saw",
"118:46": "earlier what if you stayed at the out",
"118:49": "results even on long documents so this",
"118:54": "is a super valuable technique and not",
"118:57": "remotely metrical okay so that's it",
"119:01": "right that's that's deep learning or at",
"119:04": "least you know the kind of the practical",
"119:06": "pieces from my point of view having",
"119:12": "watched this one time you won't get it",
"119:17": "all and I don't recommend that you do",
"119:19": "watch this so slowly that you get it all",
"119:21": "the first time but you go back and look",
"119:24": "at it again take your time and there'll",
"119:26": "be bits that you go like oh now I see",
"119:28": "what he's saying and",
"119:30": "you'll be able to like implement things",
"119:31": "you couldn't implement before and you'll",
"119:32": "be able to dig in more than you thought",
"119:34": "so like definitely go back and do it",
"119:35": "again and as you do write rack code not",
"119:39": "just for yourself but put it on github",
"119:41": "right it doesn't matter if you think",
"119:43": "it's great code or not you know the fact",
"119:45": "that you're writing code and sharing it",
"119:48": "is impressive and the feedback you'll",
"119:51": "get if you tell people on the forum you",
"119:53": "know hey I wrote this code it's not",
"119:55": "great but you know it's my first effort",
"119:57": "anything you see jump out at you people",
"120:00": "will say like oh that bit was done well",
"120:02": "hey but you do know for this bit you",
"120:04": "could have used this library and",
"120:05": "Safety's in time you'll learn a lot by",
"120:07": "interacting with your peers as you've",
"120:11": "noticed I've started introducing more",
"120:13": "and more papers now part two will be a",
"120:15": "lot of papers and so it's a good time to",
"120:17": "start reading some of the papers that",
"120:21": "have been introduced in this in this",
"120:22": "section all the bits that say like",
"120:25": "derivation and theorems and lemmas you",
"120:27": "can skip them I do they add almost",
"120:29": "nothing to your understanding of",
"120:31": "practical tech learning right",
"120:32": "but the bits that say like you know why",
"120:36": "are we solving this problem and what are",
"120:38": "the results and so forth really",
"120:40": "interesting and then you know try and",
"120:43": "write English prose not know English",
"120:49": "prose that you want to be read by Geoff",
"120:50": "Hinton and Yann LeCun",
"120:51": "but English prose that you want to be",
"120:53": "written read by you as of six months ago",
"120:56": "right because there's a lot more people",
"120:58": "in the audience of you as well six",
"121:01": "months ago then there is of Geoffrey",
"121:03": "Hinton Danielle Loughran right that's",
"121:04": "that's the person you best understand",
"121:06": "you know what they need right go and get",
"121:11": "help and help others",
"121:12": "tell us about your success stories but",
"121:16": "perhaps the most important one is get",
"121:17": "together with others that people's",
"121:19": "learning works much better if you've got",
"121:21": "that social experience so start a book",
"121:25": "club get involved in meetups create",
"121:27": "study groups and build things right and",
"121:32": "again they it doesn't have to be amazing",
"121:35": "like just build something that do you",
"121:38": "think the world would be a little bit",
"121:39": "better if that what existed or you think",
"121:42": "it would be kind of slightly delightful",
"121:44": "to your two-year-old to see that thing",
"121:45": "or you just want to show it to your",
"121:47": "brother the next time they come around",
"121:48": "to see what you're doing whatever right",
"121:50": "like just finish something you know",
"121:54": "finish something and then try and make",
"121:57": "it a bit better so for example something",
"122:01": "I just saw this afternoon is the Elan",
"122:03": "Elan musk twitch ener ATAR okay so",
"122:09": "looking at lots of older tweets creating",
"122:11": "a language model from from Elon Musk and",
"122:14": "then creating new tweets such as",
"122:16": "humanity will also have an option to",
"122:18": "publish on its own journey as an only in",
"122:20": "civilization it will always like all",
"122:23": "human being Mars is no longer possible",
"122:26": "hey I will definitely be the Central",
"122:29": "Intelligence Agency okay so this is",
"122:31": "great I love this and I love that Dave",
"122:34": "Smith wrote and said these are my",
"122:37": "first-ever commits thanks for teaching a",
"122:40": "finance guy how to build an app in 8",
"122:42": "weeks right so you know I think this is",
"122:46": "awesome and I think like clearly a lot",
"122:48": "of care and passion is being put into",
"122:50": "this project you know will it",
"122:55": "systematically change the future",
"122:57": "direction of society as a whole maybe",
"122:59": "not you know but maybe Elon will look at",
"123:02": "this and think like oh you know like I",
"123:04": "maybe I need to rethink my method of",
"123:05": "pros I don't know I think it's I think",
"123:08": "it's great",
"123:09": "and so yeah create something put it out",
"123:12": "there put a bit of yourself into it or",
"123:16": "get involved in fast AI the first thing",
"123:19": "I project there's a lot going on you",
"123:21": "know you can help with documentation and",
"123:23": "tests which might sound boring but you'd",
"123:25": "be surprised how incredibly not boring",
"123:27": "it is to like take a piece of code that",
"123:29": "hasn't been properly documented and",
"123:31": "research it and understand it and ask",
"123:33": "Sylvia and I on the forum what's going",
"123:35": "on why did you write it this way we'll",
"123:37": "send you off to the papers that we were",
"123:38": "implementing you know writing a test",
"123:40": "requires deeply understanding that part",
"123:42": "of the machine learning world to really",
"123:44": "understand how it's meant to work so",
"123:46": "that's what was interesting stairs",
"123:48": "Backman has created this nice dev",
"123:50": "projects index which you can like go",
"123:53": "onto the forum in the FASTA I dev",
"123:55": "section and find",
"123:57": "get to the dev project section and find",
"123:59": "like give some stuff going on that you",
"124:01": "might want to get involved in or maybe",
"124:02": "there's stuff you want to exist you",
"124:03": "could add your own create a study group",
"124:06": "you know Deena's already created a study",
"124:08": "group for San Francisco starting in",
"124:10": "January this is how easy it is to create",
"124:11": "a study group right go on the forum find",
"124:13": "your little time zone subcategory and",
"124:15": "add a post thing let's create a study",
"124:17": "group okay but make sure you you know",
"124:20": "give people like a little Google sheet",
"124:22": "to sign up some way to actually do",
"124:24": "something you know a great example is",
"124:27": "Pierre who's been doing a fantastic job",
"124:29": "in Brazil of running study groups for",
"124:33": "the last couple of classes of the course",
"124:34": "and you know he keeps posting these",
"124:37": "pictures of people having a good time",
"124:39": "and learning deep learning together",
"124:41": "creating wiki's together creating",
"124:43": "projects together great experience and",
"124:46": "then come back for part two",
"124:48": "right where we'll be looking at all of",
"124:52": "this interesting stuff in particular",
"124:54": "going deep into the first day our code",
"124:56": "base to understand how did we build it",
"124:58": "exactly will actually go through as we",
"125:01": "were building it we created notebooks of",
"125:03": "like here where here's where we were at",
"125:05": "each stage so we're actually going to",
"125:06": "see the software development process",
"125:07": "itself we'll talk about the process of",
"125:09": "doing research how to read academic",
"125:12": "papers how to turn math into code and",
"125:14": "then a whole bunch of additional types",
"125:17": "of models that we haven't seen yet so",
"125:19": "it'll be kind of like going beyond",
"125:21": "practical deep learning into actually",
"125:23": "cutting edge research so we've got five",
"125:28": "minutes to take some questions we had an",
"125:31": "AMA going on online and so we're going",
"125:34": "to have time for a couple of the",
"125:36": "highest-ranked AMA questions from the",
"125:38": "community and the first one is by",
"125:40": "Jeremy's request although it's not the",
"125:42": "highest ranked what's your typical day",
"125:44": "like how do you manage your time across",
"125:47": "so many things that you do yeah I",
"125:51": "thought that I hear that all the time so",
"125:53": "I thought I should answer it and I think",
"125:55": "I got a few votes because I think people",
"126:01": "who come to our study group",
"126:02": "are always shocked at how disorganized",
"126:06": "and incompetent I am and so I often get",
"126:09": "people saying like Oh",
"126:10": "Wow I thought you were like this deep",
"126:12": "learning role model and I'll get to see",
"126:14": "how to be like you and now I'm not sure",
"126:15": "what it'd be like you at all",
"126:17": "so yeah it's for me it's all about just",
"126:24": "having a good time with it I never",
"126:26": "really have many plans I just try to",
"126:28": "finish what I start",
"126:30": "if you're not having fun with it it's",
"126:32": "really really hard to continue because",
"126:34": "there's a lot of frustration in deep",
"126:36": "learning because it's not like writing a",
"126:37": "web app where it's like you know",
"126:39": "authentication check you know back-end",
"126:43": "service watchdog check okay user",
"126:49": "credentials check you know like you just",
"126:50": "you're making progress where else for",
"126:52": "stuff like this",
"126:53": "Dan stuff that we've been doing the last",
"126:55": "couple of weeks it's just like it's not",
"126:58": "working it's not working it's not",
"127:00": "working no it also didn't work it also",
"127:02": "didn't work until oh my god it's amazing",
"127:05": "it's a cat that's kind of what it is",
"127:07": "right so you don't get that regular",
"127:09": "feedback so yeah you know you got to",
"127:13": "have fun with it",
"127:14": "and so so Mike yeah my day is kind of",
"127:18": "you know and the other thing I do I'll",
"127:20": "say I don't I don't do any meetings",
"127:23": "I don't do phone calls I don't do",
"127:25": "coffees I don't watch TV at okay",
"127:28": "computer games I spend a lot of time",
"127:30": "with my family a lot of time exercising",
"127:32": "and a lot of time reading and coding and",
"127:36": "doing things I like so you know I think",
"127:41": "the you know the main thing is just",
"127:43": "finish finish something like properly",
"127:47": "finish it so when you get to that point",
"127:49": "where you think 80% of the way through",
"127:50": "but you haven't quite created to read me",
"127:52": "yet and the install process is still a",
"127:55": "bit clunky and you know this is what 99%",
"127:57": "of github projects look like you'll see",
"127:59": "the readme says to do you know complete",
"128:03": "baseline experiments document blah blah",
"128:06": "blah it's like don't be that person like",
"128:09": "just do something properly and finish it",
"128:12": "and maybe get some other people around",
"128:14": "you to work with you so that you're all",
"128:15": "doing it together and you know get it",
"128:18": "done",
"128:22": "what are the up-and-coming deep learning",
"128:24": "machine learning things that you are",
"128:25": "most excited about also you've mentioned",
"128:28": "last year that you are not a believer in",
"128:29": "reinforcement learning do you still feel",
"128:31": "the same way yeah I still feel exactly",
"128:35": "the same way as I did three years ago",
"128:36": "when we started this which is it's all",
"128:40": "about transfer learning its",
"128:41": "underappreciated its under-researched",
"128:44": "every time we put transfer learning into",
"128:45": "anything we make it much better you know",
"128:48": "our academic paper on transfer learning",
"128:51": "for NLP has you know helped be one piece",
"128:55": "of kind of changing the direction of NLP",
"128:56": "this year it made it all the way to the",
"128:59": "New York Times just a stupid obvious",
"129:01": "little thing that we threw together so I",
"129:04": "remain excited about that I remain",
"129:06": "unexcited about reinforcement learning",
"129:08": "for most things I don't see it used by",
"129:12": "normal people for normal things for",
"129:14": "nearly anything it's an incredibly",
"129:16": "inefficient way to solve problems which",
"129:18": "often solved more simply and more",
"129:20": "quickly in other ways probably has a",
"129:22": "maybe a role in the world but a limited",
"129:27": "one and not in most people's day-to-day",
"129:30": "work for someone planning to take part -",
"129:40": "and 2019 what would you recommend doing",
"129:42": "learning practicing until the part two",
"129:44": "course starts just code",
"129:48": "yeah just code all the time I know it's",
"129:51": "perfectly possible I hear from people",
"129:52": "who get to this point of the course and",
"129:54": "they haven't actually written any code",
"129:55": "yet and if that's you it's okay you know",
"129:59": "you've just go through and do it again",
"130:01": "and this time do code and and look at",
"130:05": "the input the shapes of your inputs and",
"130:07": "look at your outputs to make sure you",
"130:08": "know how to grab a mini batch and look",
"130:10": "at its main standard deviation and plot",
"130:12": "it and you know there's this so much",
"130:16": "material that we've covered if you can",
"130:19": "get to a point where you can you know",
"130:22": "rebuild those notebooks from scratch",
"130:27": "without too much cheating when I say",
"130:30": "from scratch I mean using the first day",
"130:31": "library not from scratch from scratch",
"130:34": "you know you're you're be in the top",
"130:37": "echelon of practitioners because you'll",
"130:39": "be able to do all of these things",
"130:40": "yourself and that's really really rare",
"130:42": "and that'll put you in a great position",
"130:44": "for part two nine o'clock be honest do",
"130:48": "one more and where do you see the fast",
"130:51": "day Iowa library going in the future say",
"130:53": "in five years well like I said I don't",
"130:58": "make plans I just I just piss around so",
"131:02": "I mean our only plan for fast AI you",
"131:07": "know as an you know organization is to",
"131:11": "make deep learning accessible as a tool",
"131:13": "for normal people to use for normal",
"131:16": "stuff",
"131:17": "so as long as we need to code we failed",
"131:20": "at that so the big goal you know cuz",
"131:23": "ninety-nine point eight percent of the",
"131:25": "world can't code so the main goal would",
"131:29": "be to get to a point where it's not a",
"131:30": "library but it's a piece of software",
"131:32": "that doesn't require a code it certainly",
"131:34": "shouldn't require a goddamn lengthy hard",
"131:38": "working course like this one you know so",
"131:41": "I want to get rid of the course I want",
"131:43": "to get rid of the code I want to make it",
"131:45": "so you can just do useful stuff quickly",
"131:47": "and and easily so that's that's",
"131:50": "maybe five years yeah maybe longer all",
"131:53": "right well I hope to see you all back",
"131:54": "here for part two",
"131:55": "thank you"
}
