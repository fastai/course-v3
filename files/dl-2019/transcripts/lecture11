well welcome back welcome to lesson 11 where we're going to be talking mainly about data loading and optimizers I said we would be talking about faster audio but that's gonna be a little bit later in the course we haven't quite got to where I wanted to get to yet so everything I said we'd talk about last week we will talk about but it might take a few more lessons to get there than I said so this is kind of where we're up to is the last little bit of our CNN and specifically these were the things we were going to dive into and we've we've done the first four of these kuda convolutions books normalization so we're going to keep going through this process to try to create our state-of-the-art imagenet model the specific items that we're working with our images but everything we've covered so far is equally valid equally used for tabular collaborative filtering and text and pretty much everything else as well last week we talked about batch norm and I just wanted to mention that at the end of the batch norm notebook there's another bit called simplified running batch norm we talked a little bit about D biasing last week we'll talk about it more today but as Pegman pointed out something which is kind of obvious in hindsight but I didn't notice at the time which is which is that we had sums divided by D bias and we had count divided by D bias and then we go some divided by account and some divided by D bias divided by account divided by D bias the 2 D biases cancel each other out so we can remove all of them so it's still kind of covered D biasing today for different purpose but actually we didn't really need it for last week so we can remove all the D biasing ended up with something much simpler so that's that's the version that we're going to go with and also thanks to Tom vmon who pointed out that the last step where we went subtract mean divided by standard deviation multiplied by moths add ads you can just rearrange that into this form multivariant sirs and ads minus means times that factor and if you do it this way then you don't actually have to touch X until you've done all of those things and so that's going to end up being faster if you think through the broadcasting operations there then you're doing a lot less computation this way so that was a good idea as well and I should mention tom has been helping us out quite a bit with some of this batch non-staff one of a few people involved in the PI torch community who's been amazingly helpful who I'd like to call out so thanks also to a seumas chinchilla who was one of the original founders of play torch who's been super helpful in sorting some things out for this course and also Francisco masa also super helpful they're both part of the official Facebook engineering team and Tom's not that he does so much work for pi torch she kind of seems like he must be sometimes so thanks to all of you for your great help okay before we moved on to data blocks I wanted to mention one other approach to making sure that your model trains nicely and to me this is the most fast day I ish method I wish I had come up with it but I didn't wonderful researcher named Demetriou came up with it in a paper called all you need is a good in it Demetrio Myshkin and this is the paper and he came up with this technique called LS UV layaway sequential unit variance and the basic idea is this you've seen now how fiddly it is to get your unit variances all the way through your network and little things can change that you know so if you change your activation function or something we haven't mentioned if you add drop out or change the amount of drop out these are all going to impact your the variances of your layer outputs and if they're just a little bit different to one you'll get exponentially worse as we saw through the through the model so the normal approach to fixing this is to think really carefully about your architecture and exactly analytically figure out how to initialize everything so it works and Dimitrios idea which I like a lot better is let the computer figure it out all right and here's how you let the computer figure it out we create our reminisce data set in the same way as before we create a bunch of layers you know with these number of filters like before and what I'm going to do is I'm going to create a conflict which contains our convolution and our Lu and the idea is that we're going to use this because now we can basically say this whole kind of combined Kong plus R Lu has kind of a I'm calling it bias but actually I'm taking that general Rayleigh you and just saying like how much are we subtracting from it and so this is kind of like something we can add or remove and then the weight is just the conf weights and you'll say why we're doing this in a moment basically what we'll do is we'll create our learner and the user way right and however it initializes is fine okay and so so we can train it that's fine that let's try and now train it in a better way so let's recreate our learner and let's grab a single mini batch and here's a function that'll let us grab a single mini batch making sure we're using all our callbacks that the mini batch does all the things we needed to do so here's one mini batch of so here's one mini batch of X and why and what we're going to do is we're going to find all of the modules that are we're going to find all of the modules which are of type conv layer and so is just a little function that that does that and generally speaking when you're working with PI torch modules or with neural nets more generally you need to use recursion a lot because modules can contain modules can contain modules right so you can see here find modules calls find modules to find out all the modules throughout your kind of tree you know because really a module is like a tree modules have modules have modules when so here's our list of all of our comm flares and then what we do is we create a hook right and the hook is just going to grab the mean and standard deviation of a particular module all right and so we can just we confess we'll just print those out and we can see that the means and standard deviations are not zero one that means are too high all right as we know because we've got the rail use and the standard deviations are too low so rather than coming up with our perfect in it instead we just create a loop and the loop calls the model passing in that mini-batch we have right and remember this is well so first of all we we we hook it right and then we call the model in a while loop we check whether the mean the absolute value of the mean is close to zero and if it's not we subtract the mean from the bias and so it just keeps looping through calling the model again with the hawk subtracting from the bias until we get about zero mean and then we do the same thing for the standard deviation keep checking with the standard deviation minus one is nearly zero and as long as it isn't will keep dividing by the standard deviation and so those two loops if we run this function then it's going to eventually then it's going to eventually give us what we want now it's not perfect right the the means are still not quite zero because we do the means first and then the standard deviations and the standard deviation changes will slightly change the mean but you can see our means of percent deviations are perfectly won and their means are pretty close and that's it this is called LS U V and this is how without thinking at all you can initialize any neural network pretty much to get the unit variance all the way through and this is much easier than having to think about whether you've got r lu or ALU or whether you've got dropout or whatever else so here's the super cool trick yeah then we can train it and it transferring Eisley particularly useful for complex and deeper architectures so there's kind of for me the you know the fast a I approach to initializing your neural nets which is no math no thinking just too simple a little for loop in this case a while loop alright so I think we've done enough for them missed because we're getting really good results it's running fast it's looking good so let's try something harder so what are we going to try well went up quite ready to try image net because image net takes quite a lot of time you know a few days if you've got just one GPU to train and that's really frustrating it in an expensive way to try to practice things or learn things or try things out I kept finding this problem of not knowing what data said I should try for my research or from my practice or for my learning you know it seemed like at one end there was m nest which is kind of too easy there was sci-fi 10 that a lot of people use but these are 32 by 32 pixel images and it turns out and this is something I haven't seen really well written about but our research clearly shows it turns out that small images 32 by 32 have very different characteristics to larger images and specifically it seems like once you get burned about 96 by 96 things behave really differently so stuff that works well on so far 10 tends not to work well on normal sized images 32 by 32 is tiny right and stuff that tends to work well on so far 10 doesn't necessarily work well on imagenet there's this kind of gap of like something with Naruto with with normal-sized images which I can train in a sane amount of time but also gives me a good sense of whether something's going to work well or not and actually Demetriou who wrote that LS UV paper we just looked at also had a fantastic paper called systematic evaluation something like systematic evaluation of convolutional neural networks and he noticed that if you use 128 by 128 images with image net then the kind of things that he found works well or not or doesn't work well all of those discoveries applied equally well to the full sized image net it still takes too long 128 by 128 through a 1 and a half or 1.3 million images still too long so I thought that was a good step but I wanted to go even further so I tried creating two new data sets and my two new data sets are subsets of image net and there's kind of like multiple versions in here it really is but they're both subsets of image net they both contain just 10 classes out of the thousand so they're 1/100 of the number of images of image net and I create a number of versions full size 320 pixel size and 160 pixel size one data set is specifically designed to be easy it contains 10 classes that are all very different to each other so this is like my starting point I thought well what if I create this data set then maybe I could train it for like just an epoch or to like just a couple of minutes and see whether something was going to work and then the second one I created was one designed to be hard which is 10 categories are designed to be very similar to each other so they're all dog breeds so the first data set is called image net which is very French as you can hear and there's some you know helpful pronunciation tips here and the second is called image wolf and you can see here I've created a leaderboard for image net and for image wolf and I've discovered that in my very quick experiments with this the exact observations I find about what works well for the full image net also I see the same results here that is also fascinating to see how some things are the same between the two data sets and some are different and I've found working with these two data sets has given me more insight into computer vision model training than anything else that I've done so check them out and I really wanted to mention this to say a big part of getting good at using deep learning in your domain is knowing how to create like small workable useful data sets so once I decided to make this it took me about three hours like it's not at all hard created it you know the data set is a quick little Python script to grab the things I wanted you know how did I decide which ten things I just looked at a list of categories and pick ten things that are new or different how did I decide to pick these things I just looked at ten things that are newer dogs so it's like you know it's a case or like throw something together get it working and you could and and then on your domain area whether it's you know audio or Sanskrit texts or whatever right or genomic sequences try to come up with your version of a toy problem or two which you hope might give insight into your full problem so this has been super helpful for me and if you're interested in computer vision I would strongly recommend trying this out and specifically try to beat me right because trying to beat me and I'm not these are not great they're just okay but trying to beat me will give you a sense of whether the things you're thinking about are in the ballpark of what a moderately competent practitioner is able to do a small amount of time it's also interesting to see with that with like a 1/100 the size of imagenet like a tiny dataset I was able to create a 90% accurate dog breed classifier from random weights so like you can do a lot pretty quickly without much data even if you don't have transfer learning which is kind of amazing so we're going to use this data set now yeah oh it's sorry you had a question so before we look at the data set let's do the question sorry Rachel so just to confirm LSU V is something you run on all the layers once at the beginning not during training what if your batch size is small could you over fit to that batch yeah that's right so you'd run it once at the start of training to initialize your weights just so that that additional set of steps gives you sensible gradients because it's those first two many batches that everything remember how we saw that if we didn't have a very good first few mini batches that we ended up with 90% of the weights being snotty of some of the activations being inactive so that's why we want to make sure we start well and yeah if you've got a small mini batch just run 5 mini batches and take the mean there's nothing special about the one mini batch it's just a fast way to do the computation it's not like we're doing any gradient descent or anything it's just a forward pass thanks that was good question so imaginet is too big to read it all into RAM at once yes it's not huge but it's too big to do that so we're going to need to be able to read it in one image at a time which is going to be true of most of our deep learning projects so we need some way to do that from scratch because that's the rules so let's start working through that process and in the process we're going to end up building a data block API which you're all familiar with but most people using the data block for API feel familiar enough with it to do small tweaks for things that they kind of know they can do but most people I speak to don't know how to like really change what's going on so by the end of this notebook you'll see how incredibly simple the datablock api is and you'll be able to either write your own maybe based on this one or modify the one in first AI because this is a very direct translation of the one that's in first AI with you know so you should be able to get going so the first thing to do is to read in our data and we'll see a similar thing when we build fast AI audio but whatever process you use you're going to have to find some library that can read the kind of data that you want so in our case we have images and there's a library called PIL or pillow place an imaging library which can read images so let's import it we'll grab the data set an entire it import pillow and we want to see what's inside our image net data set typing list X editor is far too complicated for me I just want to type LS so be lazy this is how easy it is to add stuff to the standard library you can just take the class and add a function to it so now we have LS so here's LS so we've got a training and a validation directory within validation we have one directory for each category ok and then if we look at one category we could grab one filename and if we look at one filename we have attention so if you want to know whether somebody is actually a deep learning practitioner shows in this photo if they don't know it's a tench they're lying to you because this is the first category an image net so if you're ever using image net and you know your tennis they're generally being held up by middle-aged men or sometimes they're in Nets that's pretty much how it always looks in image net so that's why we have them in imaginet too because it's such a classic computer vision fish we're cheating and importing none play for a moment just so I can show you what an image contains just to turn it into an array so I can print it for you this is really important it contains bytes okay it contains numbers between 0 and 255 to the integer they're not float so this is what we get when we import when we when we load up an image and it's got a geometry and it's got a number of channels and in this case it's RGB 3 channels okay so we want to have some way to read in lots of images which means we need to know what images there are in this directory structure and in the full image net is going to be 1.3 million of them so I need to be able to do that fast so the first thing we need to know is which things are images so I need a list of image extensions your computer already has a list of image extensions it's your mine mime types database so you can query Python for your mime types database for all of the images so here's a list of the image extensions that my computer knows about so now what I want to do is I want to loop through all the files in a directory and find out which ones are one of these the fastest way to check whether something's in a list is to first of all turn it into a set and of course therefore we need satify so set of five simply checks if it is a set and if it is it makes it 1 otherwise it first turns it into a list and then turns it into a set so that's how we can certify things and here's what I do when I build a little bit of functionality I just throw together a quick bunch of tests to make sure it seems to be roughly doing the right thing and remember in Lesson one we created our own test framework so we can now run any notebook as a test suite so it'll automatically check if we break this at some point ok so now we need a way to go through a single directory and grab all of the images in that so here we can say get files I always like to make sure that you can pass any of these things either a path lib path or a string to make it convenient so if you just say P equals path P if it's already a path Lib object that does doesn't do anything so this is nice easy way to make sure that works so we just go through here's a path Lib object and so you'll see in a moment how we actually grab the list of files but this is our parent directory this is going to be our list of files we go through the list of files we check that it doesn't start with dot if it does that's a UNIX hidden file or Mac hidden file and we also check either they didn't ask for some particular extensions or that the extension is in the list of extensions we asked for so that will allow us to grab just the image files - has something called skander which will grab a path and list all of the files in that path so here is how we can call get files because skander and then we go get files and it looks something like that so that's just for one directory so we can put all this together like so and so this is something where we say for some path give me things with these extensions optionally recurse optionally only include these folder names and this is it okay I will go through it in detail but I just point out a couple of things because being able to rapidly look through files is important the first is that skander is super super fast this is pythons thin wrapper over a C API so this is a really great way to quickly grab stuff and for a single directory if you need to recurse check out OS dot walk this is the thing that uses skander internally to walk through recursively through a folder tree and you can do cool stuff like change the list of directories that it's going to look at and it basically returns all the information that you need it's super great so OS dot walk and OS dot skander are the things that you want to be using if you're playing with directories and files in Python and to be fast we do so he has kept files and so now we can say get files half tench just the image extensions there we go and then we're going to need recurse because we've got a few levels of directory structure so here's recurse so if we try to get all of the filenames we have 13,000 all right and specifically it takes 70 milliseconds to get 13,000 filenames for me to get a look at 13,000 files in windows explorer seems to take about 4 minutes so you know this is like this is unbelievably fast so on the full image net which is a hundred times bigger it's going to be literally just a few seconds so this gives you a sense of how incredibly fast these OS walk and skander functions yes questions are good I've often been confused as to whether the code Jeremy is writing in the notebooks or functionality that will be integrated into into the fast AI library or whether the functions and classes are meant to be written and used by the user interactively and on the fly well I guess it's really a question about what's the purpose of this deep learning from the Foundation's course the different people will get different things out of it but for me it's about demystifying what's going on so that you can take what's in your head and turn it into something real and to do that would be always some combination of using things that are in existing libraries which might be fast AI or PI torch or tensorflow or whatever and partly will be things that aren't in existing libraries and I don't want you to be in a situation where you say well that's not in fast AI therefore I don't know how to do it so really the goal is is this is why it's also called impractical deep learning for coders right is to give you the underlying expertise and tools that you need in practice I would expect a lot of the stuff I'm showing you to end up in the first day I library because that's like literally I'm showing you my research basically this is like my research journal of the last six months and that's what happens is fast AI library is is silver and I take our research and turn it into a library and some of it like this function is pretty much copied and pasted from the existing fast aov one code base because I spent like at least a week figuring out how to make this this fast I'm sure most people can do it faster but I'm slow and it took me a long time and this is what I came up with so yeah I mean it's it's gonna map pretty closely to what's in fast AI already where things are new we're telling you you know like running batch norm is new today we're going to be seeing a whole new kind of optimizer but otherwise things are going to be pretty similar to what's in fast AI so it'll make you be able to quickly hack at first AI v1 and as fastly I changes it's not going to surprise you because you'll know what what's going on sure how does skander compared to glob Skanda should be much faster than glob it's a little more awkward to work with because it doesn't try to do so much it's it's the lowest level thing I I suspect Vlad probably uses it behind the scenes you should try to a time it with glob climate with skander probably depends how you used lob exactly but I remember I used to use glob and it was quite a bit slower and when I say quite a bit you know that for those of you that have been using faster faster eye for a while you might have noticed that the speed at which you can grab the imagenet folder is some orders of magnitude faster than it used to be so it's quite a big difference okay so the reason that fast AI has a data blocks API and nobody else does is because I got so frustrated in the last course at having to create every boss combination of independent and dependent variable that I actually sat back for a while and did some thinking and specifically this is what I did when I thought was I wrote this down there's like water you actually need to do so let's go ahead and do these things so we've already got files we need some way to split the validation set or multiple validation sets out somewhere to do labeling optionally some augmentation transform it to a tensor make it into data into batches optionally transform the batches and then combine the data loaders together into a data bunch and optionally add a test set and so when I wrote it down like that I just went ahead and implemented an API for each of those things to say like okay you can plug in anything you like to that part of the API so let's do it right so so we've already got the basic functionality to get the files so now we have to put them somewhere we've already created that list container all right so we basically can just dump our files into a list container but in the end we what if we actually want is an image list for this one right and an image list when you call so that we're going to have this get method and when you get something from the image list it should open the image so PIL don't original open is how you open an image but we could get all kinds of different objects so therefore we have this superclass which has a get method that you override and by default it just returns whatever you put in there which in this case would be the file name so this is basically all item list does right is it's got a list of items right so in this case it's going to be our file names the path that they came from and then optionally also there could be a list of transforms and transforms is some kind of functions and we'll look at this in more detail in a moment but basically what will happen is when you when you index into your item list remember done to get item does that we'll pass that back up to list container get item and that will return I a single item or a list of items and if it's a single item we'll just call self dot underscore get if it's a list of items we'll call self dot underscore get on all of them and what that's going to do is it's going to call the gap method which in the case of an image list will open the image and then it'll compose the transforms so for those of you that haven't done any kind of more functional style programming compose is just a concept that says go through a list of functions and call the function and replace myself with the result of that and then call the next function and replace myself with the result of that and so forth so in other words a deep neural network is just a composition of functions each layer is a function we compose them all together this compose does a little bit more than most composers specifically you can optionally say I want to order them in some way and it checks to see whether other things have an underscore order key and sorts them and also you can pass in some keyword arguments and if you do it'll just keep passing in those keyword arguments right but it's basically other than that it's a pretty standard function composition function if you haven't seen compose used elsewhere and programming before Google that because it's a super useful concept comes up all the time we use it all the time and as you can see in this case it means action has passed in a list of transforms and this will simply call each of those transforms in turn modifying in this case the image did I had so here's how you create an image list and then here's a method to create an image list from a path and it's just going to call that get files and then that's going to give us a list of files which we will then pass to the class constructor which expects a list of files or a list of something okay so this is basically the same as item list in the stay over in one it's just a it's just a list it's just a list where when you try to index into it it will call something which subclasses over read okay so now we've got an image list we can use it now one thing that happens all the time is you try to create a mini batch of images but one of your images was black and white and when pillow opens up a black and white image it gives you back by default a rank two tensor just the x and y know channel axis and then you can't stack them into a mini batch because they're not all the same shape so what you can do is you can call the pillow dot convert and RGB and if something's not a GB it'll turn it into RGB so here's our first transform so a transform is just a class with an underscore order and then Meck RGB is a transform that when you call it will call convert or you could just do it this way just make it a function both is fine these are both going to do the same thing and this is often the case right you can we stated a bunch of times before you can have done to call or you can have a function so here's our first transform alright and so here's the simplest version is just a function and so if we create a image list from files using our path and plus in that transform then now we have an item list and remember that item list inherits from list container which we gave a dunder repre to so it's going to give us nice printing this is why we create these little convenient things to subclass from because we get all this behavior for free so we can now see that we've got 13,000 items and here's a few of them is the path and we can index into it and when we index into it it calls get and great calls image dot open and pillow automatically displays images in Jupiter and so there it is and this of course is a man with a tan yes thank you okay he looks very happy with it we're going to be seeing him a lot and because we're using the functionality that we wrote last time for list container we can also index with a list of boolean's with a slice with a list of intz and so forth so here's a slice containing one item for instance all right so that's step one step two split validation set so to do that we look and we see here's a path here's the file name is the parent here's the grandparent so here's the grandparents name that's the thing we use to split so let's create a function called grandparents bitter that grabs the grandparents name and you call it telling it the name of your validation set and the name of your training set and it returns true if it's the validation set or false if it's the training set or none if it's neither and so here's something that will create a mask containing your pass at some function so we're going to be using grandparents bitter and it will just grab all the things with that mask is false that's the training set all the things where the mask is true that's a validation set and we'll return them okay so there's our splitter remember we used partial so here's a splitter that splits on grandparents and we're the validation name is Val because that's what it is for imagenet and let's check that that seems to work yes it does we've now got a validation set with 500 things and a training set with 12,800 things so that's looking good so let's use it so split data object is just something with a training data set and a validation set you pass it in you save them away and then that's basically it everything else from here is just convenience right so we'll give it a representation so that you can print it will define done to get attribute so that if you pass it some attribute that it doesn't know about it'll grab it from the training set and then let's add a split by funk method that just calls that split by funk thing we just had there's one trick here though which is we want split by funk to return item lists of the same type that we gave it in this case it would be an image list so we call item list dot new and that's why an our item list we defined something called new and this is a really handy trick pight watch has the concept of a new method as well it says alright let's look at this object let's see what class it is because it might not be item list right it might be image list or some other subclass it has doesn't exist yet and this is now the constructor for that class and it's just pass it in the items that we asked for and then pass in our path and our transforms so new is going to create a new item list of the same type with the same path and the same transforms but with these new items and so that's why this is now going to give us a training set and a validation set with the same path the same transforms in the same type and so if we call spit data split by funk now you can see we've got our training set and our validation set easy so next in our list of things to do is labeling labeling is a little more tricky and the reason it's tricky is because we need processes processes are things which are first applied to the training set they get some state and then they get applied to the validation set for example our labels should not be tench and French horn they should be like zero and two because when we go to do a cross entropy loss we expect to see along they're not a string they're right so we need to be able to map tench to zero or French horn door - we need the training set to have the same mapping as the validation set and for any inference we do in the future is going to have the same mapping as well because otherwise the different data sets are going to be talking about completely different things when they see the number zero for instance so we're going to create something called a vocab and a vocab is just the list saying these are our classes and this is the order they're in zero is tench one is golf-ball two is French horn and so forth so we're going to create the vocab from the training set right and then we're going to convert all those strings into in this using the vocab and then we're going to do the same thing for the validation set but we'll use the training sets vocab right so that's an example of a processor that converts label strings to numbers in a consistent and reproducible ways other things we could do would be processing texts to tokenize them and then you marelize them numerical izing them is a lot like converting the label strings to numbers while taking tabular data and filling the missing values with the median computed on the training set or whatever so most things we do in in this labeling process is going to require some kind of processor okay so in our case we want a processor that can convert label strings to numbers so the first thing we need to know is what are all of the possible labels and so therefore we need to know all the possible unique things in a list so here's some lists here's something that you Nick advise them so that's how we can get all the unique values of something so now that we've got that we can create a processor and a processor is just something that can says some items and so let's create a category processor and this is the thing that's going to create our list of all of the possible categories so basically when you say process we're going to see if there's a vocab yet and if there's not this must be the training set so we're creative vocab and it's just the unique values of all the items okay and then we'll create the thing that goes not from inter to object that goes from object to int so it's the reverse mapping through just enumerate the vocabulary and create a dictionary with reverse mapping so now that we have a vocab we can then go through all the items and process one of them at a time and process one of them simply means look in that reverse mapping we could also do process which would take a bunch of indexes we would use this for example to print out the the inferences that we're doing so we better make sure we got a vocab by now otherwise we can't do anything and then we just do process one for each index and D process one just looks it up in the vocab okay so that's all we need and so with this we can now combine it all together and let's create a processed item list and it's just a list container that contains a processor and the items in it whatever we were given after being processed okay and so then as well as being able to index in it to grab those processed items will also define something called object and that's just the thing that's going to D process the items again okay so that's all the stuff we need to label things so we already know that for splitting we needed the grandparent for labeling we need to parent so here's a parent label okay and here is something which labels things using a function it just calls a function for each thing and so here is our class and we're going to have to pass it some independent variable and some dependent variable and store them away and then we need a indexer to grab the X and grab the Y at those indexes we need a length we may as well make it print out nicely and then we'll just add something just like we did before which does the labeling and passes those two reprocessed item lists to grab the labels and then passes the inputs and outputs to our constructor to give us our label data so that's basically it alright so with that we have a label by function where we can create our category processor we can label the training set we can label the validation set and we can return the result this bit data result so the main thing to notice here is that when we say train equals label data label passing in this processor this processor has no vocab so it goes to that bit we saw it says oh there's no vocab so let's create a list of all the unique possibilities on the other hand when it goes to the validation set proc now does have a vocab so it will skip that step and use the training sets vocab so this is really important right people get mixed up by this all the time in machine learning and deep learning is like very often when somebody says my models no better than random the most common reason is that they're using some kind of different mapping between their training set and their validation set okay so if you use a process like this that's never going to happen because you're ensuring that you're always using the same mapping so you know the details of the code aren't particularly important the important idea is that you your labeling process you know needs to include some kind of processor idea and if you're doing this stuff like manually which basically every other machine learning and deep learning framework does you're asking for difficult to fix bugs because any time your computer's not doing something for you it means you have to remember to do it yourself so whatever framework you're using most trained I don't think I don't know if any other frameworks that's something quite like this so like create something like this for yourself so that you don't have that problem all right let's go in the case of online streaming data how do you deal with having new categories and the test set that you don't see in training yeah I mean great question it's not just online streaming data I mean it happens all the time is you do inference either on your validation set or test set or in production where you know you see something you haven't seen before for labels it's less of a problem in an inference because for inference and inference you don't have labels by definition but you could certainly have that problem in your validation set so what I attempted like to do is if I have like some kind of if I have thing where there's lots and lots of categories and some of them don't occur very often and I know that in the future there there might be new categories appearing I'll take the few least common and I'll group them together into a group called like other and that way I now have some way to ensure that my model can hand all these rare other cases something like that tends to work pretty well but you do have to think of it ahead of time right for many kinds of problems you know that there's a fixed set of of possibilities and if you know that it's not a fixed set yeah I would generally try to create another category with a few examples so make sure you train with things some things in that other category right in the labeled data class what is the class method decorator doing sure so I'll be quick because you can google it but basically this is the difference between an instance method and a class method so you'll see it's not getting passed itself so you'll see that I am NOT I'm not going to call this on an object of type labeled data but I'm calling it on the labeled data class itself so it's just a convenience really class methods the thing that they get passed in is the actual class that was requested so I could create a subclass of this and then ask for that subclass so anyway they called class methods you should google them pretty much every language supports class methods or something like it they're pretty they're pretty convenient you can get away without them they're pretty convenient great so now we've got our labeled list and if we print it out it's got a training set and a validation set and each one has an x and a y our category items are a little less convenient than the first a version one ones because the faster ones will actually print out the name of each category we haven't done anything to make that happen so if we want the name of each category we would actually have to refer to the table which you can see we're doing here why don't or wider objects with a slice okay so in in fast our vision one there's one extra thing we have which is this concept of an of an item base and you can actually define things like category items that know how to print themselves out whether that's that convenience is worth the extra complexity is up to you if you're designing something similar yourself so we still can't train a model with these because we have pillo object we need tensors so here's our labeled list training set zeroth object and that has an x and a y so the zero thing and that topple is the X if they're all going to be in the batch together they have to be the same size so we can just go drop resize no problem I mean that's not a great way to draw it but it's it's a start so here's a transform that resizes things and it has to be after all the other transforms we've seen so far because we want you know conversion to RGB to happen beforehand probably stuff like that so we'll give this an order of 10 and this is something you pass in a size if you pass in an integer we'll turn it into a tuple and when you call it it'll call resize and it'll do bilinear resizing for you so there's a transform once you've turned them all into the same size then we can turn them into tensors I stole this from torch vision this is how to ensure vision turns pillo objects into tenses and this has to happen after the resizing so we'll give this a lesser order and you see there's two ways here of adding kind of class level state or transform level state I can actually attach state to a function this is really underused in Python but it's super handy right we've got a function and we just want to say like what's the order of the function or we can put it in the tree plus and then that's turned into a byte 10 so we actually did a float tensor so here's how you turn into a float and we don't want to be naught between Norton to 55 we put it between naught and 1 so we divide it in place by 255 and that has to happen after it's a byte so we'll give that a higher-order again so now here's our list of transforms doesn't matter what order they're in the array because they're going to order them by the underscore order attribute so we can pass that to our image list we can split it we can label it is little viens to permit the order back again I don't know if you noticed this but into byte tensor I had a commute to 0 1 because pillow has the channel or last where else piped watch the streams the channel comes first so this is just going to pop the channel first so to print them out we have to put the channel last again so now we can grab something from that list and show image there it is and you can see that it is something of a torch thing of this size so that's looking good so we now have tensors that are floats and all the same size so we can train a model so we've got a batch size we'll use to get data load as we had before we can just pass in train and valid directly from our labelled list let's grab a mini batch and here it is 64 by 3 by 128 by 128 and we can have a look at it and we can see the vocab for it we can see the whole mini batch of y-values so now we can create a data bunch it's going to have our data loaders and to make life even easier for the future let's add two optional things channels in and channels out and that way any models that want to be automatically created can automatically create themselves with the correct number of inputs and the correct number of outputs for our data set and let's create add to our split data something called - data bunch which is just this function it just calls that get deals we saw before so like in practice in your you know actual module you would go back and you would paste the contents of this back into your split data definition but this is kind of a nice way when you're just iteratively building stuff you can just you don't just have to you can't only monkey patch PI torch things or standard library things you can monkey patch your own things so here's how you can add something to a previous class when you realize later that you want it okay so let's go through and see what happens so here are all the steps literally all the steps grab the path untie the data grab the transforms grab the item list passing the transforms spit the data using the grandparent using this validation name label it using parent label ah and then turn it into a date abunch with this batch size three channels in ten channels out and we'll use four processes here's our callback functions from last time let's make sure that we normalize in the past we've normalized things that have had only one channel being M list now we've got three channels so we need to make sure that we take the mean over the other channels so that we get a 3 channel mean and a 3 channel standard deviation so let's define a function that normalizes things that are three channels so we're just broadcasting here so here's the mean and standard deviation of this imagenet batch so here's a function called norm imaginet which we can use from now on to normalize anything with this data set so let's add that as a callback using the batch transform we we built earlier we will create a comm net with this number of layers and here's the confident we're going to come back to that and then we will do our one cycle scheduling using Co cycle one cycle annealing cosine one cycle annealing pass that into our gate learn run and train and that's going to give us 72 point six percent which if we look at which if we look at the emerged net leaderboard for 128 pixels 405 epochs the best is eighty four point six so far so this is looking pretty good we're we're very much on the right track so let's take a look and see what model we built because it's kind of interesting there's a few interesting features of this model and we're going to be looking at these features quite a lot in the next two lessons the model knows how big its first layer has to start out because we pass in data and data has a channels in so this is nice already this is a model which you don't have to change its definition if you go if you have hyper spectral imaging with four channels or you have black and white with one channel or whatever alright so this is going to change itself now what's the second layer going to be so or I should say what's the output of the first layer going to be the inputs going to be C in what's the output gonna be this is gonna be 16 32 64 well what we're going to do what we're going to do is we're going to say well our input has we don't know some number of channels right but we do know that the first layer is going to be a 3 by 3 kernel and then there's going to be some number of channels see in channels which in our case is 3 right so as the convolution kernel kind of Scrolls over the input image at each time the number of things that it's multiplying together is going to be 3 by 3 by C in so 9 by C in so remember we talked about this last week right we basically want to put that we basically want to make sure that our first convolution actually has something useful to do right so if we're getting line by C incoming in you wouldn't want more than that going out because it's basically a wasted time okay so we discussed that briefly last week so what I'm going to do is I'm going to say okay let's take that value C in by three by three and let's just look for the next largest number that's a power of two and we'll use that so then that's how I do that and then I'll just go ahead and multiply by two for each of the next two layers so this way we've got the first these vital first three layers are going to work out pretty well so back in the old days we used to use five by five and seven by seven kernels okay we'd have the first layer would be one of those but we just we know now that's not a good idea I'm still most people do it because people stick with what they know but when you look at the bag of tricks for image classification paper which in turn refers to previous many previous citations of in many of which are state of the art and competition winning models the message is always clear three by three kernels give you more bang for your buck you get deeper you end up with the same receptive field it's faster because you've got less work going on right and and really this goes all the way back to the classic Siler and Fergus paper that we've looked at so many times over the years that we've been doing this course and even before that to the vgg paper it really is three by three kernels everywhere so any place you see something that's not a three by three kernel Havok a big think about whether that makes sense okay so that's basically what we have for our first those critical first three layers that's that's where that initial feature representation is happening and then the rest of the layers is whatever we've asked for and so then we can build those layers up to displaying number of filters into number of filters out for each filter and then as usual average pooling flatten and a linear layer to however many classes is in our data okay that's it it's very hard to every time I write something like this I break it the first 12 times and the only way to debug it is to see exactly what's going on to see exactly what's going on you need to see that what module is is there at each point and what is the output shape at each module so that's why we've created this model summary so modal summary is going to use that get batch that we added in the LS UV notebook to grab one batch of data we will make sure that that batch is on the correct device we will use the find module thing that we used in the LS UV to find all of the places that there's a linear layer if you said find or otherwise we will grab just the immediate children we will grab a hook for every layer using the hooks that we made and so now we can pass that model through and the function that we that we've used for hooking simply prints out the module and the outputs shape so that's how easy it is to create this wonderfully useful model summary so to answer your question of earlier and another reason like why are we doing this or what are you meant to be getting out of it is to say like you don't have to write much code to create really useful tools and symmetry you know so we've seen how to create like you know parallel histogram viewers how to create model summaries like with the tools that you have at your disposal now I really hope that you can like dig inside your models what they are and what they're doing and you see that it's all about hooks so this hooks thing we have is just like super super useful now very grateful of the PI torch team for adding this fantastic functionality so you can see here we start the input is 128 because it's that's a batch size 128 128 by 3 they 128 by 128 and then we gradually go through these convolutions the first one has a stride of 1 the next two you have a straight of 2 so that goes 64 32 and you can see after each one they have a straight of 2 gets smaller and smaller and then average pull that to a one by one and then we flatten it and then we have a linear so that's it's a it's a really it's like as basic a confident as you could get it really is it's just a bunch of 3x3 confer about norms but it does terrifically well you know it's pretty it's it's deep enough so I think that's a good start all right I think that's a good time to take a break so let's come back at 7:45 this is one of the bits I'm most excited about in this course actually but hopefully it's going to be like totally unexcited to you because is just going to be so obvious that you should do it this way but the reason I'm excited is that we're going to be talking about optimizers and anybody who's done work with kind of optimizers in deep learning in the past will know that every library treats every optimizer as a totally different thing so there's an atom up too might like imply torch there's an atom optimizer and a SGD optimizer and our miss prop optimizer and the Sun somebody comes along and says hey we've invented this thing called decoupled wait decay also known as atom W and the PI torch folks go oh damn what are we going to do and they have to add a parameter to everyone at their optimizers and they have to change every one of their optimizes and then somebody else comes along and says oh we've invented a thing called AMS grad there's another parameter we have to put into any one of those optimizers and it's not just like inefficient and frustrating but it it holds back research because it starts feeling like there are all these things called different kinds of optimizers but there's not I'm going to show you this not there's one optimizer and there's one optimizer in which you can inject different pieces of behavior in a very very small number of ways and what we're going to do is we're going to start with this generic optimizer and we're going to end up with this this came out last week and it's a massive improvement as you see in what we couldn't do with natural language processing this is the equation set that we're going to end up implementing from the paper and what if I told you that not only I think are we the first library to have this implemented but this is the total amount of code that we're going to write to do it so that's where we're going so we're going to continue with imaginet and we're going to continue with the basic set of transforms we had before and the basic set of stuff to create a data bench this is our model and this is something to pop it on CUDA to get our statistics written out to do a batch transform with the normalization and so we could have start here 52 % after an epoch and so let's try to create an optimizer now in pi torch the base thing called optimizer is just a dictionary that stores away some hyper parameters and we've actually already used it and I deeply apologize for this we cheated we used something that is not part of our approved set of foundations without building it ourselves and we did it here we never wrote program groups we never rope around groups so we're going to go back and do it now right because the reason we did this is because we were using torches Optum optimizer we've already built the kind of the main part of that which is the thing that multiplies by the learning rate and subtracts from the other gradients but we didn't build / em breves so let's do it here so here's what's going to happen as always we need something called 0 grad which is going to go through some parameters and zero them out and also remove any gradient computation history and we're going to have a step function that does some kind of step the main difference here though is our step function isn't actually going to do anything it's going to use composition on some things that we pass on and ask for them to do something so this optimizer is going to do nothing at all until we build on top of it but we're going to set it up to be able to handle things like discriminative learning rates and one cycle annealing and stuff like that and so to be able to do that we need some way to create parameter groups this is what we call in fast AI layer groups and I kind of wish I hadn't called them layer groups I should call them parameter groups because we have a perfectly good name for them already in play torch so I'm not going to call them layer groups anymore I'm just gonna call them parameter groups but it's the same thing okay parameter groups and leg groups so a parameter group so remember what is a in when we say parameters in pi torch remember right back to when we've created our first mini a layer we had a weight tensor and we had a bias tensor and each one of those is a parameter right it's a parameter tensor so in order to optimize something we need to know what all the parameter tensors are in a model and you can just say model dot parameters to grab them all in play torch and that's going to give us it gives this a generator but as soon as you call list on a generator it turns it into a national list so that's going to give us a list of all of the the tenses all of the all of the weights and all of the biases basically but we might want to be able to say you know the the last two layers should have a different learning rate to all the other layers and so the way we can do that is rather than just passing in a list of parameters we'll pass in a list of lists and so let's say a list of lists has two items the first item contains all the parameters in the main body of the architecture and the last item contains just the parameters from the last two layers so if we if we make this decide that this is a list of Lists then that lets us do parameter groups now that's how we tell the optimizer these sets of parameters should be handled differently with discriminative learning rates and stuff and so that's what we're going to do we're going to assume that this thing being passed in is a list of lists well we want quite a few in will check right if it's not then we'll turn it into a list of lists by just wrapping it in a list so if it only has one thing in it we'll just make it a list of with one item containing a list so now per M groups is a list of lists of parameter tensors and so you can either pass in so you could decide how you want to split them up into different parameter groups or you can just have them turn into a single parameter group for you so that's the first thing we need so now we have a optimizer object has a per am Groot attribute containing our parameter groups so just keep remembering that's a list of Lists all right each parameter group can have its own set of hyper parameters so happy' parameters could be learning rate momentum beta in atom epsilon in atom and so forth so those hyper parameters are going to be stored as a dictionary and so there's going to be one dictionary for each parameter group so here's where we created self dot Hyper's contains for each parameter group a dictionary and what's in the dictionary what's in the dictionary is whatever you pass to the constructor okay so this is how you just pass a single bunch of keyword arguments to the constructor and it's going to construct a dictionary for everyone and this is just a way of cloning a dictionary so that they're not all referring to the same reference but they all have their own reference all right so that's doing much the same stuff as torches torches of Kindle optimizer and here's the new bit snipper in order to see what a stepper is let's write one here's a stepper it's a function it's called SGD step what does it do it does the SGD step we've seen it before so in other words to create an SGD optimizer we create a partial with our optimizer with the steppers being SGD step so now when we call step it goes through our parameters composes together our steppers which is just one thing right and calls the parameter so the parameter is going to go P dot data dot add - learning rate P grab data okay so that's how we can create SGD so with that optimization function we can fit it's not doing anything different at all right but what we have done is we've done the same thing we've done a thousand times without ever creating an SGD optimizer it's an optimizer with an SGD step I've created this thing called grab params which is just a little convenience basically when we like zero the gradients we have to go through every parameter to go through every parameter we to go through every parameter group and then within each parameter group we have to go through every parameter in that group where the gradient exists they're the ones that we have to zero and ditto for when we do a step so that's why I just refashioned it okay and also when we call the stepper we want to pass to it all of our hyper parameters right because the stepper might want them like it'll probably want learning right right and learning rates just one of the things that we've listed in our hyper parameters so remember how I said that our compose is a bit special that it passes along any keyword arguments it got to everything that it composes here's a nice way to use that right so that's how comm SGD step can say oh I need the learning rate and so as long as hyper has a learning rate in it it's going to end up here okay and it'll be here as long as you passed it here and then you can change it for each different layer group you can anneal it and so forth so we're going to need to change our parameter scheduler to use our new generic optimizer it's simply now that we have to say go through each hyper parameter in software optional Hyper's and schedule it so that's basically the same as what we had in premature jeweller before but for our new thing and ditto for recorder this used to use per m groups now it uses ibis so minor change to make these keep working so now I was super excited when we first got this working so it's like well we've just built an SGD optimizer that works without ever writing an SGD optimizer so now when we want to add weight decay writes a weight decay remember is the thing where we don't want something that fits this we want something that fits this and the reason we the way we do it is we use ultra regularization which just as we add the summer Squared's weights times some parameter we choose and remember that the derivative of that is actually just wtd times weight so you could either add an l2 regularization to the loss or you can add WD times weight to the gradients if you've forgotten this go back and look at weight decay in part one to remind yourself and so if we want to add either this or this we can do it we can add a stepper so weight decay is going to get an L R and a W D and it's going to simply do that there it is okay or LT regularization is grand adjust to that by the way if you haven't seen this before add in PI torch normally it just adds this tensor to this tensor but if you add a scalar here it multiplies these together first this is a nice fast way to go W D times parameter and add that to the gradient right so there's is that okay so we've got our outrigger ization we've got our weight decay what we need to be able to do now is to be able to somehow have the idea of defaults because we don't want to have to say wait two K equals zero every time we want to turn it off so see how we've attached some state here to our function object so the function now has something something called defaults that says it's a dictionary with WD equals zero so let's just grab the exactly the same optimizer we had before but what we're going to do is we're going to maybe update our defaults with whatever self dot steppers has in there defaults and the reason it's maybe update is that it's not going to replace if you explicitly say I want this weight decay it's not going to update it it'll only update it if it's missing and so that's just what this little loop does right just goes through each of the things and then goes through each of the things on the dictionary and it just checks if it's not there then it updates it so this is now everything else here is exactly the same as before so now we can say let's create an SGD optimizer is just an optimizer with a SGD step and weight decay and so let's create a learner and let's try creating an optimizer which is an SGD optimizer with our models parameters with some learning rate and make sure that the hyper parameter for weight decay should be zero the hyper parameter for LR should be 0.1 yep it passes let's try giving it a different weight decay make sure it's there okay that passes as well so we've now got an ability to basically add any step functions we want and those step functions can have their own state that gets added automatically to our optimization object and we can go ahead and fit so that's fine so now we've got an SGD optimizer with weight decay is one line of code let's now add momentum so momentum is going to require a slightly better optimizer or a slightly different optimizer because momentum needs some more state it doesn't just have parameters and hyper parameters but also momentum knows that for every set of activations it knows what were they updated by last time because remember the momentum equation is if momentum is 0.9 then it would be 0.9 times whatever you did last time plus this step right so we actually need to track for every single parameter what happened last time and that's actually quite a bit of state right if you got 10 million activations in your network you've now got 10 million more floats that you have to store because that's your momentum so we're going to store that in a dictionary called state so a stateful optimizer is just an optimizer that has state and then we're going to have to have some stats and stats are a lot like steppers they're objects that we're going to pass in to say when we create this state how do you create it so how what are you doing mentum what's the function that you run to calculate momentum so that's going to be called a something of a stat plus so for example momentum is calculated by simply averaging the gradient like so we take whatever the gradient average before we multiply it by momentum and we add the current gradient that's the definition of momentum so this is an example of a stat plus so we don't it's not enough just to have update because we actually need this to be something at the start we can't multiply by something that doesn't exist so we also going to define something court in its state that will create a dictionary containing the initial state so that's all that stateful optimizer is going to do right it's going to look at each of our parameters and it's going to check to see whether that parameter already exists in the state dictionary and if it doesn't it hasn't been initialized so we'll initialize it with an empty dictionary and then we'll update it with the results of that in its state core we just saw so now that we have every parameter can now be looked up in this state dictionary to find out its state and we can down therefore grab it and then we can call update like so well this one's on a plane like so to do for example average gradients and then we can call compose with our parameter and our steppers and now we don't just plus in a hyper parameters but we also pass in our state so now that we have average gradients which is sticking it into this thing called grad average and it's going to be passed into our steppers we can now do a momentum step and the momentum step takes not just LR but it's now going to be getting this grad average and here is the momentum step it's just this grant average times the learning rate that's what you do so now we can create an SGD with a mentum optimizer with the line of code it can have a momentum step you can have a weight decay step you can have an average grad stat we can even give it some default weight decay and away we go so here's something that might just blow your mind let me read it to you here is a paper l2 regularization versus batch and weight non batch normalization is a commonly used trick to improve training of deep neural networks and they also use l2 regularization or density lis to prevent overfitting however we show that l2 bicular ization has no regularizing effect brought okay it's true watch this I realized this when I was chatting to Silvia at Europe's and like we're walking around the poster session and I suddenly said to him wait so where if this bitch norm how can L tree regularization possibly work and I've told you what I laid out to him this is before i discovered this paper we've got some layer of activations right and some layer and we've got some weights that was used to create that layer of activations so these are our weights and these are our reservations and then we pass it through some batch norm layer right and the batch norm layer does two things it's got a bunch of ads and it's got a bunch of multiplies right it also normalizes but this is these are the learned parameters okay so we come along and we say okay wait 2k time your weight decay is a million understand because uh-oh what do I do because now the squared of these the sum of the squares of these gets multiplied by 186 my loss functions destroyed I can't possibly learn anything but then the batch norm layer goes oh no don't worry friends and it fills every single one of these with one divided by a million okay so what just happened well no sorry bye it multiplies by positive so it multiplies them all by a million so what now happens oh these now have to be to get the same activations we had before all of our weights so like w1 now have to get divided by a million to get the same result and so now our weight decay basically is nothing so the true so in other words we could just we can decide exactly how much weight decay loss there is by simply using the batch norm alts right now the better-known mulch get a tiny bit of weight decay applied to them unless you turn it off which people often do but it's tiny right because is there's very few parameters here and there's lots of parameters here so it's true it's true l true regularization has no regularizing effect which is not what I've been telling people who have been listening to these lessons the last three years for which I apologise I was wrong I feel a little bit better in knowing that pretty much everybody in the community is wrong we've all been doing it wrong so one van laarhoven mentioned this in the middle of 2017 basically nobody noticed there's a couple more papers I've mentioned in today's lesson notes from the last few months where people are finally starting to really think about this but I'm not aware of any other course which has actually pointed out we're all doing it wrong so you know how to keep mention how none of us know what we're doing we don't even know what l2 regularization does because it doesn't even do anything but it does do something because if you change it something happens so this guy's wrong - it doesn't do nothing so a more recent paper by a team led by Roger gross has found three kind of ways in which maybe regularization happens but it's not the way you think this is one of the papers in the lesson notes but even in his paper which is just a few months old the abstract says basically or the introduction says basically no one really understands what LT regularization does so we we have no idea what we're doing there's this thing that every model ever always has and it totally doesn't work at least it doesn't work in the way we thought it did so that should make you feel better about can I contribute to deep learning obviously you can because none of us have any idea what we're doing and this is a great place to contribute right is like use all this telemetry that I'm showing you the activations of different layers and see what happens experimentally because the people who study this stuff like what actually happens with batch Norman rate decay most of them don't know how to train models right they're like the theory people and then there's the like the practitioners who forget about actually thinking about the foundations at all but if you can combine the two and say like oh let's actually try some experiments let's see what happens really when we change weight decay now that I've assumed we don't know what we're doing I'm sure you can find some really interesting results so momentum is also interesting and we really don't understand much about how things like momentum work but here's some nice pictures for you and hopefully it'll give you a bit of a sense of momentum let's create 200 numbers equally spaced between minus four and four and then let's create another 200 random numbers that average point three and then let's create something that plots some function for these these numbers and we're going to look at this function for each value of something called beta and this is the function we're gonna try plotting and this is the momentum function okay so what happens if we plot this function for each value of bitter for our data where the Y is random and averages 0.3 so beta here is going to be our different values of momentum and you can see what happens is with very little momentum you just get very bumpy very bumpy once you get up to a high momentum you get a totally wrong answer why is this because if you think about it right we're constantly saying 0.9 times whatever we had before plus the new thing then basically you're continuing to say like oh the thing I had before times 0.9 plus a new thing and the things are all above zero right so you end up with a number that's too high and this is why if your momentum is too high and the the basically you're way away from where you need to be in weight space so it keeps on saying go that way go that way go that way if you get that enough with a high momentum it will literally shoot off far faster than is reasonable okay so this will give you a sense of why you've got to be really careful with high momentum it's literally biased to being to end up being a higher gradient than the actual gradient so we can fix that like when you think about it this is kind of dumb right because we shouldn't be saying beta times average plus y I we should be saying beta times average plus one minus beta times the other thing so like dampen the thing that we're adding in and that's quite an exponentially weighted moving average as we know or look in height or speak so let's plot the same thing as before but this time with exponentially weighted moving average ah perfect okay so we're done right mmm not quite what if the thing that we're trying to match isn't just random but is some function so it looks something like this well if we use a very small momentum with exponentially weighted moving averages we're fine and I've added a an outlier at the start just to show you what happens even with beta 0.7 we're fine but uh-oh now we've got trouble and the reason we've got trouble is that the second third fourth fifth observations all have a hull of a lot of this item number one in right because remember item number two is 0.99 times item number one plus 0.01 times item number two right and so item number one is is massively biasing this data even here takes a very long time and the second thing that goes wrong is with this momentum is that you see how we're a bit to the right of where we should be we're always running a bit behind where we should be which makes perfect sense right because we're always only taking point one times a new thing so we can use D biasing and D biasing is what we saw last week and it turned out thanks to stares Beckman's discovery we didn't really need it but we do need it now and D biasing is 2 divided by 1 minus beta to the power of whatever batch number we're up to so you can kind of tell right like if your initial starting point is 0 and that's what we use always when we're debasing we always start at 0 and its beta is 0.9 then your first step is going to be 0.9 times 0 plus 0.1 times your item so in other words you'll end up at 0.1 times your item so you're going to end up 10 times lower than you should be so you need to divide by 0.1 right and if you kind of work through it you'll see that each step is is simply 0.1 ^ 1 2 3 4 5 and so forth and in fact we have of course a spreadsheet showing you this so if you look at the momentum bias spreadsheet there we go so basically here's our batch number and let's say these are the values that are coming in you know our gradients 5 1 1 1 1 1 5 1 then basically this is our exponentially weighted moving average and here is our D biasing correction okay and then here is our resulting D biased exponentially weighted moving average and then you can compare it to an actual moving average of the last few so that's basically how this works and Silva loves writing latex so he wrote all this latex that basically points out that if you say what I just said which is beta times this plus 1 minus beta times that and you keep doing it to itself lots and lots of times you end up with something that they all cancel out to that so this is all we need to do to take care especially weighted moving average divide it by 1 minus beta to the power of I plus 1 and look at that it's pretty good right it deep bias is very quickly even if you have a bad starting point and it looks pretty good it's not magic right but you can see why a beta 0.9 is popular it's kind of got a pretty nice behavior so let's use all that to create Adam so what's Adam Adam is dampened D biased momentum that's the numerator divided by dampened D biased root sum of squared gradients and so we talked about why Adam does that before you were going to the details but here's our average gradient again but this time we've added optional dampening okay so if you say I want dampening then we'll set momentum down pointing to that otherwise we'll set it to 1 okay and then so this is exactly the same as before that with dampening average squared gradients is exactly the same as average gradients we could definitely refactor these a lot so this is all exactly the same as before except we'll call them different things we call it square dampening or call it squared averages and this time rather than just adding in the P grant data we will multiply P Grant data by itself in other words we get the Squared's this is the only difference and we store it in a different name so with those we're also going to need to D by us which means we need to know what step we're up to so here's a stat which just literally counts so here's our D bias function the one we just saw and so here's Adam right once that's in place Adam is just the D bias momentum with momentum dampening the D bias squared momentum with square dimension dampening and then we just take the parameter and then our learning rate and we got the D biasing here gradient average and divided by the squared and we also have a epsilon oh this is in the wrong spot be careful epsilon should always go inside the square root okay so that's an that's an atom step so now we can create an atom optimizer in one line of code and so there's an atom optimizer it has average creds it's got average squared grads it's got a step and we can now try it out okay so here's lamb by the way these equations are a little nicer than these equations and I want to point something out mathematicians hate refactoring don't be like them look at this M over the plus epsilon root landed editor it's the same as this so like it's just so complicated when things appear the same way in multiple places right so when we did this equation we gave that a new name and so now we can just look at our two goes from all that to just that and W T goes from all that to just that and so when you pull these things out when you refactor your math it's much easier to see what's going on so here's the cool thing right when we look at this even if you're a terrible mathematician like me you're going to start to recognize some patterns and that's the trick to being a less terrible mathematician is recognizing patterns beta times something plus one minus beta times another thing is exponentially weighted moving average right so here's one exponentially weighted moving average is another exponentially weighted moving average this one has a gradient this one has gradient squared this means element wise multiplication so these are the exponentially weighted moving average of the gradient and the gradients grid o beta to the T D biasing so that's the D biased version of M there's the D biased version of V naught to move the epsilon really Vaughn has a message Jennifer has a message don't move the epsilon don't listen to Jeremy don't listen to Jerry okay so that's an actual method I know an atom the epsilon goes outside the square root no way anyway so epsilon should always go inside the square root Jeremy just undid a fix I pushed a week ago where our atom wasn't working let's press ctrl-z a few times there we go that's great so to explain why this matters and why like there is no right answer here's the difference right so if epsilon is 1 in x Evan then having it here versus having it here so like the square root of the square root of 1 in X 7 is very different to one in X 7 and in batch norm they do put it inside the square root and according to Sylvia and Adam they neither is like the right place to put it or the wrong place to put it if you don't put it in the same place as they do in the paper it's just a totally different number and this is a good time to talk about Epsilon and Adam because I love epsilon and Adam because like what if we put made epsilon equal to 1 right then we've got the kind of momentum eyes the kind of momentum term on the numerator and the denominator we've got the squared sum root summer Squared's of the root of the exponentially weighted average squared gradients so we're dividing by that plus 1 and most of the time the gradients are going to be smaller than 1 and the squared version is going to be much smaller than 1 so basically then the 1 is going to be much bigger than this so basically makes this go away so if epsilon is 1 it's pretty close to being standard SGD with mentum or at least d biased dampened mentum where else is epsilon is 1 in X 7 then we're basically saying oh we want to really use these these different exponentially weighted moving average squared gradients and this is really important because if you have some activation that has had has had a very small squid gradients for a while this could well be like 1 in X 6 which means when you divide by it you're multiplying by a million and that could absolutely kill your optimizer so the trick to making Adam and Adam like things work well is to make this about point 1 somewhere between 1 a neg 3 and 1 a neg 1 tends to work pretty well most people use one in X 7 which it just makes sets like there's no way that you want to be able to multiply your step by 10 million times that guy's just never going to be a good idea so it's another place that epsilon is a super important thing to think about okay so Lam then is is stuff that we've all seen before right so it's D biased this is this is Adam right D biased exponentially weighted moving averages of gradients and gradient squared this here is the norm of the weights and it's the norm is just the sum of the squares roots on the squares so this is just weight decay so a lamb has weight 2 K built in this one here hopefully you recognize as being the atom step and so this is the norm of the atom step so basically what lamb is doing is its atom but what we do is we average all the steps over a whole layer right that's why these ELLs are really important because these things are happening over Aleya and so basically we're taking so here's a deeper cementum deeper squared momentum right and then here's our 1 and look here's this mean right so it's for a layer because remember each stepper is created for a layer for a parameter didn't say layer for a parameter okay so this is kind of like both exciting and annoying because I had been working on this exact idea which is basically Adam but averaged out over a layer for the previous week and then this lamb paper came out and I was like oh that's cool some paper about Bert training I'll check it out and there's like oh we do it with a new optimizer and I look at the new optimized it's like it's just the optimizer I wrote a week before we were going to present it so you know I'm thrilled that this thing exists I think it's exactly what we need and you should definitely check out lamb because it makes so much sense to to to use the average over the layer of that step as a kind of a kind of a you can see here it's kind of got this normalization going on because it's just really unlikely that every individual parameter in that tensor you don't want to divide it by its squared gradients because it's going to vary too much this is too much chance that there's going to be a one a neg seven in there somewhere or something right so this to me is exactly the right way to do it and this is kind of like the first optimizer I've seen where I just kind of thing like Oh finally I feel like people are heading in the right direction but what do you really study this optimizer you'll realize that everything we thought about optimizes kind of doesn't make sense the way optimizers are going with things like lamb is is the whole idea of like what what is the magnitude of our step it just looks very different to everything we reckoned of thought of before so check out this paper you know this Beth might look slightly intimidating at first but now you know all of these things you know what all they all are and you know why they exist so I think you'll be fine so here's how we create a lamb optimizer and here's how we fit with it okay that is that unless silver says otherwise alright so as I was building this I just I got so sick of Rena because I kept on wondering when do I pass a runner when do I pass a learner and then I kind of suddenly thought like again like once every month or two I actually sit and think and it's only when I get really frustrated right so like I was getting really frustrated with runners and I actually decided to sit and think and I looked at the definition of and I thought wait it doesn't do anything at all it stores three things what kind of class just doors three things and then a runner has a learner in it that stores three things like why don't we store the three things in the runner so I talked the runner I took that line of code I copied it and I pasted it just here I then renamed runner to learner I then found everything that said self-taught man and removed the dot learn and I was done and now there's no more Runner and it's like ah it's just one of those obvious refactorings that as soon as I did it Silvan was like why didn't you do it that way the first place and I was like why don't you fix it that way in the first place but now that we've done it like this is so much easier there's no more get learn round there's no more having to match these things together it's it's just super simple so one of the nice things I like about this kind of Jupiter style of development is I you know I spend a month or two just like immersing myself in the code in this very experimental way and I feel totally fine throwing it all away and changing everything because like everything's small and I can like fiddle around with it and then after a couple of months you know silver and I you'll just kind of go like okay there's a bunch of things here that worked nicely together and we turn it into her some modules and so that that's how first AI version one happened and people often say to us like ah turning it into modules what a nightmare that must have been so here's what was required for me to do that I typed into Skype silver please turn this into a module so that was pretty easy and then three hours later still there type back and he said done it was three hours of work you know it talked you know it was four four five six months of development in notebooks three hours to convert it into modules so it's really it's not it's not a hassle and I think there's a find this quite delightful it works super well so no more runner thank God rounder is now called Alona a kind of back to where we were we want progress bars so so when I wrote this fantastic package called fast progress which you should totally check out and we're allowed to import it because remember we're allowed to import modules that are not data science modules progress bar is not a data science module but now we need to attach this progress bar to our core back system so let's grab our imaginet data as before create a little thing with I don't know 432 filled layers let's rewrite our stat scroll back it's basically exactly the same as it was before except now we're storing our stats in an array okay and we're just passing off the array to logger remember logger is just a print statement at this stage and then we will create our progress bar call back and that is actually the entirety of it that's all we need so with that we can now add progress call back to our callback functions and grab our learner know runner fit now that's kind of magic right that's all the code we needed to make this happen and look at the end oh okay it's a nice little table it's pretty good so this is you know thanks to just careful simple decoupled software engineering we just said okay when you start fitting you've got to create the master bar so that's the thing that tracks the epochs and then tell the master bar we're starting and then replace the logger function not with print but with master Bard right so it's going to print the HTML into there and then after we've done a batch update our progress bar when we begin an epoch or begin validating we'll have to create a new progress bar and when we're done fitting until the master bar we're finished that's it all right so it's very easy to once you have a system like this to integrate with other libraries if you want to use tensor board or wisdom or send yourself Twilio message or whatever right it's super easy okay so we're gonna finish I think we're gonna finish unless what this goes faster than I expect with data augmentation so so far we've seen how to create their optimizers we've seen how to create our data blocks API and we can use all that to train a reasonably good image net model but to make a better image net model it's a bit short of data so we should use data augmentation as we all know now so let's load it in as before and let's just grab an image list for now okay and the only transforms we're going to use ribs resize fixed and here's our chap with attend and let's just actually open the original pillow image without resizing it to see what he looks like full size so here he is alright and I want to point something up when you resize there are various resampling methods you can use so basically when you go from one size image to another size image to you like take the pixels and take the average of them or do you put a little cubic spline through them or or what and so these are called resampling methods and pillow has a few they suggest when down sampling so going from big to small you should use anti-alias so here's what you do when you're augmenting your data and this is like nothing I'm going to say today you should is really focused on vision if you're doing audio if you're doing text if you're doing music whatever augment your data and look at or listen to or understand your augmented data so don't like just check this into a model but like look at what's going on so if I want to know what's going on here I need to be able to see the texture of this tench now I'm not very good at ten shoes but I do know a bit about clothes so let's say if we were trying to see what this guy's wearing it's a checkered shirt all right so let's zoom in and see what this guy's wearing I have no idea all right the checkered shirts gone so like I can tell that this is going to totally break my model if we use this kind of image augmentation so let's try a few more what if instead of anti-aliasing we use bilinear which is the most common no I still don't know what he's wearing okay what if we use nearest neighbors which nobody uses because everybody knows it's terrible oh it totally works so yeah like just look at stuff and and and try and find something that you can study to see whether it works here's something interesting though this looks better still don't you think and this is interesting because what I did here was I did two steps I first of all resized to 256 by 256 with bicubic and then I resize to my final 128 by 128 with nearest neighbors and so sometimes you can like combine things together in steps to get really good results anyway I don't want to go into the details here I'm just saying that when we talk about image augmentation your test is to look at or listen to or whatever your augmented data so resizing is very important for vision flipping is a great data augmentation for vision I don't particularly care out flipping the main thing I want to point out is this at this point our tensors contain bytes calculating with bytes and moving bytes around is very very fast and we really care about this because like when we were doing the dawn bench competition one of our biggest issues for speed was getting our data augmentation running fast enough and doing stuff on floats is slow if you're flipping something flipping bytes is identical to flipping floats in terms of the outcome so you should definitely do your flip while it's still a byte all right so image augmentation isn't just about like drawing some transformation functions in there but think about when you're going to do it because you've got this pipeline where you start with bytes right and you start with like bytes in a pillow thing and then they become bytes in a tensor and then they become floats and then they get turned into a batch that where are you going to do the work and so you should do whatever you can well this to your bytes but be careful don't do things that like are going to cause rounding errors or saturation problems whatever but flips definitely good so let's do our flips so there's a thing called pil X dot transpose PA or image flip left right let's check it for random numbers less than 0.5 let's create an item list and let's replace that like we build this ourselves so we know how to do this stuff now let's replace the I the items with just the first item with 64 copies of it and so that way we can now use this to create the same picture lots of times so a show batch is just something that's just going to go through our batch and show all the images everything we're using we've built ourselves so you never have to wonder what's going on so we can show batch with no augmentation or remember how we created our transforms we can add prel random flip and now some of them are backwards it might be nice to turn this into a class that you actually pass a pea into to decide what the probability of a flip is you probably want to give it an order because we need to make sure it happens you know after we've got the image and after we've converted it to RGB but before we've turned it into a tensor since all of our pil transforms are going to want to be that order we maize will create a pil transform plus and give it that order and then we can just inherit from that class every time we want a pil transform so now we've got a pil transform class we put a Pio random flip it's got this state it's going to be random we can try it out giving it P of 0.8 and so now yep most of them are flipped okay or maybe we want to be able to do all these other flips so actually pil transpose you can pass it all kinds of different things and they're basically just numbers between norton's between naught and 6 so here are all the options so let's turn that into another transform where we just pick any one of those at random and there it is so this is how we can do data-orientation all right now's a good time it's easy to evaluate data augmentation for images how would you handle tabular text or time series a text you read it yeah you would read the Augmented text so if you're renting text then you read the Augmented text for time series you would you know look at the signal of the time series or tabular you would graph or however you normally visualize that kind of tabular data you would visualize that tabular data in the same way so you just kind of come and trend as a domain expert hopefully you understand your data and you have to come up with a way you know what are the ways you normally visualize that kind of data and you use the same thing for your augmented data make sure it makes sense yeah make sure it seems reasonable sorry I think I misread how would you do the augmentation for tabular data at air times so how would you do the augmentation I mean again it kind of requires your domain expertise so just before class today actually one of our alumni Christine pain came in she's at open AI now working on music analysis of music generation and she was saying talking about her dad or augmentation saying she's like pitch shifting and volume changing and slicing bits off the front of the end and stuff like that so there isn't a an answer you know it's just a case of thinking about like Oh what kinds of things could change in your data that would almost certainly cause the label to not change but would still be a reasonable data item and that just requires your domain expertise oh except for the thing I'm going to show you next which is going to be a magic trick that works for everything so we'll come to that okay we can do random cropping and this is again something to be very careful of if we very often want to grab a small piece of an image and zoom into that piece it's a great way to do data augmentation one way would be to crop and then resize and if we do crop and resize oh we've lost his check shirt but very often you can do both in one step so for example with pillow there's a transform called extent where you tell it what crop and what resize and it does it in one step and now it's much more clear right so generally speaking you've got to be super careful particularly when your data is still bytes not to do destructive transformations of particularly multiple destructive transformations do them all in one go or wait until their floats right because bytes round off and disappear will saturate where else floats don't and the cropping one takes 193 microseconds the better one takes 500 microseconds so one approach would be to say oh crap it's more than twice as long we're screwed but that's not how to think how to think is what's your time budget does it matter so here's how I thought through our time budget for this little augmentation project I know that for Dawn bench our kind of the best we could get down to is 5 minutes per batch of imagenet on 8 GPUs and so that's 1.25 million images so that's on one GPU per minute that's 31,000 or 500 per second assuming 4 cores per GPU that's 125 per second so we're going to try to stay under 10 milliseconds I said 10 milliseconds per image I think I mean 10 milliseconds per batch so it's it's actually still a pretty small number right so we're not too worried at this point about 500 microseconds but this is always kind of a thing to think about is like how how much time if you got right and sometimes these times really add up but yeah 520 per second we've got some time especially since we've got a few normally a few calls per GPU so we can just write some code to do a kind of a general crop transform for image net and things like that for the validation set what we normally do is we find is we grab the center of the image we remove 14% from each side and grab the center so we can zoom in a little bit so we have a center crop so here we show all that that's what we do for the validation set and obviously they're all the same because validation set doesn't have the randomness but for the training set the most useful transformation by far like all the competition winners grab a small piece of the image and zoom into it this is called a random resize crop and this is going to be really useful to know about for any domain so for example in NLP really useful thing to do is to grab different size chunks of contiguous text with audio if you're doing speech recognition grab different sized pieces of the utterances and so forth right can find a way to get different slices of your data it's a fantastically useful data augmentation approach and so this is like by far the main most important organ tation used in every imagenet winner for the last six years or so it's a bit weird though because what they do in this approach is this little ratio here says squish it by between 3 / 4 aspect ratio to a 4 over 3 aspect ratio and so it literally makes the the person see here he's looking quite thin and see here he's looking quite wide it doesn't actually make any sense this transformation because like optically speaking there's no way of like looking at something in normal day-to-day life that causes them to like expand outwards or contract inwards right so when we looked at this we thought I think what happen here is that they were this is the best they could do with the tools they had but probably what they really want to do is to do the thing that's kind of like physically reasonable and so the physically reasonable thing is like you might be a bit above somebody or a bit below somebody or left of somebody or right of somebody causing your perspective to change so our guess is that what we actually want is not this but this so perspective warping is basically something that looks like this you basically have four points right and you think about how would those four points map to four other points if they were going through some angles it's like as you look from different directions roughly speaking and the reason that I really like this idea is because when you're doing data augmentation at any domain as I mentioned the idea is to try and create like physically reasonable in your domain inputs and these just aren't like you can't make somebody squish yeah in real world right but you can shift there active so if we do a perspective transform okay then they look like this and this is true right if they're if you're a bit underneath them the fish will look a bit closer or if you're a bit over here then the hats a bit closer from that side so these perspective transforms make a lot more sense right so if you're interested in perspective transforms we have some details here on how you actually do the mathematically the details aren't important but what are interesting is the transform actually requires solving a system of linear equations and did you know that apply torch has a function for solving systems of linear equations it's so amazing how much stuff is in pi torch right so for like lots of the things you're needing your domain it might be surprised to find what's already there questioner and with the cropping and resizing what happens when you lose the object of interest so and the fish has been cropped out that's a great question it's not just a fish it's a tench yeah so so there's no ten shear and so these are noisy labels and interestingly the the the kind of image net winning strategy is to randomly pick between 8% and 100% of the pixels so that literally they are very often picking eight percent of the pixels and that's the image net winning strategy so they're very often have no tension so or very often they'll have just the fin or just the eye so this tells us that if we want to use this really effective augmentation strategy really well we have to be very good at handling noisy labels which we're going to learn about in the next lesson all right and it also hopefully tells you that if you already have noisy labels don't worry about it all of the research we have tells us that we can handle labels where the thing's totally missing or sometimes it's wrong as long as it's not biased so yeah it's okay you know one of the things that'll do is it'll learn to find things associated with attention so if there's a middle-aged man looking very happy outside could well be attention okay so this is a bit of research that we're currently working on and hopefully I have some results to show you soon but our view is that this this image whopping approach is probably going to give us better results than the traditional image net style augmentations so here's our final transform for tilting in arbitrary directions and here's the result not bad so a couple of things to finish on now the first is that it's really important to measure everything and I and many people have been shocked to discover that actually the time it takes to convert an image into a float tensor is significantly longer than the amount of time it takes to do something as complicated as a walk so like you may be thinking like this image warping thing sounds really hard and slow but be careful right just converting bytes to floats is really hard and slow and then this is the one as I mentioned this one we're using here is the one that comes from porch vision we found another version that's like twice as fast which goes directly to float so this is the one that we're going to be using so time everything if you if you're running you know things are running not fast enough okay here's the thing I'm really excited about for augmentation is this stuff's all still too slow what if I told you you could do arbitrary affine transformations so warping zooming rotating shifting at a speed which would compare this is the normal speed this is our speed so up to like you know an order of magnitude or more faster how do we do it we figured out how to do it on the GPU so we can actually do augmentation on the GPU and the trick is that pi torch gives us all the functionality to make it happen so the key thing we have to do is to actually realize that our transforms our augmentation should happen after you create a batch so here's what we do for our elven tation we don't create one random number we create a mini batch of random numbers which is fine because play torch has the ability to generate batches of random numbers on the GPU and so then once we've got a mini batch of random numbers then we just have to use that to generate a mini batch of augmented images I won't kind of bore you with the details well as I find them very interesting details but if you're not a computer vision person maybe not but basically we create something called an FIM grid which is just a the coordinates of where is every pixel so like literally his coordinates from minus 1 to 1 and then what we do is we multiply it by this matrix which is called an affine transform and there are various kinds of affine transforms you can do for example you can do a rotation transform by using this particular matrix but these are all just matrix multiplications and then you just as you see here you just do the matrix multiplication and this is how you can rotate so a rotation believe it or not is just a matrix multiplication by this untimely row matrix if you do that normally it's going to take here about 17 milliseconds because peed it up a bit with own some ok or we could speed it up a little bit more with batch matrix multiply or we could stick the whole thing on the GPU and do it there and that's going to go from 11 milliseconds to any one micros seconds all right so if we can put things on the GPU it's totally different right and suddenly we don't have to worry about how long I augmentations taking so this is the thing that actually rotates the coordinates to say where the coordinates are now then we have to do the interpolation and believe it or not hi torch has an optimized batch wise interpolation function it's called grid sample and so here it is we run it there it is and not only do they have a good sample but this is actually even better than pillows because you don't have to have these black edges you can say padding mode equals reflection and the black edges are gone it just reflects what was there which most of the time is better and so reflection padding is one of these little things we find definitely helps models so now we can put this all together into a root rotate batch now we can do any kind of coordinate transform here one of them is rotate batch to do it a batch at a time and yeah as I say it's it's dramatically faster or in fact we can do it all in one step because apply torch has a thing called a fine grid that will actually do the multiplication as it creates a coordinate grid and this is where we get down to this incredibly fast speed so I feel like there's a whole you know big opportunity here there there are currently no kind of hackable anybody can write their own augmentation run on the GPU libraries out there the entire faster i dot vision library is written using pi torch tensor operations we did it so that we could eventually do it this way but currently they all run on the CPU one any image at a time but this is our template now so now you can do them a batch at a time and so whatever domain you're you're working in you can you can hopefully start to try out these you know randomized GPU batch wise orientations and next week we're going to show you this magic data augmentation called mix-up that's going to work on the GPU it's going to work on every kind of domain that you can think of and and will possibly may make most of these are relevant because it's so good you possibly don't need any others so we're that and much more next week we'll see you then [Applause]