{
  "00:00": "um welcome everybody to lesson 5 and so",
  "00:05": "we have officially peaked and everything",
  "00:09": "is down here from here as of halfway",
  "00:13": "through the last lesson we started with",
  "00:18": "computer vision because it's the most",
  "00:23": "mature kind of out-of-the-box ready to",
  "00:26": "use deep learning application it's",
  "00:30": "something which if you're not using deep",
  "00:32": "learning you won't be getting good",
  "00:34": "results so the difference you know",
  "00:35": "hopefully between not during lesson one",
  "00:38": "versus doing less than one you've gained",
  "00:40": "a new capability you didn't have before",
  "00:42": "and you kind of get to see a lot of the",
  "00:47": "kind of tradecraft of training and",
  "00:51": "effective neural net and so then we",
  "00:53": "moved into NLP because text is kind of",
  "00:58": "another one which you really kind of",
  "01:00": "can't do really well without deep",
  "01:02": "learning generally speaking and it's",
  "01:06": "just got to the point where it's pretty",
  "01:08": "you know works pretty well now in fact",
  "01:10": "the New York Times just featured an",
  "01:12": "article about the latest advances in",
  "01:14": "deep learning for text yesterday and",
  "01:17": "talked quite a lot about the work that",
  "01:19": "we've done in that area along with open",
  "01:22": "AI and Google and Allen Institute of",
  "01:26": "artificial intelligence and then we've",
  "01:32": "kind of finished our application journey",
  "01:35": "with tabula and collaborative filtering",
  "01:39": "partly because tabular and collaborative",
  "01:42": "filtering of things that you can still",
  "01:44": "do pretty well without deep learning so",
  "01:47": "it's not such a big step it's not a kind",
  "01:49": "of whole new thing that you could do",
  "01:50": "that you couldn't used to do and also",
  "01:52": "because the you know we're going to try",
  "01:58": "to get to a point where we understand",
  "01:59": "pretty much every line of code and the",
  "02:01": "implementations of these things and the",
  "02:03": "implementations of those things it's",
  "02:05": "much less intricate than",
  "02:08": "vision and NLP so as we come down this",
  "02:12": "other side of the journey which is like",
  "02:14": "all the stuff we've just done how does",
  "02:16": "it actually work by by starting where we",
  "02:19": "just ended which is starting with",
  "02:21": "collaborative filtering and then tabular",
  "02:23": "data",
  "02:23": "we're going to be able to see what all",
  "02:25": "those lines of code do by the end of",
  "02:28": "today's lesson that's our goal so",
  "02:33": "particularly this lesson you should not",
  "02:35": "expect to come away knowing how to solve",
  "02:39": "you know how to do applications you",
  "02:41": "couldn't do before but instead you",
  "02:43": "should have a better understanding of",
  "02:44": "how we've actually been solving the",
  "02:46": "applications we've seen so far",
  "02:48": "particularly we're going to understand a",
  "02:51": "lot more about regularization which is",
  "02:53": "how we go about managing over versus",
  "02:56": "under fitting and so hopefully you can",
  "02:58": "use some of the tools from this lesson",
  "02:59": "to go back to your previous projects and",
  "03:02": "get a little bit more performance or",
  "03:05": "handle models where previously maybe you",
  "03:07": "felt like your data was not enough or",
  "03:10": "maybe your underfitting",
  "03:11": "and so forth and it's also going to lay",
  "03:14": "the groundwork for understanding",
  "03:15": "convolutional neural networks and",
  "03:17": "recurrent neural networks that will do",
  "03:19": "deep dives into in the next two lessons",
  "03:21": "and as we do that we're also going to",
  "03:23": "look at some new applications some new",
  "03:25": "vision and NLP applications let's start",
  "03:34": "where we left off last week do you",
  "03:41": "remember this picture so this picture we",
  "03:47": "were looking at kind of waters a deep",
  "03:51": "neural net look like and we had various",
  "03:57": "layers and the first thing we pointed",
  "04:01": "out is that there are only and exactly",
  "04:05": "two types of layer there are layers that",
  "04:09": "contain parameters and there are layers",
  "04:12": "that contain activations parameters the",
  "04:17": "things that your model",
  "04:20": "learns they're the things that you use",
  "04:22": "gradient descent to go parameters -",
  "04:28": "equals learning rate times parameters",
  "04:31": "grad okay that's our basic that's what",
  "04:35": "we do okay and those parameters are used",
  "04:40": "by multiplying them by input activations",
  "04:44": "doing a matrix product so the yellow",
  "04:48": "things are our weight matrices your",
  "04:52": "weight tensors more generally but that's",
  "04:55": "plus enough so we take some input",
  "04:57": "activations or some layer activations",
  "04:59": "and we multiply it by weight matrix to",
  "05:03": "get a bunch of activations so",
  "05:05": "activations numbers that these are",
  "05:09": "numbers that are calculated okay so I",
  "05:12": "find in our study group I keep getting",
  "05:14": "questions about where does that number",
  "05:15": "come from and I always answer it in the",
  "05:18": "same way you tell me is it a parameter",
  "05:19": "or is it an activation because it's one",
  "05:22": "of those two things okay that's where",
  "05:24": "numbers come from",
  "05:25": "I guess inputs a kind of a special",
  "05:27": "activation so they're not calculated",
  "05:30": "they're just there so maybe that's a",
  "05:33": "special case so maybe it's an input or a",
  "05:35": "parameter or an activation activations",
  "05:40": "don't only come out of matrix",
  "05:41": "multiplications",
  "05:42": "they also come out of activation",
  "05:44": "functions and the most important thing",
  "05:46": "to remember about an activation function",
  "05:47": "is that it's an element-wise function so",
  "05:51": "it's a function that is applied to each",
  "05:52": "element of the input that evasions in",
  "05:55": "turn and creates one activation for each",
  "05:58": "input element so if it starts with a",
  "06:00": "twenty long vector it creates a twenty",
  "06:03": "long vector by looking at each one of",
  "06:05": "those in turn doing one thing to it and",
  "06:08": "spitting out the answer okay so an",
  "06:09": "element-wise function value is the main",
  "06:13": "one we've looked at and honestly it",
  "06:16": "doesn't too much matter which you pick",
  "06:18": "so we don't spend much time talking",
  "06:20": "about activation functions because if",
  "06:21": "you just use RAL you you'll get a pretty",
  "06:23": "good answer pretty much all the time and",
  "06:27": "so then we learnt that this combination",
  "06:29": "of matrix multiplications followed by",
  "06:32": "values",
  "06:33": "stack together has this amazing",
  "06:35": "mathematical property called the",
  "06:37": "universal approximation theorem which is",
  "06:39": "if you have big enough weight matrices",
  "06:41": "and enough of them it can solve any",
  "06:45": "arbitrarily complex mathematical",
  "06:47": "function to any arbitrarily high level",
  "06:51": "of accuracy assuming that you can train",
  "06:55": "the parameters both in terms of time and",
  "06:58": "data availability and so forth okay so",
  "07:03": "that's the bit which I find particularly",
  "07:07": "more advanced computer scientists get",
  "07:10": "really confused about is they're always",
  "07:12": "asking like where's the next bit what's",
  "07:15": "the trick how does it work but that's it",
  "07:18": "you know you just do those things and",
  "07:20": "you pass back the gradients and you",
  "07:24": "update the weights with the learning",
  "07:25": "rate and that's it so that piece where",
  "07:29": "we take the loss function between the",
  "07:34": "actual targets and the output of the",
  "07:39": "final layer for the final activations we",
  "07:42": "calculate the gradients with respect to",
  "07:44": "all of these yellow things and then we",
  "07:46": "update those yellow things by learning",
  "07:48": "rate by subtracting learning rate times",
  "07:50": "the gradient that process of calculating",
  "07:53": "those gradients and then subtracting",
  "07:54": "like that is called back propagation",
  "07:57": "okay so when you hear the term well",
  "08:01": "that's this very small fun so when you",
  "08:05": "see when you hear the term back",
  "08:09": "propagation it's one of these terms that",
  "08:14": "neural networking folks love to use it",
  "08:16": "sounds very impressive okay that you can",
  "08:19": "replace it with your head with weights",
  "08:23": "minus equals weight start grad times",
  "08:26": "learning rate or parameters I should say",
  "08:28": "rather than weights a bit more general",
  "08:30": "okay",
  "08:31": "so that's what we covered last week and",
  "08:35": "then I mentioned last week that we're",
  "08:37": "going to cover a couple more things I'm",
  "08:41": "going to come back to these ones cross",
  "08:42": "entropy and softmax later today let's",
  "08:45": "talk about fine-tuning",
  "08:47": "now so what happens when we take a res",
  "08:50": "net 34 and we do transfer learning",
  "08:54": "what's actually going on so the first",
  "08:56": "thing to notice is there is net 34 that",
  "08:59": "that we grabbed from image net has a",
  "09:03": "very specific weight matrix at the end",
  "09:05": "it's a weight matrix that has 1000",
  "09:08": "columns why is that because image net",
  "09:11": "the problem they asked you to solve in",
  "09:13": "the image net competition is please",
  "09:15": "figure out which one of these 1000 image",
  "09:18": "categories this picture is so that's why",
  "09:21": "they need a thousand things here because",
  "09:23": "an image net this target vector is",
  "09:26": "length a thousand it's a you've got to",
  "09:29": "pick the probability that it's which one",
  "09:32": "of those thousand things so there's a",
  "09:37": "couple of reasons this weight matrix is",
  "09:39": "no good to you when you're doing",
  "09:40": "transfer learning the first is that you",
  "09:43": "probably don't have a thousand",
  "09:44": "categories you know I was trying to do",
  "09:45": "teddy bears black bears or brown bears",
  "09:47": "so I don't want a thousand categories so",
  "09:50": "the second is even if I did have it",
  "09:51": "exactly a thousand categories they're",
  "09:52": "not the same thousand categories that",
  "09:54": "are in image net so basically this whole",
  "09:57": "weight matrix is a waste of time for me",
  "10:00": "so what do we do we throw it away so",
  "10:04": "when you go create CNN in first AI it",
  "10:06": "deletes that and what does it do instead",
  "10:09": "instead it puts in two new weight",
  "10:13": "matrices in there for you with a rel you",
  "10:20": "in between and so there are some",
  "10:26": "defaults as to what size this first one",
  "10:30": "is but the second one the size there is",
  "10:34": "as big as you need it to be so in your",
  "10:37": "data bunch which you passed your learner",
  "10:40": "from that we know how many activations",
  "10:43": "you need if you're doing classification",
  "10:45": "it's wherever many classes you have if",
  "10:48": "you're doing regression it so if in many",
  "10:50": "numbers you're trying to predict in the",
  "10:52": "regression problem and so remember that",
  "10:53": "in your if your data bunch is called",
  "10:55": "data that'll be called data dot C so",
  "10:58": "we'll add for you",
  "10:59": "the",
  "11:00": "weight matrix of size dado C by however",
  "11:03": "much was in the previous layer okay so",
  "11:10": "now we need to train those because",
  "11:11": "initially these weight matrices are full",
  "11:14": "of random numbers okay because new",
  "11:17": "weight matrices are always full of",
  "11:19": "random numbers if they're new and these",
  "11:21": "ones and you we're just we've grabbed",
  "11:23": "them and thrown them in there so we need",
  "11:25": "to train them but the other layers are",
  "11:30": "not new the other layers are good at",
  "11:32": "something right and what are they good",
  "11:35": "at well let's remember that xyler and",
  "11:39": "furgus paper here are examples of some",
  "11:43": "visualization of some filters some some",
  "11:47": "weight matrices in the first layer and",
  "11:49": "some examples of some things that they",
  "11:50": "found right so the first layer had one",
  "11:54": "part of the weight matrix was good at",
  "11:56": "finding diagonal edges in this direction",
  "11:58": "and then in layer two one of the filters",
  "12:01": "was good at finding corners in the top",
  "12:04": "left and then in layer three one of the",
  "12:08": "filters was good at finding repeating",
  "12:10": "patterns another one was good at finding",
  "12:13": "round orange things another one was good",
  "12:16": "at finding kind of like fairy your",
  "12:18": "floral textures so as we go up they're",
  "12:22": "becoming more sophisticated but also",
  "12:26": "more specific right so like layer four I",
  "12:30": "think was finding like eyeballs for",
  "12:32": "instance now if you're wanting to",
  "12:34": "transfer and learn to something for",
  "12:38": "histopathology slides there's probably",
  "12:41": "going to be no eyeballs in that right so",
  "12:43": "the later layers are no good for you but",
  "12:46": "they'll certainly be some repeating",
  "12:47": "patterns and they'll certainly be some",
  "12:49": "diagonal edges right so the earlier you",
  "12:51": "grow in the model the more likely it is",
  "12:54": "that you want those weights to stay as",
  "12:56": "they are well to start with we",
  "13:02": "definitely need to train these new",
  "13:03": "weights because they're random so let's",
  "13:05": "not bother training any of the other",
  "13:06": "weights at all to start with so what we",
  "13:09": "do is we basically say let's freeze",
  "13:17": "let's freeze all of those other layers",
  "13:19": "so what does that mean all that means is",
  "13:22": "that we're asking first di and PI torch",
  "13:25": "that when we train you know however many",
  "13:29": "epochs we do when we call fit don't back",
  "13:32": "propagate the weights but don't prep a",
  "13:36": "back propagate the gradients back into",
  "13:38": "those layers in other words when you go",
  "13:40": "parameters equals parameters - learning",
  "13:44": "rate times gradient only do it for the",
  "13:47": "new layers don't bother doing it for the",
  "13:49": "other layers",
  "13:49": "that's what freezing means okay just",
  "13:51": "means don't update those parameters so",
  "13:54": "it'll be a little bit faster as well",
  "13:59": "because there's a few less calculations",
  "14:00": "to do it'll take up a little bit less",
  "14:03": "memory because there's a few less",
  "14:04": "gradients that we have to store but most",
  "14:07": "importantly it's not going to change",
  "14:10": "weights that are already better than",
  "14:13": "nothing they're better than random at",
  "14:15": "the very least so that's what happens",
  "14:17": "when you call freeze it doesn't freeze",
  "14:18": "the whole thing it freezes everything",
  "14:20": "except the randomly generated atom added",
  "14:22": "layers that we put on for you so then",
  "14:25": "what happens next okay after a while we",
  "14:28": "say okay this is looking pretty good we",
  "14:31": "probably should train the rest of the",
  "14:32": "network now so we unfreeze okay and so",
  "14:39": "now we're gonna chain the whole thing",
  "14:42": "but we still have a pretty good sense",
  "14:44": "that these new layers we added to the",
  "14:46": "end probably need more training and",
  "14:49": "these ones right at the start that might",
  "14:52": "just be like diagonal edges probably",
  "14:54": "don't need much training at all so we",
  "14:57": "split our our model into a few sections",
  "15:04": "right and we say let's give different",
  "15:09": "parts of the model different learning",
  "15:11": "rates so this part of the model we might",
  "15:16": "give a learning rate of 1e neg 5 and",
  "15:21": "this part of the model we might give a",
  "15:23": "learning rate of 1",
  "15:27": "a neg 3c and so what's gonna happen now",
  "15:31": "is that we can keep training the entire",
  "15:33": "network but because the learning rate",
  "15:36": "for the early layers is smaller it's",
  "15:39": "going to move them around less because",
  "15:41": "we think they're already pretty good and",
  "15:43": "also like if it's already pretty good",
  "15:45": "too the optimal value if you used a",
  "15:47": "higher learning rate it could kick it",
  "15:48": "out right it could actually make it",
  "15:50": "worse which we really don't want to",
  "15:51": "happen okay so this this process is",
  "15:55": "called using discriminative learning",
  "15:57": "rates you won't find much online about",
  "16:01": "it because I think we were kind of the",
  "16:04": "first to use it for this purpose or at",
  "16:06": "least talked about it extensively maybe",
  "16:08": "other probably other people used it",
  "16:09": "without writing it down so most of the",
  "16:12": "stuff you'll find about this will be",
  "16:13": "fast AI students but it's it's starting",
  "16:17": "to get more well-known slowly now but",
  "16:21": "it's a really really important concept",
  "16:22": "for transfer learning without using this",
  "16:24": "you just can't get nearly as good",
  "16:26": "results so how do we do discriminative",
  "16:28": "learning rates in fast AI when you when",
  "16:37": "you anywhere you can put a learning rate",
  "16:38": "in fast AI such as with the fit function",
  "16:42": "the first thing you put in is the number",
  "16:44": "of epochs and then the second thing you",
  "16:46": "put in is learning rate saying if you",
  "16:48": "use fit one cycle the learning rate you",
  "16:51": "can put a number of things that you can",
  "16:52": "put a single number like one a neg three",
  "16:55": "you can write a slice so you can write",
  "16:59": "slice for example one a neg three with a",
  "17:02": "single number or you can write slice",
  "17:06": "with two numbers",
  "17:14": "which of those men in the first case",
  "17:18": "just using a single number means every",
  "17:21": "layer gets the same learning rate so",
  "17:24": "you're not using discriminative learning",
  "17:25": "rates if you pass a single number to",
  "17:31": "slice it means the final layers get a",
  "17:34": "learning rate of whatever you wrote down",
  "17:37": "of whatever you wrote down one e neck 3",
  "17:41": "and then all the other layers get the",
  "17:44": "same learning rate which is that divided",
  "17:46": "by 3 okay so all of the other layers",
  "17:50": "will be one a neg 3/3 the last layers",
  "17:53": "will be one in x3 in the last case the",
  "17:57": "final layers the these randomly hadn't",
  "17:59": "added layers will still be again 1 enoch",
  "18:01": "3 the first layers will get 1 in egg 5",
  "18:06": "and the other layers will get learning",
  "18:10": "rates that are equally spread between",
  "18:12": "those two so ink it multiplicatively",
  "18:17": "equal right so if there were three",
  "18:20": "layers there would be one in egg five",
  "18:22": "one in egg for one in egg three so equal",
  "18:25": "multiples each time one slight tweak to",
  "18:31": "make things a little bit simpler to",
  "18:32": "manage we don't actually give a",
  "18:35": "different learning rate to every layer",
  "18:37": "we give a different learning rate to",
  "18:39": "every layer group which is just we",
  "18:41": "decided to put the groups together for",
  "18:44": "you and so specifically what we do is",
  "18:47": "the randomly added extra layers we call",
  "18:50": "those one layer group this is by default",
  "18:52": "you can modify it and then all the rest",
  "18:54": "we split in half into two layer groups",
  "18:58": "so by default at least with a CNN you'll",
  "19:00": "get three layer groups and so if you say",
  "19:02": "slice one enoch in egg three you",
  "19:05": "will get one in egg five learning rate",
  "19:07": "for the first layer group one in egg",
  "19:09": "four for the second one day neck three",
  "19:10": "for the third so now if you go back and",
  "19:13": "look at the way that we're training",
  "19:14": "hopefully you'll see that this makes a",
  "19:16": "lot of sense this divided by three thing",
  "19:19": "there's a little weird and we won't talk",
  "19:23": "about why that is until part two of the",
  "19:26": "course it says",
  "19:27": "specific quirk around batch",
  "19:30": "normalization so we can discuss that in",
  "19:33": "the advanced topic if anybody's",
  "19:34": "interested all right",
  "19:37": "so that is that is fine tuning so",
  "19:43": "hopefully that makes that a little bit",
  "19:45": "less mysterious so we were looking at",
  "19:54": "collaborative filtering last week and in",
  "19:59": "the collaborative filtering example we",
  "20:02": "called fit one cycle and we passed in",
  "20:04": "just a single number and that makes",
  "20:06": "sense because in collaborative filtering",
  "20:08": "we only have one layer there's a few",
  "20:14": "different pieces in it but there isn't",
  "20:15": "you know a matrix multiply followed by",
  "20:20": "an activation function followed by",
  "20:22": "another matrix multiply I'm going to",
  "20:25": "introduce a another piece of jargon here",
  "20:29": "they're not always exactly matrix",
  "20:32": "multiplications there's something very",
  "20:34": "similar there they're linear functions",
  "20:37": "that we add together but the more",
  "20:39": "general term for these for these things",
  "20:41": "that are written more general than",
  "20:42": "matrix multiplications is our fine",
  "20:44": "functions okay so if you hear me say the",
  "20:47": "word a fine function you can replace it",
  "20:50": "in your head with matrix multiplication",
  "20:54": "but as we'll see when we do convolutions",
  "20:56": "convolutions our matrix multiplications",
  "20:59": "where some of the weights are tied and",
  "21:01": "so it would be slightly more accurate to",
  "21:04": "call them f-fine functions and I'd like",
  "21:06": "to introduce a little bit more drug in",
  "21:07": "each lesson so that when you you know",
  "21:10": "read books or papers or watch other",
  "21:12": "courses or read documentation there will",
  "21:15": "be more of the words you're recognized",
  "21:17": "okay so when you see a fine function it",
  "21:20": "just means a linear function right and",
  "21:23": "it means something very very close to",
  "21:26": "matrix multiplication now matrix",
  "21:28": "multiplication is the most common kind",
  "21:30": "of fi-in function at least in deep",
  "21:32": "learning so specifically for",
  "21:38": "collaborative filtering",
  "21:41": "the model we were using was this one it",
  "21:46": "was where we had a bunch of numbers here",
  "21:49": "and a bunch of numbers here and we took",
  "21:54": "the dot product of them and given that",
  "21:58": "one here is a row and when there's a",
  "21:59": "column we can actually that's the same",
  "22:01": "as a matrix product so M mult in Excel",
  "22:03": "multiplies matrices so here's the matrix",
  "22:06": "product of those two and so I started",
  "22:10": "this training last week by using solver",
  "22:13": "in Excel and we never actually went back",
  "22:15": "to see how it went so let's go and have",
  "22:18": "a look now so the average sum of squared",
  "22:23": "error got down to 0.39 so we're trying",
  "22:26": "to predict something on a scale of 0.5",
  "22:29": "to 5 so on average we're being wrong by",
  "22:32": "about 0.4 that's pretty good and you can",
  "22:34": "kind of see it's pretty good if you look",
  "22:38": "at like 3 5 1 is what it meant to be",
  "22:41": "three point two five five point one or",
  "22:43": "0.98 that's pretty close right so you",
  "22:47": "get the general idea and then I started",
  "22:51": "to talk about this idea of embedding",
  "22:53": "matrices and so in order to understand",
  "22:56": "that let's put this worksheet aside now",
  "23:01": "look at another worksheet so here's",
  "23:04": "another worksheet and what I've done",
  "23:07": "here is I have copied over those two",
  "23:10": "weight matrices from the previous",
  "23:12": "worksheet right here's the one for users",
  "23:17": "and here's the one for movies and the",
  "23:18": "movies one I've transposed it so it's",
  "23:21": "now got exactly the same dimensions as",
  "23:23": "the users one okay so the here are two",
  "23:27": "weight matrices initially they were",
  "23:30": "random we can train them with gradient",
  "23:33": "descent in the original data the user",
  "23:36": "IDs and movie IDs were numbers like",
  "23:39": "these okay to make life more convenient",
  "23:44": "I've converted them to numbers from 1 to",
  "23:48": "15 okay so in these columns I've got for",
  "23:52": "every rating",
  "23:53": "I've got user ID",
  "23:54": "movie ID rating using these mapped",
  "23:57": "numbers so that they're contiguous",
  "23:59": "starting at one okay now I'm going to",
  "24:01": "replace user ID number one with this",
  "24:07": "vector the vector contains a 1 followed",
  "24:11": "by 14 zeros and then user number 2 I'm",
  "24:16": "going to replace with a vector of 0 and",
  "24:19": "then 1 and then 13 zeros and so forth so",
  "24:22": "movie ID 14 or these are movie ID 14",
  "24:25": "I've also replaced with another vector",
  "24:29": "which is 13 zeros and then a 1 and then",
  "24:32": "a 0 okay so these are called one-hot",
  "24:36": "encodings by the way so this is not part",
  "24:42": "of a neural net this is just like some",
  "24:44": "input pre-processing room literally",
  "24:46": "making this my new input so this is my",
  "24:50": "new inputs for the mo movies this is my",
  "24:52": "new inputs for my users ok so these are",
  "24:56": "the inputs to a neural net so what I'm",
  "24:59": "going to do now is I'm going to take",
  "25:00": "this input matrix and I'm going to do a",
  "25:05": "matrix multiplied by this weight matrix",
  "25:11": "and that'll work",
  "25:12": "because this has 15 rows and this has 15",
  "25:18": "columns so I can multiply those two",
  "25:20": "matrices together because they match and",
  "25:22": "you can do matrix multiplication in",
  "25:24": "Excel using the air mult function just",
  "25:28": "be careful if you're using Excel because",
  "25:31": "this is a function that returns multiple",
  "25:33": "numbers you can't just hit enter when",
  "25:36": "you finish with it you have to hit",
  "25:37": "control shift enter control shift enter",
  "25:39": "means this is a array function is",
  "25:41": "something that returns multiple values",
  "25:42": "so here is the matrix product of this",
  "25:47": "input matrix of per of of inputs and",
  "25:54": "this make this parameter matrix or",
  "25:58": "weight matrix so that's just a normal",
  "26:02": "neural network layer okay it's just",
  "26:06": "a regular matrix model play and so we",
  "26:09": "can do the same thing for movies and so",
  "26:11": "here's the matrix model play for movies",
  "26:15": "well here's the thing this input is we",
  "26:22": "claim is this kind of one hot encoded",
  "26:24": "version of user ID number one and these",
  "26:27": "activations are the activations for user",
  "26:32": "ID number one why is that because if you",
  "26:34": "think about it",
  "26:35": "the matrix multiplication between a one",
  "26:37": "hot encoded vector and some matrix is",
  "26:41": "actually going to find the nth row of",
  "26:45": "that matrix when the one is in position",
  "26:48": "in does that make sense so what we've",
  "26:51": "done here is we've actually got a matrix",
  "26:57": "multiply that is creating this these",
  "26:59": "output activations right but it's doing",
  "27:01": "it in a very interesting way which is",
  "27:03": "it's effectively finding a particular",
  "27:05": "row when the input matrix so having done",
  "27:07": "that we can then multiply those two sets",
  "27:12": "together just a dot product and we can",
  "27:17": "then find the loss squared and then we",
  "27:22": "can find the average loss and lo and",
  "27:25": "behold that number 0.39 is the same as",
  "27:33": "this number because they're doing the",
  "27:35": "same thing so this one was kind of",
  "27:41": "finding this particular users embedding",
  "27:48": "vector this one is just doing a matrix",
  "27:52": "multiply and therefore we know they are",
  "27:55": "mathematically identical so let's lay",
  "27:58": "that out again so here's our final",
  "28:00": "version this is the same weight matrices",
  "28:04": "again exactly the same I've copied them",
  "28:06": "over and here's those user IDs and movie",
  "28:10": "IDs again right but this time I've laid",
  "28:13": "them out just in a normal kind of",
  "28:16": "tabular form just like you would expect",
  "28:18": "to see",
  "28:19": "the input to your model and this time I",
  "28:22": "have got exactly the same set of",
  "28:24": "activations here that I had here but in",
  "28:32": "this case I've calculated these",
  "28:34": "activations using excels offset function",
  "28:37": "which is an array look up right it says",
  "28:40": "find the first row of this so this is",
  "28:45": "doing it as an array lookup so this",
  "28:48": "version is identical to this version but",
  "28:52": "obviously it's much less memory",
  "28:54": "intensive and much faster because I",
  "28:56": "don't actually create the one hot",
  "28:58": "encoded matrix and I don't actually do a",
  "29:00": "matrix multiply because that matrix",
  "29:02": "multiply is nearly all multiplying by",
  "29:04": "zero which is a total waste of time so",
  "29:07": "in other words multiplying by a one",
  "29:10": "handed matrix is identical to doing an",
  "29:13": "array lookup therefore we should always",
  "29:17": "do the array lookup version and",
  "29:20": "therefore we have a specific way of",
  "29:23": "doing we have a specific way of saying I",
  "29:25": "want to do a matrix multiplication by a",
  "29:27": "one hot encoded matrix without ever",
  "29:30": "actually creating it I'm just instead",
  "29:32": "going to pass in a bunch of intz and",
  "29:34": "pretend they're one not encoded and that",
  "29:37": "is called",
  "29:38": "and embedding right so you might have",
  "29:42": "heard this word embedding all over the",
  "29:44": "places if there's some magic advanced",
  "29:47": "mathy thing but embedding means look",
  "29:52": "something up in an array okay but it's",
  "29:57": "interesting to know that looking",
  "29:59": "something up in an array is",
  "30:00": "mathematically identical to doing a",
  "30:03": "matrix product by a one hot encoded",
  "30:05": "matrix and therefore an embedding fits",
  "30:10": "very nicely in our standard model of our",
  "30:12": "neural networks work so now suddenly",
  "30:15": "it's as if we have another whole kind of",
  "30:17": "layer it's a kind of layer where we get",
  "30:20": "to look things up in an array but we",
  "30:22": "actually didn't do anything special",
  "30:24": "right we just added this computational",
  "30:26": "shortcut this thing called an embedding",
  "30:28": "which is simply a fast memory efficient",
  "30:31": "way of multiplying by",
  "30:32": "hot encoded matrix okay so this is",
  "30:37": "really important because when you hear",
  "30:39": "people say embedding you need to replace",
  "30:42": "it in your head with an array lookup",
  "30:46": "which we know is mathematically",
  "30:48": "identical to matrix multiplied by a one",
  "30:51": "hot encoder matrix here's the thing",
  "30:55": "though it has kind of interesting",
  "30:57": "semantics right because when you do",
  "31:00": "multiply something by a one hot encoded",
  "31:02": "matrix you get this nice feature where",
  "31:05": "the rows of your weight matrix the",
  "31:10": "values only appear for row number one",
  "31:13": "for example where you get user ID number",
  "31:16": "one in your inputs right so in other",
  "31:20": "words you kind of end up with this",
  "31:22": "weight matrix where certain rows of",
  "31:25": "weights correspond to certain values of",
  "31:29": "your input and that's pretty interesting",
  "31:32": "it's particularly interesting here",
  "31:35": "because going back to a kind of most",
  "31:37": "convenient way to look at this because",
  "31:41": "the only way that we can calculate an",
  "31:43": "output activation is by doing a dot",
  "31:46": "product of these two input vectors that",
  "31:49": "means that they kind of have to",
  "31:53": "correspond with each other right like",
  "31:56": "there has to be some way of saying if",
  "32:00": "this number for a user is high and this",
  "32:04": "number for a movie is high then the user",
  "32:06": "will like the movie so the only way that",
  "32:09": "can possibly make sense is if these",
  "32:12": "numbers represent features of personal",
  "32:15": "taste and corresponding features of",
  "32:18": "movies for example the movie has John",
  "32:22": "Travolta in it and user ID likes John",
  "32:29": "Travolta then you'll like this movie",
  "32:33": "okay so like we're not actually deciding",
  "32:37": "the Rhodes mean anything we're not doing",
  "32:39": "anything to make the Rhodes mean",
  "32:40": "anything but the only way that this",
  "32:42": "gradient descent could possibly come up",
  "32:44": "with a good answer is if it figures",
  "32:46": "out what the aspects of movie taste are",
  "32:50": "and the corresponding features of movies",
  "32:53": "are so those underlying kind of features",
  "32:57": "that appear that are called latent",
  "32:59": "factors or latent features they're these",
  "33:03": "hidden things that were there all along",
  "33:04": "and once we train this neural net they",
  "33:07": "suddenly appear now here's the problem",
  "33:11": "no one's going to like Battlefield Earth",
  "33:13": "right it's not a good movie even though",
  "33:16": "it has John Travolta in it so how are we",
  "33:19": "going to deal with that right because",
  "33:21": "there's this feature called",
  "33:22": "I like John Travolta movies and this",
  "33:24": "feature called this movie has John",
  "33:26": "Travolta and so this is now like you're",
  "33:29": "gonna like the movie but we need to save",
  "33:31": "some way to say unless it's Battlefield",
  "33:33": "Earth or you're a Scientologist either",
  "33:35": "one right so how do we do that we need",
  "33:38": "to add in bias right so here is the same",
  "33:44": "thing again same weight matrix sorry not",
  "33:48": "the same weight because he's the same",
  "33:51": "construct right same shape of everything",
  "33:55": "but this time you've got an extra row so",
  "33:59": "now this is not just the matrix product",
  "34:03": "of that and that but I'm also adding on",
  "34:07": "this number and this number which means",
  "34:12": "now each movie can have an overall this",
  "34:16": "is a great movie versus this isn't a",
  "34:17": "great movie and every user can have an",
  "34:20": "overall this user rates movies highly or",
  "34:23": "this user doesn't rate movies highly so",
  "34:26": "that's called the bias so this is",
  "34:28": "hopefully going to look very familiar",
  "34:30": "right this is the same usual linear",
  "34:33": "model concept or linear layer concept",
  "34:35": "from a neural net that you have a matrix",
  "34:37": "product and a bias and remember from",
  "34:39": "lesson to lesson 2's genie SGD notebook",
  "34:42": "you never actually need a bias you could",
  "34:46": "always just add a column of ones to your",
  "34:48": "input data and then that gives you bias",
  "34:50": "for free but that's pretty inefficient",
  "34:53": "right so in practice all neural networks",
  "34:56": "library explicitly have a concept",
  "34:58": "of bias we don't actually add the column",
  "35:01": "of ones so what does that do well just",
  "35:05": "before I came in today I ran tools",
  "35:07": "solver or notes data solver on this as",
  "35:11": "well and we can check the RMS see and so",
  "35:15": "the root mean squared here is 0.32",
  "35:17": "versus the version without bias whereas",
  "35:22": "0.39 okay so you can see that this",
  "35:27": "slightly better model gives us a better",
  "35:30": "result and it's better because it's it's",
  "35:32": "it's giving both more flexibility right",
  "35:35": "and it's also just makes sense",
  "35:37": "semantically that you need to be able to",
  "35:39": "say it's not the weather I'd like the",
  "35:42": "movie is not just about the combination",
  "35:44": "of what actors it has and whether it's",
  "35:46": "dialogue-driven and how much action is",
  "35:48": "in it but just is it a good movie",
  "35:49": "okay or am i somebody who rates movies",
  "35:53": "highly okay so there's all the pieces of",
  "36:00": "this collaborative filtering model how",
  "36:04": "are we going sancisco any questions we",
  "36:07": "have three questions okay okay so a",
  "36:12": "first question then is when we load a",
  "36:16": "pre trained model can we explore the",
  "36:20": "activation grids to see what they might",
  "36:22": "be good at recognizing yes you can and",
  "36:25": "we will learn how to should be in the",
  "36:28": "next lesson can we have an explanation",
  "36:33": "of what the first argument in fit one",
  "36:36": "cycle actually represents is it",
  "36:38": "equivalent to an epoch yes",
  "36:40": "the first argument to fit one cycle or",
  "36:42": "fit is number of epochs it's in other",
  "36:48": "words an epoch is looking at every input",
  "36:52": "once so if you do ten epochs you're",
  "36:57": "looking at every every input ten times",
  "37:00": "and so there's a chance you might start",
  "37:02": "overfitting if you've got lots of lots",
  "37:04": "of parameters and a high learning rate",
  "37:05": "if you only do one epoch it's impossible",
  "37:10": "to have a fit",
  "37:11": "and so that's why it's kind of useful to",
  "37:13": "remember how many epochs you're doing",
  "37:18": "can we have an exponent that one what is",
  "37:22": "an affine function an affine function is",
  "37:26": "a linear function I don't know if we",
  "37:30": "need much more detail than that",
  "37:32": "if you're multiplying things together",
  "37:34": "and adding them up it's an air fine",
  "37:37": "function I'm not going to bother with",
  "37:41": "them",
  "37:41": "exact mathematical definition partly cuz",
  "37:44": "I'm a terrible mathematician and partly",
  "37:45": "because it doesn't matter but if you",
  "37:47": "just remember that you're multiplying",
  "37:48": "things together and then adding them up",
  "37:50": "that's the most important thing it's",
  "37:52": "linear okay and therefore if you put an",
  "37:55": "F fine function on top of an F fine",
  "37:56": "function that's just another F fine",
  "37:58": "function you haven't won anything at all",
  "38:00": "that's a total waste of time right so",
  "38:03": "you need to sandwich it with any kind of",
  "38:07": "non-linearity pretty much works right",
  "38:09": "including replacing the negatives with",
  "38:10": "zeros which we call value okay so if you",
  "38:13": "draw Fi and really F I'm really f I'm",
  "38:15": "really you you have a deep neural",
  "38:17": "network okay so so let's go back to the",
  "38:26": "collaborative filtering notebook and",
  "38:29": "this time we're going to grab the whole",
  "38:31": "movie lens 100k dataset there's also a",
  "38:35": "20 million dataset",
  "38:37": "by the way so really a great project",
  "38:42": "available made by this group called",
  "38:44": "group lens they actually update the",
  "38:46": "movie lens data sets on a regular basis",
  "38:49": "but they helpfully provide the original",
  "38:51": "one and we're going to use the original",
  "38:53": "one because that means that we can",
  "38:54": "compare two baselines because everybody",
  "38:58": "basically they say hey if you're going",
  "38:59": "to compare the baselines make sure you",
  "39:01": "all use the same data set here's the one",
  "39:03": "you should use unfortunately it means",
  "39:05": "that we're going to be restricted to",
  "39:06": "movies that are before 1998 so maybe you",
  "39:10": "won't have seen them all but that's the",
  "39:13": "price we pay you can replace this with",
  "39:15": "ml latest when you download it and use",
  "39:18": "it if you want to play around with",
  "39:20": "movies that are up to date okay",
  "39:24": "the original movie lens data set the",
  "39:28": "more recent ones are in a CSV file it's",
  "39:30": "super convenient to use the original one",
  "39:32": "is a slightly messy first of all they",
  "39:35": "don't use commas for two limiters they",
  "39:37": "use tabs so in pandas you can just say",
  "39:39": "what's the delimiter and you loaded in",
  "39:41": "the second is they don't add a header",
  "39:43": "row so that you know what color room is",
  "39:45": "what so you have to tell pandas there's",
  "39:46": "no header row and then since there's no",
  "39:48": "header row you have to tell pandas what",
  "39:51": "are the names of four columns rather",
  "39:53": "than that that's what we need okay so we",
  "39:57": "can then have a look at head which",
  "39:59": "remembers the first few rows and there",
  "40:00": "is our user ratings user movie rating",
  "40:04": "and let's make it more fun let's see",
  "40:07": "what the movies actually are I'll just",
  "40:10": "point something out here which is",
  "40:14": "there's this thing called encoding",
  "40:15": "equals I'm going to get rid of it and I",
  "40:19": "get this error Unicode I just want to",
  "40:21": "point this out because you'll all see",
  "40:23": "this at some point in your lives codec",
  "40:26": "can't decode blah blah blah what this",
  "40:28": "means is that this is not a Unicode file",
  "40:31": "this will be quite common when you're",
  "40:33": "using datasets are a little bit older",
  "40:36": "back before you know us folks in the",
  "40:39": "West really realized that there are",
  "40:41": "people that use languages other than",
  "40:42": "well English people we English languages",
  "40:44": "other than English nowadays you know",
  "40:48": "we're much better at handling different",
  "40:50": "languages we use this standard called",
  "40:53": "Unicode for it and Python very helpfully",
  "40:57": "uses Unicode by default but so if you",
  "40:59": "try to load an old file it's not Unicode",
  "41:01": "you actually believe it or not have to",
  "41:03": "guess how it was coded but since like",
  "41:07": "it's really likely that it was created",
  "41:10": "by you know some Western European or",
  "41:14": "American person they almost certainly",
  "41:17": "used Latin one so if you just papi an",
  "41:20": "encoding equals Latin one if you use",
  "41:23": "file open in Python or pandas open or",
  "41:26": "whatever that will generally get around",
  "41:30": "your problem again they didn't have the",
  "41:35": "names so we had to list of the names are",
  "41:37": "this is kind of interesting they had a",
  "41:39": "separate column for every one of however",
  "41:41": "many genres they had 19 genres and",
  "41:44": "you'll see it this looks one hot encoded",
  "41:46": "but it's actually not it's actually n",
  "41:48": "hot encoded in other words a movie can",
  "41:51": "be in multiple genres we're not going to",
  "41:52": "look at Jean Roos today but it's just",
  "41:54": "interesting to point out that this is a",
  "41:56": "way that sometimes people will represent",
  "41:58": "something like genre and the more recent",
  "42:01": "version they actually listed the genres",
  "42:03": "directly which is much more convenient",
  "42:05": "ok so I find life is so we got a hundred",
  "42:10": "thousand ratings I find life is easier",
  "42:12": "when you're modeling when you actually",
  "42:13": "denormalize the data so I actually want",
  "42:16": "the movie title directly in my ratings",
  "42:18": "so pandas has a merge function to let us",
  "42:21": "do that so here's the ratings table with",
  "42:23": "actual titles so as per usual we can",
  "42:27": "create a data bunch for our application",
  "42:29": "so a collab data bunch for the collab",
  "42:31": "application from what from a data frame",
  "42:34": "there's our data frame set aside some",
  "42:38": "validation data really we should use the",
  "42:42": "validation sets and cross validation",
  "42:43": "approach that they used if you're going",
  "42:45": "to properly compare with a benchmark so",
  "42:46": "take these comparisons with a grain of",
  "42:48": "salt",
  "42:49": "by default car lab data bunch assumes",
  "42:52": "that your first column is you user",
  "42:57": "second column of item the third column",
  "42:59": "is rating but now we're actually going",
  "43:01": "to use the title column as item so we",
  "43:04": "have to tell it what the item column",
  "43:07": "name is and then all of our data bunches",
  "43:10": "support show batch so you can just check",
  "43:12": "what's in there and there it is okay so",
  "43:18": "I'm going to try and get as good a",
  "43:21": "result as I can so I'm gonna try and use",
  "43:24": "whatever tricks I can come up with to",
  "43:26": "get a good answer now one of the tricks",
  "43:28": "is to use the Y range and remember the",
  "43:32": "the Y range was the thing that made the",
  "43:35": "final activation function a sigmoid and",
  "43:40": "specifically last week we said let's",
  "43:43": "have a sigmoid that goes from naught to",
  "43:45": "5 and that way it's going to ensure that",
  "43:48": "it kind of is going to help the neural",
  "43:49": "network predict things that are in the",
  "43:51": "range actually didn't do that in my",
  "43:56": "Excel version and so you can see I've",
  "43:58": "actually got some negatives Maddon",
  "44:00": "there's also some things bigger than",
  "44:01": "five so if you want to beat me in Excel",
  "44:04": "you could you could add the sigmoid to",
  "44:08": "excel and train this and you'll get a",
  "44:10": "slightly better answer now the problem",
  "44:15": "is that a sigmoid actually asymptotes at",
  "44:18": "say whatever the maximum is we said five",
  "44:21": "which means you can never actually",
  "44:23": "predict five but plenty of movies have a",
  "44:25": "rating of five so that's a problem so",
  "44:28": "actually it's slightly better to make",
  "44:30": "your way range go from a little bit less",
  "44:32": "than the minimum to a little bit more",
  "44:33": "than the maximum and the minimum of this",
  "44:35": "data is 0.5 and the maximum is 5 so this",
  "44:39": "range is just a little bit further so",
  "44:42": "that's a that's one little trick to get",
  "44:44": "a little bit more accuracy the other",
  "44:49": "trick I used is to add something called",
  "44:51": "weight decay and we're going to look at",
  "44:53": "that next ok after this section we got",
  "44:56": "to learn about weight okay so then how",
  "44:59": "many how many factors do you want or",
  "45:04": "what are factors the number of factors",
  "45:07": "is the width of the embedding matrix so",
  "45:10": "why don't we say embedding sighs maybe",
  "45:13": "we should but in the world of",
  "45:16": "collaborative filtering they don't use",
  "45:18": "that word they use the word factors",
  "45:20": "because of this idea of latent factors",
  "45:22": "and because the standard way of doing",
  "45:24": "collaborative filtering has been with",
  "45:27": "something called matrix factorization",
  "45:28": "and in fact what we just saw happens to",
  "45:33": "actually be a way of doing matrix",
  "45:35": "factorization so we've we've actually",
  "45:37": "accidentally learned how to do matrix",
  "45:39": "factorization today so so this is a term",
  "45:42": "that's kind of specific to this domain",
  "45:44": "okay but you can just remember it as the",
  "45:47": "width of the embedding matrix and so why",
  "45:50": "40 well this is one of these",
  "45:51": "architectural decisions you have to play",
  "45:54": "around with and see what works so I",
  "45:55": "tried 10 20 40 and 80 and I found 42 in",
  "45:59": "to work pretty well and it rained really",
  "46:02": "quickly so like you can check it in a",
  "46:04": "little for loop",
  "46:05": "to try a few things and see what looks",
  "46:06": "best and then for learning rates so",
  "46:10": "here's the learning rate finder as usual",
  "46:14": "so five Enoch three seemed to work",
  "46:17": "pretty well remember this is just a rule",
  "46:18": "of thumb right five e neg three is a bit",
  "46:21": "lower than both Silver's rule in my rule",
  "46:23": "so Silver's role is find the bottom and",
  "46:26": "go back by ten",
  "46:27": "so his rule would be more like two e neg",
  "46:30": "- I reckon my rule is kind of find about",
  "46:33": "the steepest section which is about here",
  "46:35": "which again like often it agrees with",
  "46:37": "your man so that would be about - Enoch",
  "46:39": "- I tried that but I always like to try",
  "46:42": "like 10 X less than 10x more just to",
  "46:45": "check and actually I found a bit less",
  "46:46": "was helpful so the answer to the",
  "46:50": "question like should I do blah is always",
  "46:53": "try blah and see now that's how you",
  "46:56": "actually become a good practitioner so",
  "47:01": "that gave me point eight one three right",
  "47:03": "and as usual you can save the result to",
  "47:05": "save you another 33 seconds from having",
  "47:07": "to do it again later and so there's a",
  "47:11": "library called Lib wreck and they",
  "47:14": "published some benchmarks for movie lens",
  "47:19": "100k and there's a root mean squared",
  "47:21": "error section and about point nine one",
  "47:24": "is about as good as they seem to have",
  "47:25": "been able to get point nine one is the",
  "47:28": "root mean square error we use the mean",
  "47:30": "square error not the root so we have to",
  "47:32": "go to point nine one squared which is",
  "47:34": "0.8 three and what we're getting point",
  "47:37": "eight one so that's cool with this very",
  "47:39": "simple model we're doing a little bit",
  "47:43": "better quite a lot better actually",
  "47:47": "although as I said take it with a grain",
  "47:49": "of salt because we're not doing the same",
  "47:51": "spits and the same cross validation that",
  "47:54": "so we're at least highly competitive",
  "47:55": "with their approaches okay so we're",
  "48:01": "going to look at the Python code that",
  "48:03": "does this in a moment we're going to",
  "48:06": "look at the Python code that does this",
  "48:07": "in a moment but for now just take my",
  "48:10": "word for it that we're going to say",
  "48:11": "something that's just doing this",
  "48:17": "right looking things up in an array and",
  "48:19": "then model plugging them together adding",
  "48:22": "them up and doing the mean square error",
  "48:24": "loss function so given that and given",
  "48:28": "that we noticed that the only way that",
  "48:30": "that can do anything interesting is by",
  "48:32": "trying to kind of find these latent",
  "48:37": "factors it makes sense to look and see",
  "48:40": "what they found",
  "48:40": "right particularly since as well as",
  "48:43": "finding latent factors we also now have",
  "48:46": "a specific bias number for every user",
  "48:49": "and every movie right now you could just",
  "48:52": "say what's the average rating to each",
  "48:56": "movie but there's a few issues with that",
  "48:59": "in particular this is something you see",
  "49:01": "a lot with like anime people who like",
  "49:04": "anime just love anime right and so",
  "49:07": "they're watching lots of anime and then",
  "49:09": "they just rate all the enemy highly and",
  "49:11": "so very often on kind of charts of",
  "49:14": "movies you'll see a lot of anime at the",
  "49:16": "top particularly if it's like you know a",
  "49:19": "hundred long series of anime you'll find",
  "49:22": "you know every single item of that",
  "49:25": "series in the top thousand movie lists",
  "49:27": "or something so how do we deal with that",
  "49:30": "well the nice thing is that instead if",
  "49:33": "we look at the movie bias right the",
  "49:36": "movie bias says kind of once we've",
  "49:40": "included the user bias right which for",
  "49:42": "an anime lover might be a very high",
  "49:45": "number because they're just reading a",
  "49:46": "lot of movies highly now once we account",
  "49:48": "for the specifics of this kind of movie",
  "49:50": "which again might be people love anime",
  "49:53": "right what's left over is something",
  "49:56": "specific to that movie itself so it's",
  "49:58": "kind of interesting to look at movie",
  "50:01": "bias numbers as a way of saying what are",
  "50:04": "the best movies or what people what do",
  "50:06": "people really like as movies even if",
  "50:09": "those people don't rate movies very",
  "50:11": "highly or even if there does that movie",
  "50:14": "doesn't have the kind of features that",
  "50:15": "people tend to have rate rate highly so",
  "50:17": "it's kind of nice",
  "50:19": "it's funny to say this and I'm by a by",
  "50:22": "using the bias we get an unbiased kind",
  "50:24": "of movie score so how do we do that well",
  "50:31": "to make it interesting because",
  "50:32": "particularly because this data set only",
  "50:34": "start only goes to 1998 let's only look",
  "50:39": "at movies that are plenty of people",
  "50:41": "watch right so we'll use pandas to grab",
  "50:45": "our reading movie table grip it by title",
  "50:48": "and then count the number of ratings and",
  "50:52": "not measuring how high their rating just",
  "50:54": "how many ratings do they have okay and",
  "50:56": "so the top thousand is that is them",
  "51:02": "other movies that have been rated the",
  "51:04": "most and so there hopefully movies that",
  "51:06": "we might have seen okay that's the only",
  "51:08": "reason I'm doing this and so I've called",
  "51:09": "this top movies by which I mean not not",
  "51:12": "good movies just movies were likely to",
  "51:15": "have seen so not surprisingly Star Wars",
  "51:17": "is the one that at that point most the",
  "51:21": "most people were had put a rating to",
  "51:27": "Independence Day there you go so we can",
  "51:32": "then take our loner that we trained and",
  "51:36": "asked it for the bias of the items",
  "51:42": "listed here okay so is item equals true",
  "51:46": "you would pass true to say I want the",
  "51:48": "items or false to say I want the users",
  "51:51": "and so this is kind of like a pretty",
  "51:53": "common piece of nomenclature for",
  "51:56": "collaborative filtering these IDs tend",
  "52:01": "to be called users these IDs tend to be",
  "52:05": "called items even if your problem has",
  "52:08": "got nothing to do with users and items",
  "52:10": "at all you know you just use these names",
  "52:13": "for convenience okay so they're just",
  "52:16": "they're just words",
  "52:17": "so in our case we want the items this is",
  "52:20": "the list of items we want we want the",
  "52:22": "bias so this is specific to klepto",
  "52:24": "filtering and so that's going to give us",
  "52:26": "back a thousand numbers back because we",
  "52:29": "asked for this has a thousand movies in",
  "52:31": "it so we can now take and just for",
  "52:36": "comparison let's also group the titles",
  "52:39": "by the mean rating so then we can zip",
  "52:42": "through so going through together",
  "52:45": "each of the movies along with the bias",
  "52:47": "and grab their rating and the bias and",
  "52:51": "the movie and then we can sort them all",
  "52:56": "by the zero index thing which is the",
  "53:02": "bias so here are the lowest numbers so I",
  "53:08": "can say you know",
  "53:11": "Mortal Kombat annihilation not a great",
  "53:14": "movie lawnmower man - not a great movie",
  "53:16": "I haven't seen children of the corn but",
  "53:18": "we did have a long discussion at SF",
  "53:20": "study group today and people who have",
  "53:21": "seen it agree not a great movie and you",
  "53:25": "can kind of see like some of them",
  "53:27": "actually have pretty decent ratings even",
  "53:32": "as though like relative to right so this",
  "53:35": "one's actually got a much higher rating",
  "53:36": "than the next one right but you know",
  "53:40": "that's kind of saying well the kind of",
  "53:42": "actors that were in this in the kind of",
  "53:44": "movie that this was and the kind of",
  "53:45": "people who do like it and watch it you",
  "53:48": "would expect it to be higher and then",
  "53:51": "here's the sort by reverse okay",
  "53:54": "Schindler's List Titanic Shawshank",
  "53:56": "Redemption seems reasonable and again",
  "53:58": "you can kind of look for ones where like",
  "54:01": "the rating you know isn't that high but",
  "54:04": "it's still very high here so that's kind",
  "54:07": "of like you know at least in 1998 people",
  "54:09": "weren't that into Leonardo DiCaprio or",
  "54:12": "you know people aren't that into",
  "54:14": "dialogue-driven movies or people aren't",
  "54:17": "that into romances or whatever but still",
  "54:19": "people liked it more than you would have",
  "54:20": "expected so it's interesting to kind of",
  "54:23": "like interpret our models in this way we",
  "54:27": "can go a bit further and grab not just",
  "54:29": "the biases but the weights so that is",
  "54:35": "these things and again we're going to",
  "54:40": "grab the weights for the items for our",
  "54:42": "top movies and that is a thousand by",
  "54:45": "forty because we asked for forty factors",
  "54:47": "so rather than having a width of five we",
  "54:50": "have a width of 40 often",
  "54:57": "really there's there isn't really",
  "55:00": "conceptually forty latent factors",
  "55:02": "involved in taste and so trying to look",
  "55:05": "at the forty can be you know not that",
  "55:09": "intuitive so what we want to do is we",
  "55:11": "want to squish those forty down to just",
  "55:14": "three and there's something that we're",
  "55:17": "not going to look into called PCA stands",
  "55:19": "for principal components analysis so",
  "55:21": "this is a movie W is a torch tensor and",
  "55:25": "fast AI adds the PCA method to torch",
  "55:30": "tensors and what PCA does principal",
  "55:32": "components analysis is it's a simple",
  "55:35": "linear transformation that takes an",
  "55:37": "input matrix and tries to find a smaller",
  "55:40": "number of columns that kind of cover a",
  "55:45": "lot of the space of that original matrix",
  "55:47": "if that sounds interesting which it",
  "55:49": "totally is you should check out our",
  "55:52": "course computational linear algebra",
  "55:54": "which Rachele teachers where we will",
  "55:58": "show you how to calculate PCA from",
  "56:02": "scratch and why you'd want to do it and",
  "56:03": "lots of stuff like that it's absolutely",
  "56:07": "not a prerequisite for anything in this",
  "56:09": "course but it's definitely worth knowing",
  "56:12": "that taking layers of neural nets and",
  "56:16": "chucking them through PCA is very often",
  "56:18": "a good idea because very often you have",
  "56:20": "like way more activations than you want",
  "56:22": "in a layer and there's all kinds of",
  "56:24": "reasons you would might want to play",
  "56:25": "with it for example Francisco who's",
  "56:29": "sitting next to me today is has been",
  "56:32": "working on something to do image",
  "56:38": "similarity right and very weak",
  "56:39": "similarity a nice way to do that is to",
  "56:41": "compare activations from a model but",
  "56:43": "often those activations will be huge and",
  "56:45": "therefore your thing could be really",
  "56:46": "slow and unwieldy so people often for",
  "56:49": "something like image similarity will",
  "56:51": "chuck it through a PCA first and that's",
  "56:55": "kind of cool",
  "56:56": "in our case we're just going to do it so",
  "56:58": "that we take our 40 components down to",
  "57:00": "three components so hopefully they'll be",
  "57:02": "easier for us to interpret so we can",
  "57:06": "grab each of those three factors will",
  "57:08": "call them factor naught one and two",
  "57:10": "and let's grab that movie components and",
  "57:14": "then sort and now the thing is we have",
  "57:19": "no idea what this is going to mean but",
  "57:21": "we're pretty sure it's going to be some",
  "57:22": "aspect of taste and movie feature so if",
  "57:28": "we print it out the top and the bottom",
  "57:30": "we can see that the highest ranked",
  "57:34": "things on this feature you would kind of",
  "57:37": "describe them as you know connoisseur",
  "57:41": "movies I guess you know like Chinatown",
  "57:44": "you know really classic Jack Nicholson",
  "57:47": "movie",
  "57:48": "everybody knows Casablanca and even like",
  "57:50": "wrong trousers is like this kind of",
  "57:52": "classic claymation movie",
  "57:55": "and so forth right so yeah this this is",
  "57:58": "definitely measuring like things that",
  "58:00": "are very high on the kind of connoisseur",
  "58:02": "level where else maybe home alone 3 not",
  "58:06": "such a favorite with Connor says perhaps",
  "58:08": "it's just not to say that there aren't",
  "58:10": "people who don't like it right but",
  "58:12": "probably not the same kind of people",
  "58:14": "that would appreciate secrets and lies",
  "58:17": "ok so you can kind of see this idea that",
  "58:19": "this has found some feature of movies",
  "58:22": "and a corresponding feature of the kind",
  "58:24": "of things people like so let's look at",
  "58:26": "another feature",
  "58:26": "so here's factor number one so this",
  "58:31": "seems to have found like okay these are",
  "58:33": "just big hits that you could watch with",
  "58:36": "the family you know these are definitely",
  "58:41": "not that you know Trainspotting very",
  "58:43": "gritty kind of you know thing so again",
  "58:47": "it's kind of found this interesting",
  "58:49": "feature of taste and we could even like",
  "58:53": "draw them on a graph right I've just",
  "58:56": "cuddled them randomly to make them",
  "58:57": "easier to see and you can kind of see",
  "58:59": "like and this is just the top 50 most",
  "59:03": "popular movies by rating by how many",
  "59:07": "times they've been rated and so kind of",
  "59:09": "on this one factor you've got the head",
  "59:10": "of the terminators really high up here",
  "59:12": "and the kind of English Patient and",
  "59:14": "students list at the other end and then",
  "59:16": "kind of is your godfather and Auntie",
  "59:18": "Python over here and Independence Day",
  "59:21": "and lie a liar over there so you get the",
  "59:23": "idea",
  "59:23": "so that's kind",
  "59:24": "it would be interesting to see if you",
  "59:26": "can come up with some stuff at work or",
  "59:30": "other kind of datasets where you could",
  "59:31": "try to pull out some some features and",
  "59:34": "play with them so how does that work",
  "59:44": "any questions what okay the question is",
  "59:50": "why am I sometimes getting negative loss",
  "59:54": "when training you shouldn't be so you're",
  "60:03": "doing something wrong so ask on show us",
  "60:07": "your your penny particularly since",
  "60:09": "people are uploading this I guess other",
  "60:10": "people seen it too so to put it on the",
  "60:12": "forum I mean they sit there doing",
  "60:16": "negative love likelihood yeah so we're",
  "60:19": "going to be learning about cross or",
  "60:20": "entropy and negative log likelihood",
  "60:21": "after the break",
  "60:22": "today they are lost functions that have",
  "60:27": "very specific expectations about what",
  "60:29": "your input looks like and if your input",
  "60:30": "doesn't look like that then they're",
  "60:32": "going to give very weird answers so",
  "60:35": "probably you press the wrong buttons so",
  "60:38": "don't do that okay okay",
  "60:44": "so we said collab learner and so here is",
  "60:55": "the collab learner function the collab",
  "61:02": "learner function as per usual takes a",
  "61:08": "data bunch and normally learners also",
  "61:12": "take something where you ask for",
  "61:14": "particular architectural details in this",
  "61:17": "case there's only one thing which does",
  "61:18": "that which is basically do you want to",
  "61:19": "use a multi-layer neural net or do you",
  "61:21": "want to use a classic collaborative",
  "61:22": "filtering and we're only going to look",
  "61:24": "at the classic collaborative filtering",
  "61:25": "today or maybe your briefly look at the",
  "61:28": "other one too let's see and so what",
  "61:32": "actually happens here well basically",
  "61:33": "we're going to create we create a",
  "61:37": "an embedding dot bias model and then we",
  "61:40": "pass back a learner which has our data",
  "61:42": "and that model so obviously all the",
  "61:44": "interesting stuff is happening here and",
  "61:45": "embedding drop bias so let's take a look",
  "61:48": "at that",
  "61:49": "I clearly press the wrong button",
  "61:51": "embedding dot bias there we go okay so",
  "61:58": "here's our embedding dot bias model it",
  "62:04": "is a NN module so in in pi torch to",
  "62:10": "remind you all PI torch layers and",
  "62:14": "models are NN modules they are things",
  "62:17": "that once you create them look exactly",
  "62:20": "like a function you call them with",
  "62:22": "parentheses and you pass them arguments",
  "62:25": "but they're not functions they don't",
  "62:28": "even have normally in Python to make",
  "62:31": "something look like a function you have",
  "62:33": "to give it a method called dunder call",
  "62:35": "remember that means underscore",
  "62:37": "underscore call underscore underscore",
  "62:38": "which doesn't exist here and the reason",
  "62:40": "is that pi torch actually expects you to",
  "62:44": "have something called forward and that's",
  "62:47": "what pi torch will call for you",
  "62:48": "when you call it like a function so when",
  "62:51": "this model is being trained to get the",
  "62:55": "predictions it's actually going to call",
  "62:57": "forward for us so this is where we do",
  "63:02": "the calculations right to calculate our",
  "63:07": "predictions so this is where you can see",
  "63:10": "we grab our why is this users rather",
  "63:14": "than user that's because everything's",
  "63:16": "done a mini-batch at a time right so it",
  "63:18": "is kind of when I read the forward in in",
  "63:23": "a PI torch module I tend to ignore in my",
  "63:28": "head the fact that there's a mini batch",
  "63:29": "and I pretend there's just one because",
  "63:32": "PI torch automatically handles all of",
  "63:35": "the stuff about doing it to everything",
  "63:37": "in the mini batch for you right so let's",
  "63:40": "pretend there's just one user right so",
  "63:42": "grab that user and what is this self dot",
  "63:45": "u underscore weight self dot u",
  "63:47": "underscore weight is",
  "63:50": "bedding we create an embedding for each",
  "63:53": "of users by factors items by factors",
  "63:58": "users by one items by one well that",
  "64:02": "makes sense right so what users by one",
  "64:05": "is yeah that's the users bias right and",
  "64:12": "then users by Factor is here so users by",
  "64:21": "factors is the first couple so that's",
  "64:23": "going to go in you underscore weight and",
  "64:25": "users comma one is the third so that's",
  "64:28": "going to go in you underscore bias so",
  "64:30": "remember when PI torch creates our NN",
  "64:34": "module",
  "64:35": "it calls dunder init and so this is",
  "64:38": "where we have to create our weight",
  "64:40": "matrices right and we don't normally",
  "64:43": "create the actual weight matrix tensors",
  "64:45": "we normally use PI torches convenience",
  "64:48": "functions to do that for us",
  "64:49": "and we're going to see some of that",
  "64:50": "after the break so for now just",
  "64:54": "recognize that this function is going to",
  "64:56": "create an embedding matrix for us it's",
  "65:00": "going to be a PI torch and n dot module",
  "65:03": "as well so therefore to actually pass",
  "65:07": "stuff into that embedding matrix and get",
  "65:10": "activations out you treat it as if it",
  "65:14": "was a function okay stick it in",
  "65:16": "parentheses so if you want to look in",
  "65:17": "the PI that pipe torch source code and",
  "65:20": "find n n dot embedding you will find",
  "65:23": "there's something called dot forward in",
  "65:24": "there which will do this array lookup",
  "65:26": "for us so here's where we grab the users",
  "65:33": "here's where we grab the items and so",
  "65:37": "we've now got the embeddings for each",
  "65:39": "right and so at this point we're kind of",
  "65:44": "like here and we found that and that so",
  "65:50": "we multiply them together and sum them",
  "65:53": "up and then we add on the user bias and",
  "65:57": "the item bias and then if we've got a",
  "66:00": "wide range then we do our sigmoid",
  "66:03": "trick and so the nice thing is you know",
  "66:08": "and you now understand the entirety of",
  "66:11": "this model and this is not just any",
  "66:13": "model this is a model that we just found",
  "66:15": "is at the very least highly competitive",
  "66:19": "with and perhaps slightly better than",
  "66:22": "some published table of pretty good",
  "66:25": "numbers from a software group that does",
  "66:29": "nothing about this so you're doing well",
  "66:31": "right this is nice",
  "66:35": "so that's probably a good place to have",
  "66:40": "a break and so after the break we're",
  "66:43": "going to come back and we're going to",
  "66:44": "talk about the one piece of this puzzle",
  "66:47": "we haven't learnt yet which is what the",
  "66:49": "hell is this - okay so let's come back",
  "66:53": "at 750 okay",
  "67:03": "so this idea of interpreting embeddings",
  "67:06": "is really interesting and as we'll see",
  "67:09": "later in this lesson the the things that",
  "67:12": "we create for categorical variables more",
  "67:15": "generally in tabular data sets are also",
  "67:17": "embedding matrices and again that's just",
  "67:20": "a normal matrix multiplied by a one hot",
  "67:22": "encoded input where we skip the",
  "67:26": "computational computational and memory",
  "67:28": "burden of it by doing it in a more",
  "67:30": "efficient way and it happens to end up",
  "67:33": "with these interesting semantics kind of",
  "67:35": "accidentally and there was this really",
  "67:38": "interesting paper by these folks who",
  "67:41": "came second in a capital competition for",
  "67:46": "something called AI Rossman will",
  "67:49": "probably look in more detail at the",
  "67:50": "rustman competition in part two I think",
  "67:53": "we're gonna run out of time in part one",
  "67:55": "but it's basically there's pretty",
  "67:57": "standard tabular stuff the main",
  "68:00": "interesting stuffs in the pre-processing",
  "68:03": "and it was interesting because Eve they",
  "68:05": "came second despite the fact that the",
  "68:08": "person who came first and pretty much",
  "68:09": "everybody else was the top of the",
  "68:10": "leaderboard did a massive amount of",
  "68:12": "highly specific feature engineering",
  "68:15": "where else these folks did",
  "68:17": "we're less feature engineering than",
  "68:18": "anybody else but instead they used a",
  "68:20": "neural net and this was at a time in",
  "68:22": "2016 when just no one did that no one",
  "68:26": "was doing neural nets for tabular data",
  "68:27": "so they have you know the kind of stuff",
  "68:30": "that we've been talking about kind of a",
  "68:34": "rose there or at least was kind of",
  "68:37": "popularized there and when I say",
  "68:38": "popularized I mean only popularized a",
  "68:40": "tiny bit it's still most people unaware",
  "68:42": "of this idea but it's pretty cool",
  "68:45": "because in their paper they showed that",
  "68:46": "the main average percentage error for",
  "68:49": "various techniques K nearest neighbors",
  "68:50": "random forests and gradient boosted",
  "68:53": "trees",
  "68:55": "well first you know neural Nets just",
  "68:56": "works work worked a lot better but then",
  "68:58": "with entity embeddings which is what",
  "69:00": "they call this just using entity",
  "69:03": "matrices in capital data you can",
  "69:06": "actually they actually added the entity",
  "69:08": "embeddings to all of these different",
  "69:09": "tasks after training them and they all",
  "69:12": "got way better right so neural nets with",
  "69:15": "entity embeddings are still the best but",
  "69:17": "a random forest with empty embeddings",
  "69:18": "was not at all far behind and you know",
  "69:22": "that's often kind of that's kind of nice",
  "69:23": "right because you could train these",
  "69:25": "entity matrices for products or stores",
  "69:29": "or genome motifs or whatever and then",
  "69:33": "use them in lots of different models",
  "69:35": "possibly you know using faster things",
  "69:37": "like random forests about getting a lot",
  "69:41": "of the benefits that was something",
  "69:43": "interesting they took a two-dimensional",
  "69:47": "projection of their of their embedding",
  "69:51": "matrix for state for example German",
  "69:54": "state because this was a German",
  "69:56": "supermarket chain I think using the same",
  "69:59": "kind of approach we did I don't remember",
  "70:00": "if they use PCA or something else",
  "70:02": "slightly different and then here's the",
  "70:05": "interesting thing I've circled here you",
  "70:09": "know a few things in this embedding",
  "70:11": "space and I've circled it with the same",
  "70:13": "color over here and here I've circled",
  "70:16": "some same color over here and it's like",
  "70:19": "oh my god the embedding projection has",
  "70:25": "actually discovered geography",
  "70:27": "like but they didn't do that right but",
  "70:30": "it's it's it's found things that are",
  "70:32": "near by each other in grocery purchasing",
  "70:36": "patterns because this was about",
  "70:38": "predicting how many sales there will be",
  "70:40": "you know it's it there is some",
  "70:43": "Geographic element of that in fact here",
  "70:48": "is a graph of the distance between two",
  "70:50": "embedding vectors so you can just take",
  "70:53": "an embedding vector and say what's the",
  "70:55": "sum of squared you know compared to some",
  "70:58": "other embedding vector that's the",
  "70:59": "Euclidean distance what's the distance",
  "71:01": "in embedding space and then plotted",
  "71:03": "against the distance in real life",
  "71:05": "between shops and you get this very",
  "71:07": "strong positive correlation here is an",
  "71:11": "embedding space for the days of the week",
  "71:12": "and as you can see there's a very clear",
  "71:16": "path through them here's the embedding",
  "71:18": "space for the month of the year and",
  "71:20": "again there's a very clear path through",
  "71:21": "them so like embeddings are amazing and",
  "71:26": "I don't feel like anybody's even close",
  "71:29": "to exploring the kind of interpretation",
  "71:35": "that you could get right so if you've",
  "71:38": "got genome motifs or plant species or",
  "71:45": "products that your shop sells or",
  "71:47": "whatever like it would be really",
  "71:48": "interesting to train a few models and",
  "71:52": "try and kind of fine tune some",
  "71:53": "embeddings and then like start looking",
  "71:57": "at them in these ways in terms of",
  "71:58": "similarity to other ones and clustering",
  "72:00": "them and projecting them into 2d spaces",
  "72:03": "and whatever I think is really",
  "72:05": "interesting now so we're trying to make",
  "72:10": "sure we understood what every line of",
  "72:12": "code did in this some pretty good collab",
  "72:15": "liner model we built and so the one",
  "72:17": "piece missing is this WD piece and WD",
  "72:21": "start stands for weight decay so what is",
  "72:23": "weight decay weight decay is a type of",
  "72:27": "regularization what is regularization",
  "72:30": "well let's start by going back to this",
  "72:32": "nice little chart that Andrew owned did",
  "72:34": "in his terrific machine learning course",
  "72:36": "where he plot you know plotted some data",
  "72:39": "and then",
  "72:40": "showed a few different lines through it",
  "72:42": "this one here because Andrews at",
  "72:46": "Stanford he has to use Greek letters",
  "72:48": "okay",
  "72:49": "so we couldn't say this is a plus BX but",
  "72:51": "you know if you want to go there theta",
  "72:53": "naught plus theta 1 X here's a line",
  "72:57": "right it's a line even if it's a Greek",
  "72:59": "letters is still alone so here's a",
  "73:03": "second-degree polynomial a plus BX plus",
  "73:07": "CX squared bit of curve right and here's",
  "73:10": "a high degree polynomial which is curvy",
  "73:15": "as anything so models with more",
  "73:20": "parameters tend to look more like this",
  "73:23": "and so in traditional statistics we say",
  "73:27": "hey let's use less parameters because we",
  "73:31": "don't want it to look like this because",
  "73:32": "if it looks like this then the",
  "73:34": "predictions over here and over here",
  "73:36": "they're going to be you're wrong right",
  "73:39": "it's not going to generalize well we're",
  "73:42": "overfitting",
  "73:43": "so we avoid overfitting by using less",
  "73:45": "parameters and so if any of you are",
  "73:48": "unlucky enough to have been brainwashed",
  "73:51": "by a background in statistics or",
  "73:54": "psychology or econometrics or any of",
  "73:56": "these kinds of courses you'll have you",
  "73:58": "know you're gonna have to unlearn the",
  "74:00": "idea that you need less parameters",
  "74:02": "because what you instead need to realize",
  "74:04": "this is you will fit this lie that you",
  "74:07": "need less parameters because it's a",
  "74:09": "convenient fiction for the real truth",
  "74:12": "which is you don't want your function to",
  "74:15": "be too complex and having less",
  "74:17": "parameters is one way of making it less",
  "74:20": "complex but what if you had a thousand",
  "74:24": "parameters and 999 of those parameters",
  "74:27": "were 1 a neg 9 well what if there was 0",
  "74:32": "if there's 0 they're not they're not",
  "74:33": "really there or if they want a neg 9",
  "74:35": "they're hardly there right so like why",
  "74:38": "can't I have lots of parameters if like",
  "74:40": "lots of them are really small man the",
  "74:43": "answer is you can okay you know so this",
  "74:47": "this thing of like counting the number",
  "74:49": "of parameters is how we limit complexity",
  "74:51": "is actually",
  "74:54": "extremely limiting it's a fiction that",
  "74:57": "really has a lot of problems right and",
  "75:00": "so if in your head",
  "75:01": "complexity is scored by how many",
  "75:03": "parameters you have you're doing it all",
  "75:05": "wrong right score it properly right so",
  "75:09": "why do we care why would I want to use",
  "75:12": "more parameters because more parameters",
  "75:15": "means more nonlinearities more",
  "75:19": "interactions more curvy bits right and",
  "75:23": "real life is full of curvy bits but real",
  "75:27": "life does not look like this but we",
  "75:32": "don't want them to be more curvy than",
  "75:34": "necessary or more interacting than",
  "75:37": "necessary so therefore let's use lots of",
  "75:40": "parameters and then penalize complexity",
  "75:45": "okay so one way to penalize complexity",
  "75:49": "is as I kind of suggested before is",
  "75:52": "let's sum up the value of your",
  "75:55": "parameters now that doesn't quite work",
  "75:57": "because some parameters are positive and",
  "75:59": "some are negative right so what if we",
  "76:02": "sum up the square of the parameters all",
  "76:06": "right and that's actually a really good",
  "76:09": "idea okay let's actually create a model",
  "76:12": "and in the loss function we're going to",
  "76:15": "add the sum of the square of the",
  "76:18": "parameters now here's a problem with",
  "76:21": "that though maybe that number is way too",
  "76:23": "big and it's so big that the best loss",
  "76:28": "is to set all of the parameters to zero",
  "76:32": "now that would be no good",
  "76:34": "right so actually we want to make sure",
  "76:37": "that doesn't happen so therefore let's",
  "76:40": "not just add the sum of the squares of",
  "76:42": "the parameters to the model but let's",
  "76:44": "multiply that by some number that we",
  "76:47": "choose and that number that we choose in",
  "76:50": "first AI is called WD okay so that's",
  "76:58": "what",
  "76:58": "we're gonna take our loss function and",
  "77:02": "we're going to add to it the sum of the",
  "77:03": "squares of parameters multiplied by some",
  "77:07": "number WD what should that number be",
  "77:13": "well generally it should be zero point",
  "77:17": "one people with fancy machine learning",
  "77:24": "PhDs are extremely skeptical and",
  "77:27": "dismissive of any claims that a learning",
  "77:30": "rate can be 3 in X 3 most of the time or",
  "77:33": "a weight decay can be point 1 what's the",
  "77:35": "time but here's the thing we've done a",
  "77:37": "lot of experiments on a lot of data sets",
  "77:40": "and we've had a lot of trouble finding",
  "77:41": "anywhere a weight decay of 0.1 isn't",
  "77:44": "great however we don't make that the",
  "77:49": "default we actually make the default",
  "77:51": "0.01 why because in those rare occasions",
  "77:57": "where you have too much weight decay no",
  "78:00": "matter how much you train it just never",
  "78:04": "quite fits well enough where else if you",
  "78:07": "have too little weight decay you can",
  "78:10": "still train well you're just at to",
  "78:12": "overfit so you just have to stop a",
  "78:13": "little bit early so we've been a little",
  "78:16": "bit conservative with our defaults but",
  "78:18": "my suggestion to you is this now that",
  "78:20": "you know that every learner has a wd",
  "78:24": "argument and I should mention you won't",
  "78:27": "always see it in this list right because",
  "78:30": "there's this concept of kW args in",
  "78:33": "Python which is basically parameters",
  "78:36": "that are going to get passed up the",
  "78:37": "chain to the next thing that we call now",
  "78:39": "so basically all of the learners will",
  "78:42": "call eventually this constructor and",
  "78:48": "this constructor has a WD right so this",
  "78:52": "is just one of those things that you can",
  "78:53": "either look in the docs or you now know",
  "78:56": "it anytime you're constructing a learner",
  "78:59": "from pretty much any kind of function in",
  "79:01": "fast AI you can pass WD okay and so",
  "79:05": "passing 0.1 instead of the default point",
  "79:08": "0 1 will often help ok so give it a go",
  "79:15": "so what's really going on here it would",
  "79:21": "be helpful I think to go back to lesson",
  "79:27": "two SGD because everything we're doing",
  "79:30": "the rest of today really is based on",
  "79:33": "this right and this is where we created",
  "79:35": "some data and then we try and then we",
  "79:39": "add at a loss function MSE and then we",
  "79:42": "created a function called update which",
  "79:45": "calculated our predictions",
  "79:47": "that's our weight make matrix multiply",
  "79:50": "now this is just a one layer so there's",
  "79:52": "no value we calculated our loss using",
  "79:55": "that mean squared error we calculated",
  "79:57": "the gradients using loss type backward",
  "79:59": "we then subtracted in place the learning",
  "80:04": "rate times the gradients and that is",
  "80:06": "gradient descent so if you haven't",
  "80:08": "reviewed lesson two SGD please do",
  "80:13": "because this is where we're this is our",
  "80:15": "starting point so if you don't get this",
  "80:17": "then none of this is going to make sense",
  "80:19": "if you watching the video maybe pause",
  "80:21": "now go back re-watch this part of listen",
  "80:23": "to make sure you get it remember a dot",
  "80:28": "sub underscore is basically the same as",
  "80:31": "a minus equals because a dotsub is",
  "80:36": "subtract and everything in pi torch if",
  "80:39": "you add an underscore to it means do it",
  "80:41": "in place so this is updating our a",
  "80:44": "parameters which started out as minus",
  "80:48": "0.1 one we just barbeque pick those",
  "80:50": "numbers and it gradually makes them",
  "80:52": "better all right so let's write that",
  "80:55": "down",
  "80:58": "so we are trying to calculate the",
  "81:05": "parameters I'm going to call them",
  "81:07": "weights because this is just more common",
  "81:15": "in kind of epoch tea or time tea and",
  "81:18": "they're going to be equal to whatever",
  "81:21": "the weights were in the previous epoch",
  "81:25": "- our learning rate multiplied by it's",
  "81:31": "the derivative of our loss function with",
  "81:36": "respect to our weights at time t minus 1",
  "81:42": "okay so that's that's what this is doing",
  "81:49": "okay and we don't have to calculate the",
  "81:51": "derivative because it's boring and",
  "81:53": "because it computers do it for us fast",
  "81:57": "and then they store it here for us so",
  "82:01": "we're good to go okay so make sure",
  "82:06": "you're exceptionally comfortable with",
  "82:09": "either that equation or that line of",
  "82:14": "code because they are the same thing",
  "82:19": "where do we go from here all right so",
  "82:25": "what's that what's our loss our loss is",
  "82:31": "some function of our independent",
  "82:37": "variable variables X and now weights",
  "82:44": "right and in our case we're using mean",
  "82:50": "squared error for example and it's",
  "82:53": "between our predictions and our actuals",
  "82:57": "right so where does X and W come in",
  "83:01": "well our predictions come from running",
  "83:04": "some model we'll call it m on those",
  "83:09": "predictions and that model contains some",
  "83:11": "weights all right so that's that's what",
  "83:14": "our loss function might be and this",
  "83:16": "might be you all kinds of other loss",
  "83:17": "functions will see some more today and",
  "83:20": "so that's what ends up creating",
  "83:27": "grab over here so we're going to do",
  "83:33": "something else we're going to add weight",
  "83:37": "decay some number which in our case is",
  "83:40": "0.1 x times the sum of weights squared",
  "83:58": "okay so let's do that and let's make it",
  "84:05": "interesting by not using synthetic data",
  "84:09": "but let's do some real data and we're",
  "84:12": "going to use em NIST the hand-drawn",
  "84:14": "digits right but we're going to do this",
  "84:17": "as a standard fully connected net not as",
  "84:21": "a convolutional net because we haven't",
  "84:22": "learnt the details of how to really",
  "84:24": "create one of those from scratch so in",
  "84:27": "this case is actually deep learning net",
  "84:29": "provides amnesty as a python pickle file",
  "84:34": "in other words it's a file that pickle",
  "84:35": "that Python can just open up and it'll",
  "84:38": "give you numpy arrays straight away and",
  "84:40": "they're flat and umpire rays we don't",
  "84:42": "have to do anything to them so go grab",
  "84:44": "that and it's a gzip file so you can",
  "84:49": "actually just gzip don't open it",
  "84:50": "directly and then you can pick all",
  "84:54": "download it directly and again encoding",
  "84:57": "equals latin-1 because yeah you know and",
  "85:01": "then we can just put that static that'll",
  "85:03": "give us the training the validation and",
  "85:05": "the test set I don't care about the test",
  "85:07": "set so generally in Python if there's",
  "85:09": "like something you don't care about you",
  "85:11": "tend to use this special variable called",
  "85:13": "underscore there's no reason you have to",
  "85:15": "it's just kind of people know you mean I",
  "85:17": "don't care about this right so there's",
  "85:19": "our trading trading X&Y; and a valid X&Y;",
  "85:23": "now this actually comes in as a as you",
  "85:26": "can see if I print the shape 50,000 rows",
  "85:29": "by 784 columns but those 784 columns are",
  "85:32": "actually 28 by 28 pixel pictures so if I",
  "85:36": "reshape one of them into a 28 by 28",
  "85:38": "pixel picture and plot it",
  "85:41": "right then you can see it's the number",
  "85:42": "five okay so that's our data we've seen",
  "85:45": "em nest before in its kind of pre",
  "85:48": "reshaped version here it is in its",
  "85:49": "flattened version so I'm going to be",
  "85:51": "using it in its flattened version okay",
  "85:54": "and currently they are numpy arrays I",
  "85:58": "need them to be tensors so I can just",
  "86:01": "map Torche tensor across all of them and",
  "86:04": "so now they're tensors okay I may as",
  "86:10": "well create a variable with the number",
  "86:12": "of things I have which we normally call",
  "86:13": "n and remember we normally have a thing",
  "86:15": "called you know we don't use seed I mean",
  "86:17": "the number of activations we need we",
  "86:20": "actually say this is not going to be",
  "86:21": "activation sorry this is going to be",
  "86:22": "number of columns that's not a great",
  "86:24": "name for it sorry okay so there we are",
  "86:28": "and then the the why not surprisingly",
  "86:32": "the minimum value is zero and the",
  "86:34": "maximum value is nine because that's the",
  "86:36": "actual number we're gonna predict great",
  "86:38": "so in lesson two SGD we like we created",
  "86:42": "a data where we actually added a column",
  "86:44": "of ones on so that we didn't have to",
  "86:47": "worry about bias we're not going to do",
  "86:49": "that we're going to have plate watch to",
  "86:51": "do that kind of implicitly for us we had",
  "86:53": "to write our own MSC function we're not",
  "86:55": "going to do that we had to write our own",
  "86:57": "little matrix model location thing we're",
  "87:00": "not going to do that we're gonna have",
  "87:01": "plate or do all this stuff for us now",
  "87:03": "okay",
  "87:03": "and what's more and really important",
  "87:06": "we're going to do mini batches right",
  "87:09": "because this is a big enough data set we",
  "87:10": "probably don't want to do it all at once",
  "87:13": "so if you want to do mini batches so we",
  "87:17": "could we're not going to use too much",
  "87:18": "faster i stuff here apply torch has",
  "87:22": "something called intense a data set that",
  "87:24": "basically grabs a any kind of tensor",
  "87:30": "sorry two tensors and creates a data set",
  "87:33": "remember a data set is something where",
  "87:35": "if you index into it you get back an x",
  "87:38": "value and a y value just one of them",
  "87:40": "okay so it kind of looks like it looks a",
  "87:44": "lot like a list of XY tuples once you",
  "87:49": "have a data set then you can use a",
  "87:51": "little bit of convenience by calling",
  "87:54": "data by",
  "87:55": "create and what's that going to do is",
  "87:57": "it's going to create data loaders for",
  "87:59": "you a data loader is something which you",
  "88:03": "don't say I want the first thing or the",
  "88:05": "fifth thing you just say I want the next",
  "88:07": "thing and it will give you a batch a",
  "88:10": "mini batch of whatever size you asked",
  "88:12": "for and specifically it'll give you the",
  "88:14": "X and the y of a mini batch so if I just",
  "88:17": "grab the next of the iterator this is",
  "88:20": "just standard Python if you haven't used",
  "88:22": "it a radius in Python before here's my",
  "88:25": "training data loader that data bunch",
  "88:27": "courier creates for you and you can",
  "88:31": "check that as you would expect the X is",
  "88:34": "64 by 784 because there's 784 pixels",
  "88:37": "flattened out 64 in a mini batch and the",
  "88:40": "Y is just 64 numbers there are things",
  "88:43": "we're trying to predict so and you know",
  "88:47": "if you look at the source code for data",
  "88:48": "batch job create you'll see there's not",
  "88:50": "much there but so feel free to do so we",
  "88:53": "just make sure that like your training",
  "88:54": "set gets shuffled randomly shuffled for",
  "88:57": "you we make sure that the data is put on",
  "89:00": "the GPU for you just a couple of little",
  "89:03": "convenience things like that but don't",
  "89:06": "let it be magic",
  "89:07": "if it feels magic check out the source",
  "89:09": "code to make sure you see what's going",
  "89:10": "on okay",
  "89:11": "so rather than do this y hat equals x at",
  "89:15": "a thing we're going to create an NN",
  "89:18": "module all right if you want to create",
  "89:20": "an end up module that does something",
  "89:22": "different to what's already out there",
  "89:24": "you have to subclass it right so sub",
  "89:27": "classing is very very very normal in",
  "89:30": "plaid torch so if you're not comfortable",
  "89:32": "with sub classing stuff in python go",
  "89:36": "read a couple of tutorials to make sure",
  "89:38": "you our main thing is you have to",
  "89:39": "override the constructor dunder init and",
  "89:42": "make sure that you call the super",
  "89:45": "classes constructor because n n dot",
  "89:47": "modules super classes constructor is",
  "89:49": "going to like set it all up to be a",
  "89:51": "proper n n dot module for you so if you",
  "89:55": "try to using if you're trying to create",
  "89:56": "your own PI torch subclass and things",
  "89:58": "don't work it's almost certainly because",
  "90:00": "you forgot this line of code alright so",
  "90:05": "the only thing we want to add is we want",
  "90:07": "to create",
  "90:08": "and an attribute in our class which",
  "90:11": "contains a linear layer and n n dot",
  "90:14": "linear module what is an N n dot linear",
  "90:16": "module it's something which does that",
  "90:21": "but actually it doesn't only do that it",
  "90:23": "actually is X at a plus B so in other",
  "90:26": "words we don't have to add the column of",
  "90:27": "ones okay that's all it does okay so if",
  "90:31": "you want to play around why don't you",
  "90:34": "try and create your own and end on",
  "90:37": "linear class you could create something",
  "90:39": "called my linear and it'll take you you",
  "90:43": "know depending on your piped watch",
  "90:45": "background an hour or two and then",
  "90:48": "you'll feel like okay this is we don't",
  "90:50": "want any of this to be magic and you",
  "90:52": "know all of the things necessary to",
  "90:53": "create this know so you know these are",
  "90:56": "the kind of things that you should be",
  "90:57": "doing for your assignments this week is",
  "90:59": "not so much new applications but try to",
  "91:02": "start writing more of these things from",
  "91:04": "scratch and get them to work learn how",
  "91:06": "to debug them check what's going in and",
  "91:08": "out and so forth okay but we could just",
  "91:11": "use n n dot linear and that's this going",
  "91:12": "to do so it's going to have a def",
  "91:14": "forward inner there goes a at X plus B",
  "91:18": "right and so then in our forward how do",
  "91:22": "we calculate the result of this well",
  "91:24": "remember every NN dot module looks like",
  "91:27": "a function so we pass our X mini-batch",
  "91:30": "so I don't use xB to mean a batch of X",
  "91:33": "to self dot Lin and that's going to give",
  "91:36": "us back the result of the ax plus B on",
  "91:40": "this mini batch so this is a logistic",
  "91:44": "regression model a logistic regression",
  "91:46": "model is also known as a neural net with",
  "91:48": "no hidden layers so it's a one layer",
  "91:51": "neural net no nonlinearities because",
  "91:55": "we're doing stuff ourself a little bit",
  "91:57": "we have to put the weight matrices the",
  "92:00": "parameters onto the GPU manually so just",
  "92:04": "type CUDA to do that so here's our model",
  "92:07": "and as you can see the end module",
  "92:10": "machinery has automatically given us a",
  "92:13": "representation of it it's automatically",
  "92:15": "stored the dot Lin thing and it's",
  "92:17": "telling us what's inside it so there's a",
  "92:19": "lot of little conveniences that PI torch",
  "92:20": "does for us",
  "92:21": "so if you look at now at model n you can",
  "92:24": "see not surprisingly here it is perhaps",
  "92:27": "the most interesting thing to point out",
  "92:29": "is that our model automatically gets a",
  "92:35": "bunch of methods and properties and",
  "92:38": "perhaps the most interesting one is the",
  "92:40": "one called parameters which contains all",
  "92:43": "of the yellow squares from our picture",
  "92:46": "but it contains our parameters it",
  "92:49": "contains our weight matrices and biased",
  "92:52": "matrices in as much as they're different",
  "92:54": "so if we have a look at P dot shape for",
  "92:57": "P and modeled up parameters there's",
  "92:59": "something of ten by two 784 and there's",
  "93:03": "something of ten so what are they well",
  "93:05": "ten by 784 okay so that's the thing",
  "93:08": "that's going to take in 784 dimensional",
  "93:11": "input and spit out a 10 dimensional",
  "93:12": "output because that's handy because our",
  "93:14": "input is 784 dimensional and we need",
  "93:17": "something that's going to give us a",
  "93:18": "probability of ten numbers after that",
  "93:21": "happens we've got ten activations which",
  "93:23": "we then want to add the bias to so there",
  "93:26": "we go here's a vector of length 10 so",
  "93:29": "you can see why this this model we've",
  "93:33": "created has exactly the stuff that we",
  "93:35": "need to do our ax plus B so let's grab a",
  "93:41": "learning rate we're going to come back",
  "93:43": "to this loss function in a moment but we",
  "93:44": "can't use em as well hmm we can't really",
  "93:48": "use MSE for this right because we're not",
  "93:50": "trying to see how close are you did you",
  "93:52": "predict three and actually it was four",
  "93:54": "gosh you were really close it's like no",
  "93:56": "three is just as far away from four as",
  "93:58": "zero is away from four when you're",
  "94:01": "trying to predict what number did",
  "94:02": "somebody draw so we're not going to use",
  "94:04": "MSE we're going to use cross-entropy",
  "94:06": "loss which we'll look at in a moment and",
  "94:08": "here's our update function I copied it",
  "94:11": "from less than two SGD but now we're",
  "94:15": "calling our model rather than going a at",
  "94:17": "X we're calling our model as if it was a",
  "94:19": "function to get Y hat and we're calling",
  "94:23": "our loss func rather than calling MSE to",
  "94:26": "get our loss and then this is all the",
  "94:28": "same as before except rather than going",
  "94:32": "through each parameter and going",
  "94:34": "parameter",
  "94:35": "sub underscore learning rate times",
  "94:37": "gradient we loop through the parameters",
  "94:39": "okay because very nicely for us pipe",
  "94:45": "torch will automatically create this",
  "94:46": "list of the parameters of anything that",
  "94:48": "we created in our dunder init and look",
  "94:51": "I've added something else I've got this",
  "94:54": "thing called w2 I go through HP and",
  "94:57": "model drop parameters and I add two",
  "94:59": "double to w2 the sum of squares so w-2",
  "95:05": "now contains my summer squared sweets",
  "95:06": "and then I multiply it by some number",
  "95:10": "which I set to one a neg five so now I",
  "95:14": "just implemented weight decay okay so",
  "95:17": "when people talk about weight decay it's",
  "95:20": "not an amazing magic complex thing",
  "95:22": "containing thousands of lines of CUDA",
  "95:25": "C++ code it's those two lines of Python",
  "95:31": "that's weight okay this is not a",
  "95:33": "simplified version that's just enough",
  "95:35": "for now this is weight okay that's it",
  "95:37": "okay and so here's the thing there's a",
  "95:42": "really interesting kind of drool way of",
  "95:45": "thinking about weight decay one is that",
  "95:48": "we're adding the sum of squares weights",
  "95:50": "and that seems like a very sound thing",
  "95:53": "to do and it is and well let's go ahead",
  "95:56": "and run this",
  "96:02": "so here I've just got a list",
  "96:03": "comprehension that's going through my",
  "96:06": "data loader so the data loader gives you",
  "96:09": "back one mini batch and it's for the",
  "96:11": "whole thing giving you XY each time I'm",
  "96:14": "gonna call update for each each one",
  "96:17": "returns loss now PI torch tensors since",
  "96:24": "I did it all on the GPU that's sitting",
  "96:25": "in the GPU and it's like got all these",
  "96:27": "stuff attached to it to calculate",
  "96:28": "gradients it's going to use up a lot of",
  "96:31": "memory so if you if you called dot item",
  "96:33": "on a scalar tensor it turns it into an",
  "96:36": "actual normal Python number so this is",
  "96:39": "just means I'm returning back normal",
  "96:41": "Python numbers and then I can plot them",
  "96:44": "and yeah there you go my loss function",
  "96:47": "is going down",
  "96:48": "and you know it's really nice to try",
  "96:51": "this stuff to see it behaves as you",
  "96:52": "expect like we thought this is what",
  "96:54": "would happen as we get closer and closer",
  "96:56": "to the answer it bounces around more and",
  "96:59": "more right because we're kind of close",
  "97:01": "to where we should be",
  "97:02": "it's kind of fitting flat probably",
  "97:03": "flatter and weight space so we kind of",
  "97:05": "jumping further and so you can see why",
  "97:07": "we would probably want to be reducing",
  "97:09": "our learning rate as we go learning rate",
  "97:11": "annealing okay now here's the thing that",
  "97:17": "is only interesting for training a",
  "97:22": "neural net because it appears here",
  "97:29": "because we take the gradient of it",
  "97:33": "that's the thing that actually updates",
  "97:35": "the weights right so they actually the",
  "97:37": "only thing interesting about WD times",
  "97:41": "sum of W squared is its gradient so we",
  "97:45": "don't do a lot of math here but I think",
  "97:47": "we can handle that the gradient of this",
  "97:51": "whole thing if you remember back to your",
  "97:53": "high school math is equal to the",
  "97:56": "gradient of H part taken separately and",
  "97:59": "then add them together so let's just",
  "98:02": "take the gradient of that right because",
  "98:04": "we already know the gradient of this is",
  "98:05": "just whatever we had before right so",
  "98:07": "what's the gradient of W D times the sum",
  "98:09": "of W squared right let's remove the",
  "98:13": "psalm and pretend there's just one",
  "98:14": "parameter it doesn't change the",
  "98:16": "generality of it so the gradient of W D",
  "98:20": "times W squared so what's the gradient",
  "98:24": "of that with respect to W it's just two",
  "98:30": "WD times W ok and so remember this is",
  "98:37": "our constant which now case was like",
  "98:39": "well in that little loop it was 1 e neg",
  "98:42": "5",
  "98:45": "and that's our weights and like we could",
  "98:48": "replace WD with like 2wd without loss of",
  "98:52": "generality so let's throw away the two",
  "98:54": "so in other words all weight decay does",
  "98:58": "is it subtracts some constant times the",
  "99:02": "weights every time we do a batch so",
  "99:07": "that's why it's called weight decay okay",
  "99:10": "when it's in this form where we add the",
  "99:14": "square to the loss function that's",
  "99:18": "called l2 regularization when it's in",
  "99:24": "this form where we subtract WD times",
  "99:28": "weights from the gradients",
  "99:30": "that's called weight decay and they are",
  "99:38": "kind of mathematically identical for",
  "99:40": "everything we've seen so far in fact",
  "99:41": "they are mathematically identical and",
  "99:44": "we'll see in a moment a place where",
  "99:45": "they're not where are things get",
  "99:46": "interesting ok so this is just a really",
  "99:50": "important tool you now have in your",
  "99:52": "toolbox you can make giant neural",
  "99:55": "networks right and still avoid",
  "99:58": "overfitting by adding more weight decay",
  "100:01": "okay or you could use really small data",
  "100:04": "sets with moderately large sized models",
  "100:08": "and avoid overfitting with weight decay",
  "100:10": "it's not magic right like you might",
  "100:14": "still find you don't have enough data in",
  "100:15": "which case like you get to the point",
  "100:17": "where you're not overfitting by adding",
  "100:18": "lots of weight decay and it's just not",
  "100:20": "training very well that can happen all",
  "100:22": "right but at least this is something",
  "100:24": "that Union can now play around with just",
  "100:30": "to kind of go on here now that we've got",
  "100:34": "this update function we could replace",
  "100:36": "this M missed logistic with amnesty row",
  "100:40": "Network and build a neural network from",
  "100:41": "scratch right now we just need two",
  "100:44": "linear layers right in the first one we",
  "100:47": "could use a weight matrix of size 50 and",
  "100:49": "so we didn't need to make sure that the",
  "100:50": "second linear layer has an input of size",
  "100:52": "50 so it matches the final layer has to",
  "100:56": "have an output of size 10",
  "100:57": "that's the number of classes we're",
  "100:58": "predicting and so now our forward just",
  "101:00": "goes to a linear layer calculate value",
  "101:04": "to a second linear layer and now we've",
  "101:07": "actually created a neural net from",
  "101:09": "scratch I mean we didn't write it in",
  "101:11": "linear but you can write it yourself or",
  "101:13": "you could like do the matrices directly",
  "101:16": "you know how to so again you know if we",
  "101:20": "go model dot CUDA and then we can",
  "101:23": "calculate losses for the exact same",
  "101:24": "update function there it goes right so",
  "101:28": "this is why this kind of idea of neural",
  "101:30": "nets is so easy right once you have",
  "101:32": "something that can do gradient descent",
  "101:34": "right then you can try different models",
  "101:38": "and then you can start to add more Pytor",
  "101:42": "stuff so like rather than add doing all",
  "101:43": "this stuff yourself why not just go opt",
  "101:49": "equals opt e m dot something so there's",
  "101:53": "something we've done so far is SGD and",
  "101:56": "so now you're saying 2pi torch i want",
  "102:00": "you to take these parameters and",
  "102:02": "optimize them using SGD and so this now",
  "102:06": "rather than saying for P in parameters P",
  "102:10": "minus equals L R times P dot grad you",
  "102:14": "just say up dot step it's the same thing",
  "102:16": "okay it's just less code right but and",
  "102:20": "it does the same thing but the reason",
  "102:22": "it's kind of particularly interesting is",
  "102:23": "that now you can replace SGD with atom",
  "102:28": "for example and you can even add things",
  "102:32": "like weight decay right because like",
  "102:37": "there's more stuff it's kind of in these",
  "102:40": "things for you right so that's why we",
  "102:43": "tend to use you know optiom glass so",
  "102:46": "behind the scenes this is actually what",
  "102:48": "we do in first ago so if I go up to m",
  "102:55": "dot sgt okay so this",
  "103:01": "right and so that's that's just that",
  "103:03": "picture but if we change to a different",
  "103:05": "optimizer so look what happened it",
  "103:14": "diverged and we've seen a great picture",
  "103:17": "of that from one of our students who",
  "103:19": "showed what divergence looks like this",
  "103:21": "is what it looks like when you try to",
  "103:22": "train something so let's use we're using",
  "103:25": "a different optimizer so we need a",
  "103:26": "different learning rate and you can't",
  "103:29": "just continue training because by the",
  "103:31": "time it's diverged the the the weights",
  "103:33": "are like really really big and really",
  "103:35": "really small they're not going to come",
  "103:36": "back so start again okay there's a",
  "103:41": "better learning rate but look at this",
  "103:43": "we're down underneath point five by",
  "103:45": "about epoch 200 where else before and",
  "103:48": "I've even sure we ever got to quite that",
  "103:51": "level so what's going on what's what's",
  "103:54": "Adam let me show you and we're gonna do",
  "104:01": "gradient descent in Excel because why",
  "104:04": "wouldn't you okay so here is some",
  "104:09": "randomly generated data okay some X's",
  "104:12": "and some whites well they're actually",
  "104:13": "they're randomly generated XS and the",
  "104:14": "Y's are all calculated by doing ax plus",
  "104:19": "B where a is 2 and B is 30 okay so this",
  "104:23": "is some data that we got to try and",
  "104:24": "match and here is SGD and so we got to",
  "104:31": "do it with SGD now in our lesson to SGD",
  "104:34": "notebook we did the whole data set at",
  "104:37": "once as a batch in the notebook we just",
  "104:41": "looked at we did mini batches in this",
  "104:43": "spreadsheet we're going to do online",
  "104:45": "gradient descent which means every",
  "104:47": "single row of data is a batch there's",
  "104:49": "kind of a batch size of one okay so as",
  "104:52": "per usual we're going to start by",
  "104:53": "picking an intercept and slope kind of",
  "104:56": "arbitrarily so I'm just going to pick",
  "104:57": "them at 1 doesn't really matter",
  "105:01": "so here I've copied over the data this",
  "105:03": "is my X&Y; and so my intercept and slope",
  "105:06": "as I said is 1 all right I'm just",
  "105:08": "literally referring back to this cell",
  "105:09": "here so my prediction for this",
  "105:13": "particular intercept them",
  "105:14": "would be 14 times one plus one which is",
  "105:17": "15 and so there's my error means that",
  "105:21": "there's my summer Squared's but not even",
  "105:23": "a sum at this point it's the squared",
  "105:24": "error okay so now I need to calculate",
  "105:29": "the gradient so that I can update",
  "105:31": "there's two ways you can calculate the",
  "105:33": "gradient one is analytically and so I",
  "105:37": "you know you can just look them up on",
  "105:39": "Wolfram Alpha or whatever so there's the",
  "105:40": "gradients if you write it out by hand or",
  "105:44": "look it up or you can do something",
  "105:46": "called finite differencing because",
  "105:47": "remember gradients just how far you move",
  "105:51": "in act sorry how far you how far the the",
  "105:53": "outcome moves divided by how far your",
  "105:56": "change was for really small changes so",
  "106:00": "let's just make a really small change so",
  "106:06": "here we've taken our intercept and added",
  "106:10": "point O 1 to it right and then",
  "106:13": "calculated our our loss and you can see",
  "106:17": "that our loss went down a little bit",
  "106:21": "right and we added 0.01 here so our",
  "106:24": "derivative is that difference divided by",
  "106:27": "that point I 1 okay now that's called",
  "106:29": "finite differencing and you can always",
  "106:31": "do derivatives of find out different",
  "106:32": "seeing it's slow we don't do it in",
  "106:35": "practice but it's nice for just checking",
  "106:36": "stuff out so we can do the same thing",
  "106:38": "for our a term at 0.012 that take the",
  "106:42": "difference and divide by 0.01 or as I",
  "106:45": "say we can calculate it directly using",
  "106:47": "the actual derivative analytical and you",
  "106:49": "can see that you know that and that as",
  "106:52": "you'd expect it's very similar and that",
  "106:54": "and that not very similar so gradient",
  "106:58": "descent then just says let's take our",
  "107:01": "current value of that weight and",
  "107:03": "subtract the learning rate times the",
  "107:06": "derivative there it is okay and so now",
  "107:10": "we can copy that intercept and that",
  "107:16": "slope to the next row and do it again",
  "107:19": "and do it lots of times and at the end",
  "107:23": "we've done one epoch so at the end of",
  "107:26": "that epoch",
  "107:27": "we could say oh great so this is our",
  "107:31": "slope so let's copy that over to where",
  "107:35": "it says slope and this is our intercept",
  "107:42": "so I'll copy it to where it says",
  "107:43": "intercept and now it's done another",
  "107:47": "epoch okay so that's kind of boring I'm",
  "107:51": "copying and pasting so I created a very",
  "107:55": "sophisticated macro which copies and",
  "107:59": "pastes for you and so I just recorded it",
  "108:03": "basically and so and then I created a",
  "108:06": "very sophisticated for loop that goes",
  "108:07": "through and does it five times and I",
  "108:10": "attach that to the Run button so if I",
  "108:12": "press run it'll go ahead and do it five",
  "108:14": "times and just keep track of the era",
  "108:17": "each time okay so that is SGD and as you",
  "108:22": "can see it is just infuriatingly slow",
  "108:27": "like particularly the intercept is meant",
  "108:30": "sorry yeah it's meant to be 30 and we're",
  "108:34": "still only up to one point five seven",
  "108:36": "and like just it's just going so slowly",
  "108:40": "so let's beat it up so the first thing",
  "108:42": "we can do to speed it up is to use them",
  "108:44": "in court momentum right so here's the",
  "108:46": "exact same spreadsheet is the last",
  "108:50": "worksheet I've removed the finite",
  "108:52": "difference seeing version of the",
  "108:53": "derivatives because they're not that",
  "108:55": "useful does the analytical ones here and",
  "108:58": "here's the thing where I take the the",
  "109:02": "derivative and I'm going to update by",
  "109:08": "the derivative but what I do it's kind",
  "109:11": "of more interesting to look at this one",
  "109:12": "is I take the derivative and I multiply",
  "109:15": "it by 0.1 now what I do is I look at the",
  "109:20": "previous update and I multiply that by",
  "109:23": "0.9 and I add the two together so in",
  "109:27": "other words the update that I do is not",
  "109:31": "just based on the derivative but 1/10 of",
  "109:35": "it is the derivative and 90% of it is",
  "109:38": "just the same direction I went last",
  "109:41": "and this is called momentum right what",
  "109:45": "it means is remember how we kind of",
  "109:51": "thought about what might happen",
  "109:54": "if you're trying to find the minimum of",
  "109:57": "this and you were here and your learning",
  "109:59": "rate was too small right and you just",
  "110:02": "keep doing the same steps or if you keep",
  "110:05": "doing the same steps then if you also",
  "110:07": "add in the step you talked last time and",
  "110:13": "your steps are going to get bigger and",
  "110:14": "bigger aren't they okay until eventually",
  "110:18": "they go too far but now of course your",
  "110:23": "gradients point in the other direction",
  "110:24": "to whether your momentum is pointing so",
  "110:26": "you might just take a little step over",
  "110:28": "here and then you'll start going small",
  "110:30": "steps bigger steps bigger steps small",
  "110:32": "steps bigger stairs like that right so",
  "110:35": "that's kind of what momentum does or if",
  "110:37": "you're if you're kind of going too far",
  "110:47": "like this which is also slow all right",
  "110:52": "then the average of your last few steps",
  "110:55": "is actually somewhere kind of between",
  "111:00": "the two isn't it all right so this is a",
  "111:04": "really common idea right it's like when",
  "111:07": "you have something that says kind of my",
  "111:11": "what is in this case it's like my step",
  "111:15": "my step at time T equals some number",
  "111:21": "people often use alpha because like I",
  "111:24": "say they've got to love these Greek",
  "111:25": "letters some number times the actual",
  "111:31": "thing I want to do right so it might in",
  "111:34": "this case it's like the gradient right",
  "111:36": "plus one minus alpha times whatever you",
  "111:42": "had last time st minus one this thing",
  "111:50": "here",
  "111:52": "is called an exponentially weighted",
  "111:54": "moving average and the reason why is",
  "111:56": "that if you think about it these 1 minus",
  "111:59": "alphas are going to mount a play so s T",
  "112:02": "minus 2 is in here with a kind of a 1",
  "112:06": "minus alpha squared and s T minus 3 is",
  "112:08": "in there with a net 1 minus alpha cubed",
  "112:10": "so in other words this ends up being the",
  "112:15": "actual thing I want plus a weighted",
  "112:18": "average of the last few time periods",
  "112:20": "where the most recent ones are",
  "112:23": "exponentially higher weighted ok and",
  "112:26": "this is going to keep popping up again",
  "112:28": "and again all right so that's what",
  "112:30": "momentum is it says I want to go based",
  "112:32": "on the current gradient plus the",
  "112:36": "exponentially weighted moving average of",
  "112:38": "my last few steps so that's useful",
  "112:42": "that's called SGD with momentum and we",
  "112:46": "can do it by changing this here to",
  "112:49": "saying SGD momentum and momentum 0.9 is",
  "112:54": "really common it's couple add a lot it's",
  "112:56": "like it's so common it's always pointing",
  "112:57": "in just about four basic stuff so that's",
  "113:02": "how you do rest you deal with momentum",
  "113:03": "and and again it's not I didn't show you",
  "113:06": "some simplified version I showed you the",
  "113:09": "version that is that is SGD ok that's",
  "113:12": "that's you again you can write your own",
  "113:13": "try it out that would be a great",
  "113:15": "assignment would be to take lesson to",
  "113:18": "SGD and add momentum to it or even the",
  "113:22": "new notebook we've got feminists get rid",
  "113:25": "of the OP team dot and write your own",
  "113:27": "update function with with momentum then",
  "113:31": "there's a cool thing called rmsprop one",
  "113:32": "of the really cool things about rmsprop",
  "113:34": "is that Geoffrey Hinton created it",
  "113:38": "famous neural net guy everybody uses it",
  "113:42": "it's like really popular it's really",
  "113:44": "common the correct citation for rmsprop",
  "113:48": "is the Coursera online free MOOC that",
  "113:52": "that's where he first mentioned rmsprop",
  "113:56": "so I love this thing that like you know",
  "113:59": "call new things appear in MOOCs that not",
  "114:01": "a paper so rmsprop is very similar to",
  "114:05": "momentum",
  "114:05": "but this time we have an exponentially",
  "114:09": "weighted moving average not of the",
  "114:11": "gradient updates but of f/8 squared",
  "114:16": "that's the gradient squared so what the",
  "114:19": "gradient squared times 0.1 plus the",
  "114:24": "previous value times 0.9 so it's",
  "114:28": "exponentially this is an exponentially",
  "114:29": "weighted moving average of the gradient",
  "114:31": "squared so what's this number gonna mean",
  "114:33": "well if my gradients really small and",
  "114:36": "consistently really small this will be a",
  "114:39": "small number if my gradient is highly",
  "114:43": "volatile it's going to be a big number",
  "114:45": "or if it's just really big all the time",
  "114:48": "it'll be a big number and why is that",
  "114:50": "interesting because when we do a update",
  "114:54": "this time we say wait - learning rate",
  "115:00": "times gradient divided by the square",
  "115:08": "root of this so in other words if our",
  "115:11": "gradients consistently very small and",
  "115:13": "not volatile let's take bigger jumps and",
  "115:16": "that's kind of what we want right when",
  "115:18": "we watched how the intercept moves so",
  "115:21": "damn slowly but it just it's like",
  "115:24": "obviously you need to just try it go",
  "115:26": "faster so if I now run this after just",
  "115:32": "five epochs this is already up to three",
  "115:34": "right where else with the basic version",
  "115:36": "after five epochs it's still at 1.27 and",
  "115:43": "remember we have to get to 30 so the",
  "115:46": "obvious thing to do and by obvious I",
  "115:48": "mean only a couple of years ago did",
  "115:50": "anybody actually figure this out is do",
  "115:53": "both right so that's called Adam so Adam",
  "115:57": "is simply keep track of the",
  "115:59": "exponentially weighted moving average of",
  "116:01": "the gradient squared and also keep track",
  "116:05": "of the exponentially weighted moving",
  "116:07": "average of my steps right and both",
  "116:11": "divided by the exponentially weighted",
  "116:15": "moving average of the squared terms",
  "116:17": "and you know take point nine of a step",
  "116:21": "in the same direction as last time so",
  "116:23": "it's it's momentum and rmsprop that's",
  "116:27": "court Adam and look at this okay",
  "116:35": "five steps we're at 25 okay so you know",
  "116:40": "these these are these optimizes people",
  "116:42": "call them dynamic learning rates a lot",
  "116:44": "of people have the misunderstanding that",
  "116:46": "you don't have to set a learning rate of",
  "116:49": "course you do right it's just like",
  "116:52": "trying to identify parameters that need",
  "116:56": "to move faster you know or consistently",
  "116:58": "go in the same direction it doesn't mean",
  "117:00": "you don't need learning rates we still",
  "117:02": "have a learning rate okay and in fact",
  "117:06": "you know if I run this again but",
  "117:09": "currently my my error not distribute so",
  "117:17": "it we trying to get to 30 comma 2 so if",
  "117:21": "I run it again it's getting better but",
  "117:30": "eventually now it's just moving around",
  "117:33": "the same place",
  "117:35": "right so you can see what's happened is",
  "117:37": "the learning rates too high so we could",
  "117:39": "just go in here and drop it down and run",
  "117:42": "it some more getting pretty close now",
  "117:47": "right so you can see how you still need",
  "117:50": "learning rate annealing even with Adam",
  "117:56": "okay so that spreadsheets fun to play",
  "118:00": "around with I do have a Google sheets",
  "118:03": "version of basic SGD that actually works",
  "118:08": "and the macros work and everything",
  "118:10": "Google sheets is so awful and I went so",
  "118:13": "insane making that work I gave up I'm",
  "118:15": "making the other ones work so I'll share",
  "118:17": "a link to the Google sheets version oh",
  "118:21": "my god they do have a macro language but",
  "118:25": "it's just ridiculous so anyway if",
  "118:28": "somebody feels like fighting it to",
  "118:29": "actually get all the other ones to work",
  "118:31": "we'll work it just assistant so maybe",
  "118:34": "somebody can get this working on Google",
  "118:36": "sheets too okay so that's weight decay",
  "118:39": "and Adam and Adam is amazingly fast and",
  "118:51": "we let's go back to this one but we",
  "118:56": "don't tend to use op teamed-up",
  "119:00": "whatever and create the optimizer",
  "119:02": "ourselves and all that stuff because",
  "119:03": "instead we had to use learner but learn",
  "119:07": "is just doing those things for you but",
  "119:10": "again there's no magic right so if you",
  "119:12": "create and learner you say here's my",
  "119:15": "data bunch here's my PI torch and n dot",
  "119:19": "module instance here's my loss function",
  "119:22": "and here are my metrics remember the",
  "119:25": "metrics are just stuff to print out",
  "119:27": "that's it right then you just get a few",
  "119:31": "nice things like learned at LR fine",
  "119:34": "starts working and it starts recording",
  "119:35": "this and you can say fit one cycle",
  "119:38": "instead of just fit but like these",
  "119:40": "things really help a lot like by using",
  "119:42": "the floating rate finder I found a good",
  "119:44": "learning rate and then like look at this",
  "119:46": "my loss here 0.13 here I wasn't getting",
  "119:50": "much beneath point five right so these",
  "119:52": "these tweets make huge differences not",
  "119:56": "tiny differences and this is still just",
  "119:59": "one one epoch now what does fit one",
  "120:03": "cycle do what does it really do this is",
  "120:08": "what it really does right and we've seen",
  "120:11": "this chart on the left before just to",
  "120:12": "remind you this is plotting the learning",
  "120:16": "rate per batch right remember Adam has a",
  "120:19": "learning rate and we use Adam by default",
  "120:22": "or minor variation which we might try to",
  "120:25": "talk about so the learning rate starts",
  "120:28": "really low and it increases about half",
  "120:31": "the time and then it decreases about",
  "120:35": "half the time because at the very start",
  "120:37": "we don't know where we are right so",
  "120:40": "we're in some part of function space",
  "120:41": "it's just bump years or hell all right",
  "120:44": "so if you start",
  "120:45": "jumping around those bumps have big",
  "120:47": "gradients and it will throw you into",
  "120:48": "crazy parts of the space right so start",
  "120:51": "slow and then you'll gradually move into",
  "120:53": "parts of the weight space that you know",
  "120:56": "and they're kind of sensible and as you",
  "120:59": "get to the points where they're sensible",
  "121:00": "you can increase the learning rate you",
  "121:02": "know because the the gradients jedd",
  "121:06": "actually in the direction you want to go",
  "121:08": "right and then as we've discussed a few",
  "121:10": "times as you get close to the final",
  "121:11": "answer",
  "121:12": "you need to anneal your learning rate to",
  "121:15": "hone in on it but here's the interesting",
  "121:17": "thing on the left is the momentum plot",
  "121:21": "and actually every time our learning",
  "121:23": "rate is small our momentum is high why",
  "121:27": "is that because if you I do have a",
  "121:30": "learning small learning rate but you",
  "121:31": "keep going in the same direction you may",
  "121:34": "as well go faster right but if you're",
  "121:37": "jumping really far don't like jump jump",
  "121:41": "really far because it's going to throw",
  "121:43": "you off right and then as you get to the",
  "121:45": "end again you're fine tuning in but",
  "121:48": "actually if you keep going the same",
  "121:49": "direction again and again go faster yeah",
  "121:52": "so this combination is called",
  "121:55": "one cycle and it's just this amazing",
  "121:58": "like it's a simple thing but it's",
  "122:00": "astonishing like this can help you get",
  "122:03": "what's called super convergence that can",
  "122:05": "let you train ten times faster now this",
  "122:08": "was just last year's paper when some of",
  "122:10": "you may have seen the interview with",
  "122:11": "Leslie Smith that I did last week",
  "122:13": "amazing guy incredibly humble and also I",
  "122:17": "should say somebody who is doing",
  "122:19": "groundbreaking research well into his",
  "122:22": "60s and all of these things are",
  "122:24": "inspiring I'll show you something else",
  "122:26": "interesting when you plot the losses",
  "122:28": "with fast AI it doesn't look like that",
  "122:31": "it looks like that why is that because",
  "122:35": "fast AI calculates the exponentially",
  "122:38": "weighted moving average of the losses",
  "122:39": "for you all right so this this concept",
  "122:41": "of exponentially weighted stuff it's",
  "122:43": "just really handy and I use it all the",
  "122:46": "time",
  "122:46": "and one of the things that is to make it",
  "122:48": "easier to read these charts okay it does",
  "122:50": "mean that these charts from faster I",
  "122:53": "might be kind of an epoch or two sorry a",
  "122:56": "batch or two behind",
  "122:58": "where they should be you know there's",
  "123:01": "that slight downside when you use an",
  "123:02": "exponentially weighted moving average is",
  "123:04": "you've got a little bit of history in",
  "123:06": "there as well",
  "123:06": "but I can make it much easier to see",
  "123:08": "what's going on so we're now at a point",
  "123:19": "coming to the end of this collab in",
  "123:21": "tabular section where we're going to try",
  "123:23": "to understand all of the code in our",
  "123:26": "tabular model so remember the tabular",
  "123:28": "model use this data set called adult",
  "123:31": "which is trying to predict who's going",
  "123:33": "to make more money it's a classification",
  "123:35": "problem and we've got a number of",
  "123:40": "categorical variables and a number of",
  "123:41": "continuous variables so the first thing",
  "123:43": "we realize is we actually don't know how",
  "123:46": "to predict a categorical variable yet",
  "123:47": "because so far we did some hand waving",
  "123:50": "around the fact that our loss function",
  "123:52": "was an n dot cross entropy loss what is",
  "123:55": "that",
  "123:56": "let's find out and of course we're going",
  "124:00": "to find out by looking at Microsoft",
  "124:04": "Excel so cross-entropy loss is just",
  "124:08": "another loss function well you already",
  "124:10": "know 1 loss function which has means",
  "124:12": "grid error",
  "124:12": "y hat minus y squared ok so that's not a",
  "124:17": "good loss function for us because in our",
  "124:19": "case we have like for M list 10 possible",
  "124:22": "digits and we have 10 activations each",
  "124:24": "with a probability of that digit okay so",
  "124:29": "we need something we're predicting the",
  "124:32": "right thing correctly and confidently",
  "124:35": "should have very little loss predicting",
  "124:39": "the wrong thing confidently should have",
  "124:42": "a lot of loss so that's what we want",
  "124:44": "okay",
  "124:45": "so here's an example here is cat versus",
  "124:49": "dog",
  "124:50": "one hot encoded ok and here are my two",
  "124:55": "activations for each one from some model",
  "124:57": "that I built probability cat probability",
  "125:00": "dog this one's not very confident of",
  "125:03": "anything this one's very confident",
  "125:05": "perfect being a cat that's right this",
  "125:07": "one's very confident for being a cat and",
  "125:08": "it's wrong so we want to loss that",
  "125:11": "for this one should be a moderate loss",
  "125:13": "because not predicting anything",
  "125:15": "confidently is not really what we want",
  "125:18": "so here's a point in three",
  "125:20": "this thing's predicting the correct",
  "125:22": "thing very confidently",
  "125:23": "so 0.01 there's things predicting the",
  "125:26": "wrong thing very confidently so one so",
  "125:29": "how do we do that this is the cross",
  "125:33": "entropy loss and it is equal to whether",
  "125:39": "it's a cat multiplied by log of the",
  "125:44": "probability of cat well this is actually",
  "125:46": "an activation so I should say so it's",
  "125:48": "multiplied by the log of the cat",
  "125:50": "activation negative that - is it a dog",
  "125:56": "times the log of the dog activation and",
  "126:02": "that's it",
  "126:03": "okay so in other words it's the sum of",
  "126:05": "all of your one hot encoded variables",
  "126:09": "times all of your activations so",
  "126:14": "interestingly these ones here exactly",
  "126:17": "the same numbers as these ones here but",
  "126:19": "I've written it differently I've written",
  "126:22": "up with an if function because it's",
  "126:24": "exactly this quiz because the zeros",
  "126:26": "don't actually add anything all right so",
  "126:29": "actually it's exactly the same as saying",
  "126:31": "if it's a cat then take the log of",
  "126:36": "cattiness and if it's a dog yes or",
  "126:40": "otherwise take the log of one minus",
  "126:43": "cattiness in other words the log of dog",
  "126:46": "Eunice so the sum of the one hot encoded",
  "126:50": "times the activations is the same as an",
  "126:54": "if function which if you think about it",
  "126:57": "it's actually because this is just a",
  "127:01": "matrix multiply this is we now know from",
  "127:04": "our from our embedding discussion that's",
  "127:06": "the same as an index lookup so you can",
  "127:09": "also - do cross entropy you can also",
  "127:12": "just look up the log of the activation",
  "127:16": "for the correct answer now that's only",
  "127:20": "going to work",
  "127:22": "if these rows add up to one and this is",
  "127:25": "one reason that you can get screwy",
  "127:28": "cross-entropy numbers is this way I said",
  "127:30": "you press the wrong button",
  "127:31": "if they don't add up to 1 you've got a",
  "127:33": "trouble so how do you make sure that",
  "127:35": "they add up to 1 you make sure they add",
  "127:37": "up to 1 by using the correct activation",
  "127:41": "function in your last layer and the",
  "127:43": "correct activation function to use for",
  "127:45": "this is softmax softmax is an activation",
  "127:48": "function where all of the activations",
  "127:52": "add up to 1 all of the activations are",
  "127:55": "greater than 0 and all of the",
  "127:57": "activations are less than 1 so that's",
  "128:00": "what we want right that's what we need",
  "128:04": "how do you do that well let's say we",
  "128:06": "were predicting one of five things cat",
  "128:08": "dog plane fish building and these were",
  "128:12": "the numbers that came out of our neural",
  "128:13": "net for one set of predictions well what",
  "128:18": "if I did",
  "128:18": "e to the power of that so that's one",
  "128:21": "step in the right direction because e to",
  "128:22": "the power of something is always bigger",
  "128:24": "than zero so there's a bunch of numbers",
  "128:27": "that are always bigger than zero here's",
  "128:31": "the sum of those numbers here is a to",
  "128:38": "the number divided by the sum of e to",
  "128:40": "the number now this number is always",
  "128:44": "less than one right because all of the",
  "128:47": "things were positive so you can't",
  "128:49": "possibly have one of the pieces be",
  "128:51": "bigger than 100 percent of its sum okay",
  "128:54": "and all of those things must add up to",
  "128:58": "one right because each one of them was",
  "129:01": "just that percentage of the total so",
  "129:05": "that's it",
  "129:05": "so this thing softmax is equal to e to",
  "129:09": "the activation divided by the sum of e",
  "129:13": "to the activations",
  "129:15": "that's called softmax and so when we're",
  "129:19": "doing single label multi-class",
  "129:22": "classification you generally want",
  "129:25": "softmax as your activation function and",
  "129:27": "you generally want cross-entropy",
  "129:30": "as your loss now because these things go",
  "129:33": "together in such",
  "129:35": "friendly ways play torch we'll do them",
  "129:39": "both for you all right so you might have",
  "129:42": "noticed that in this eminent example I",
  "129:45": "never added a soft max here and that's",
  "129:50": "because if you ask for cross entropy",
  "129:52": "loss it actually does the softmax in",
  "129:55": "inside the loss function so it's not",
  "129:57": "really just cross entropy loss it's",
  "129:59": "actually softmax then cross entropy loss",
  "130:02": "so you've probably noticed this but",
  "130:05": "sometimes your predictions from your",
  "130:09": "models will come out looking more like",
  "130:12": "this pretty big numbers with negatives",
  "130:15": "in rather than this numbers between",
  "130:17": "norton 1 that add up to 1",
  "130:19": "the reason would be that pi torch",
  "130:22": "it's a pi torch model that doesn't have",
  "130:24": "a softmax in because we're using cross",
  "130:27": "entropy loss and so you might have to do",
  "130:29": "the softmax for it fast AI is getting",
  "130:34": "increasingly good at knowing when this",
  "130:38": "is happening generally if you're using a",
  "130:39": "loss function that we recognize when you",
  "130:42": "get the predictions we will try to add",
  "130:45": "the softmax in there for you but if you",
  "130:47": "particularly if you're using a custom",
  "130:49": "loss function that you know might call",
  "130:52": "and end up crossing entropy loss behind",
  "130:54": "the scenes or something like that you",
  "130:55": "might find yourself with this situation",
  "131:00": "we only have 3 minutes less but I'm",
  "131:03": "going to point something out to you",
  "131:04": "which is that next week when we finish",
  "131:08": "off tabular which we'll do in like the",
  "131:11": "first 10 minutes this is forward in",
  "131:15": "tabular and it basically goes through a",
  "131:19": "bunch of embeddings right it's going to",
  "131:23": "call each one of those embeddings E and",
  "131:24": "you can use it like a function of course",
  "131:26": "so it's going to pass a nitch",
  "131:28": "categorical variable to each embedding",
  "131:29": "it's going to concatenate them together",
  "131:31": "into a single matrix it's going to then",
  "131:36": "call a bunch of layers which are",
  "131:41": "basically a bunch of linear layers and",
  "131:43": "then it's going to do our sigmoid trick",
  "131:46": "and then there's only",
  "131:49": "two new things we'll need to learn one",
  "131:51": "is dropout and the other is the end",
  "131:58": "can't better not and these are two",
  "132:01": "additional regularization strategies",
  "132:03": "right there are basically better on does",
  "132:07": "more than just regularization but",
  "132:08": "amongst other things it does",
  "132:09": "regularization and the basic ways you",
  "132:11": "regular as your model weight decay batch",
  "132:16": "norm and dropout okay and then you can",
  "132:21": "also avoid overfitting using something",
  "132:23": "called data augmentation so better",
  "132:25": "Normand dropout we're going to touch on",
  "132:26": "at the start of next week and we're also",
  "132:29": "going to look at data augmentation and",
  "132:31": "then we're also going to look at what",
  "132:33": "convolutions are and we're going to",
  "132:34": "learn some new computer vision",
  "132:37": "architectures and some new computer",
  "132:39": "vision applications but basically we're",
  "132:43": "very nearly there you already know how",
  "132:45": "the entirety of collab py first light",
  "132:49": "club works you know what why it's there",
  "132:53": "and what it does and you're very close",
  "132:55": "to knowing what the entirety of tabular",
  "133:00": "model does and this tabular model is",
  "133:03": "actually the one that if you run it on",
  "133:05": "rossmann you'll get the same answer that",
  "133:08": "I showed you in that paper you'll get",
  "133:09": "that second place result in fact even a",
  "133:12": "little bit better I'll show you next",
  "133:15": "week if I remember how I actually ran",
  "133:17": "some additional experiments where I",
  "133:19": "figured out some minor tweaks that can",
  "133:21": "do even slightly better than that so",
  "133:23": "yeah we'll see you next week thanks very",
  "133:25": "much and enjoy the smoke outside"
}
