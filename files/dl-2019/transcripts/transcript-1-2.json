{
  "00:01": "Welcome to Lesson 2 where we're going to be taking a deeper dive into",
  "00:08": "computer vision applications and",
  "00:10": "Taking some of the amazing stuff that you've all been doing during the week and going even further",
  "00:15": "so let's take a look before we do a",
  "00:19": "reminder that",
  "00:21": "We have these two really important topics on the forums",
  "00:26": "They're pinned at the top of the forum. Category One is FAQ, Resources and Official Course Updates",
  "00:32": "This is where if there's something useful for you to know during the course. We will post there",
  "00:38": "Nobody else can reply to that thread",
  "00:40": "So if you set that thread to watching and notifications",
  "00:43": "You're not going to be bugged by anybody else except stuff that we think you need to know for the course",
  "00:48": "And it's got all the official information about how to get set up on each platform",
  "00:54": "Please note a lot of people post all kinds of other tidbits about",
  "00:59": "How they've set up things on previous solutions or previous courses or other places",
  "01:03": "I don't recommend you use those because these are the ones that we're testing",
  "01:08": "Everyday and that the folks involved in these platforms are testing every day and they definitely work",
  "01:13": "Okay, so so I would strongly suggest you follow",
  "01:16": "those",
  "01:17": "Tips and if you do have a question about using one of these platforms",
  "01:21": "Please use these discussions not some other",
  "01:24": "Topic that you create because this way people that are involved in these",
  "01:28": "Platforms will be able to see it and things won't get messy and then secondly",
  "01:33": "For every lesson, there will be an 'official",
  "01:37": "updates thread' for that lesson. So 'Lesson One Official Updates' and the same thing only Fast.ai people will be posting to that",
  "01:45": "so",
  "01:47": "You can you can watch it safely and we'll have all the things like the videos the notebooks and so forth",
  "01:53": "And they're all wiki threads so you can help us to make them better as well",
  "01:58": "So I mentioned the idea of watching a thread",
  "02:01": "So this is a really good idea is that you can go to a thread like particularly those",
  "02:05": "official update ones and click at the bottom... 'Watching'",
  "02:08": "Ok, and if you do that that's going to enable notifications or any updates to that thread",
  "02:13": "Secondly, if you go in to click on your little user name in the top, right?",
  "02:17": "Preferences and turn this on that'll get gives you an email as well. Okay, so",
  "02:22": "any of you that have missed some of the updates so far go back and have a look through because we're really trained to",
  "02:29": "Make sure that we keep you updated with anything that we think's important",
  "02:33": "One thing which can be more than a little overwhelming is even now after just one week the most popular thread has 1.1 thousand (1100) replies",
  "02:41": "So that's that's an intimidatingly large number",
  "02:45": "I've actually read every single one of them and I know Rachel has and I know ( ) has and I think Francisco has",
  "02:52": "But you shouldn't need to",
  "02:54": "what you should do is click 'Summarize This Topic' and",
  "02:58": "It'll appear like this which is all of the most liked",
  "03:01": "Ones will appear and then they'll be view 31 hidden replies or whatever in between. So that's how you navigate these",
  "03:08": "giant topics that also way it's important you click the like button because that's the thing that's going to cause people to",
  "03:14": "To see it in this recommended",
  "03:17": "view",
  "03:20": "So when you come back to work, hopefully you've realized by now that on the official course website, 'course.Fast.ai' v3",
  "03:28": "you will click 'Returning to work',",
  "03:30": "you will click the name of the platform you're using, and you will then follow the two steps.",
  "03:35": "Step one will be how to make sure that you've got the latest notebooks and",
  "03:40": "Step two will be how to make sure you've got the latest Python library software",
  "03:44": "Okay, they all look pretty much like this, but they're slightly different from platform to platform",
  "03:49": "So, please don't use some different set of commands",
  "03:52": "you read somewhere else - only use the commands that you read about here. And that will make everything very smooth",
  "03:58": "If things aren't working for you, if you get some into some kind of messy situation, which we all do,",
  "04:04": "and",
  "04:05": "uh, just",
  "04:07": "delete your instance and start again. Unless you've got mission-critical stuff there",
  "04:10": "It's the easiest way just to get out of a sticky situation",
  "04:13": "And you know, if you follow the instructions here, you really should find it works fine",
  "04:20": "So this is what I really wanted to talk about",
  "04:23": "Most of all is what people have been doing this week",
  "04:26": "If you've noticed and a lot of you have so there's have been a hundred and sixty-seven people",
  "04:31": "Sharing their work and this is really cool because it's pretty intimidating to put yourself out there and say like I'm new to all this",
  "04:38": "But here's what I've done",
  "04:40": "And so example four things. I thought was really interesting was figuring out who's talking. Is it Ben Affleck or",
  "04:47": "Joe Rogan",
  "04:49": "I thought this is really interesting. This is like actually very practical",
  "04:52": "I wanted to clean up while whatsapp downloaded images to get rid of memes. So I actually built a little neural network",
  "04:59": "I mean, how cool is that to say like? Oh, yeah. I've got something that cleans up my whatsapp. It's a deep learning application",
  "05:05": "I wrote last week why not like it's so easy. Now you can do stuff like this",
  "05:11": "And then there's been some really interesting",
  "05:16": "Projects",
  "05:17": "one was looking at the the",
  "05:21": "Sounds data that was used in this paper. And in this paper, they were trying to figure out what kind of sound",
  "05:28": "things were and they got, as you would expect since they published the paper,",
  "05:31": "they got a state of the art of nearly 80% accuracy",
  "05:35": "Ethan Sutan then tried using the Lesson 1 techniques and got 80.5 % accuracy. So I think this is pretty awesome",
  "05:43": "Best as we know it's a new state of the art",
  "05:45": "for",
  "05:46": "this problem.  Now, maybe somebody since then has published something, we haven't found it yet.",
  "05:50": "Then take all of these with a slight grain of salt, but I've mentioned them on Twitter and lots of people on Twitter",
  "05:55": "follow me, so if everybody knew that there was a much better approach, I'm sure somebody would have said so.",
  "06:01": "This one is pretty cool;  Suvash has a new state of the art accuracy for",
  "06:08": "for devanagari text recognition, I think he's got it even higher than this now",
  "06:13": "And this is actually confirmed by the person on twitter who created the data set.",
  "06:17": "Like, I don't think he had any idea - he just posted 'Here's a nice thing",
  "06:20": "I did' and this guy on Twitter was like, \"oh I made that data set. Congratulations. You've got a new record.\"",
  "06:25": "So that was pretty cool.",
  "06:27": "Um, I",
  "06:28": "really liked this post from Alena Harley.",
  "06:32": "She describes in",
  "06:34": "quite a bit of detail about the issue of metastasizing cancers",
  "06:40": "And the use of point mutations and why that's a challenging important problem",
  "06:45": "And she's got some nice pictures describing like what she wants to do with this and like how she can go about turning this into pictures",
  "06:52": "See, this is the cool trick, right? It's the same with this",
  "06:56": "this 'sound' one, turning sounds into pictures and then using the Lesson 1 approach and here it's",
  "07:02": "turning point mutations into pictures and then using the Lesson 1 approach and",
  "07:07": "What did she find?",
  "07:09": "It seems that she's got a new state of the art result by more than 30% beating the previous best",
  "07:16": "somebody on twitter who's a VP at a genomics analysis company looked at this as well and you know",
  "07:24": "Thought it looked to be a state of the art in this particular point mutation one as well. So that's pretty exciting",
  "07:30": "so you can see you know, when we talked about last week this idea that",
  "07:34": "This simple process is something which can take you a long way. It really can I will mention that",
  "07:42": "You know something like this one in particular is is using a lot of domain expertise like it's figuring out what picture",
  "07:49": "To create I wouldn't know",
  "07:51": "How to do that because I don't even really know what a point mutation is let alone how to create",
  "07:56": "You know something that visually is meaningful that a CNN could recognize",
  "08:00": "but the actual big learning side",
  "08:03": "Is is actually pretty straightforward.",
  "08:08": "Another very cool result from Simon Willison and Natalie Downe.  They",
  "08:15": "Created a 'cougar or not' web application",
  "08:19": "Over the weekend and won the 'Science Hack Day' award in San Francisco",
  "08:24": "And so I think that's pretty pretty fantastic",
  "08:28": "So lots of examples of people doing really interesting work",
  "08:33": "Hopefully this will be",
  "08:34": "Inspiring to you to think well, this is this is cool that I can do this with what I've learned",
  "08:39": "it can also be intimidating to think like wow, these people are doing amazing things, but",
  "08:44": "It's important to realize that out of thousands of people during this course",
  "08:48": "You know, I'm just picking out the kind of a few of the really amazing ones and in fact",
  "08:54": "Simon is one of these very annoying people like Christine Payne who talked about last week who seems to be good at everything he does",
  "08:59": "He created Django when it's the world's most popular web frameworks. He founded a very successful startup and bla bla bla bla bla",
  "09:05": "so",
  "09:06": "You know one of these really annoying people who?",
  "09:08": "Tends to keep being good at things now turns out he's good at deep learning as well",
  "09:11": "So, you know, that's fine. You know Simon can go and win a hackathon on his first week of playing with deep learning",
  "09:18": "Maybe it'll take you two weeks to win your first hackathon. That's okay",
  "09:22": "um, and I think like it's important to mention this because there was this really inspiring blog post this week from James Dellinger who talked",
  "09:30": "About how he created a bird classifier using the techniques from lesson one",
  "09:34": "But what I really found interesting was at the end",
  "09:36": "He said he he nearly didn't start on deep learning at all because he went through the scikit-learn website",
  "09:43": "Which is one of the most important libraries of python and he saw this",
  "09:46": "And he described in this blog post or how he was just like 'that's not something I can do,",
  "09:51": "It's not something I understand' and then this kind of realization of like, oh I can do useful things",
  "09:56": "without",
  "09:57": "reading the Greek so I thought that was a really",
  "09:60": "Cool message and I really want to highlight actually Daniel Armstrong on the forum. I think really",
  "10:07": "Shows is a great role model here",
  "10:09": "which was here saying I want to contribute to the library and I looked at the docs and I just started overwhelming and",
  "10:16": "the next message one day later was... 'I",
  "10:20": "don't know what any of this is, I didn't know how much there was to it, it caught me off guard,",
  "10:24": "My brain shut down",
  "10:27": "But I love the way it forces me to learn so much and then one day later, I just submitted my first pull request",
  "10:34": "So I think that's also right. It's just kind of like it's okay to feel intimidated",
  "10:39": "There's a lot, right? But just pick one piece and dig into it",
  "10:43": "you know, try and try and push a piece of code or a documentation update or create a classifier or whatever",
  "10:50": "So here's lots of cool classifiers people have built. It's been really really inspiring",
  "10:56": "Trinidad and Tobago Islander versus Masquerader classifier.  A Zucchini versus Cucumber classifier,",
  "11:04": "This one was really nice. This was taking the dog breeds",
  "11:08": "dog and cat breeds thing from last week and actually doing some exploratory work to see what the main features were and",
  "11:15": "discovered that they could have a 'hairiness",
  "11:18": "classifier'",
  "11:20": "and so here we have the most hairied dogs and the most bald cats.",
  "11:25": "So there are you know",
  "11:26": "Interesting things you can do with interpretation",
  "11:28": "Somebody else in the forum took that and did the same thing for anime to find that they had accidentally discovered an anime haircolor classifier",
  "11:36": "We can now detect the new versus the old",
  "11:40": "Panamanian buses correctly. Apparently these are the new ones I much prefer the old ones, but maybe that's just me",
  "11:47": "This was a really interesting Henri Pallacci discovered that he can recognize, with 85% accuracy, which of",
  "11:54": "110 cities..., sorry, which of 110 countries,",
  "11:57": "a satellite image is of.  Which you know, has definitely got to be",
  "12:03": "Beyond human performance of just about anybody like I can't imagine anybody",
  "12:07": "Who can do that in practice. So that was fascinating.",
  "12:13": "Batik cloth classification with a hundred percent accuracy",
  "12:19": "Those rewarded this interesting one we actually went a little bit further using some techniques",
  "12:23": "We'll be discussing in the next couple of courses to build something that can recognize",
  "12:27": "'complete or incomplete foundation buildings' and actually plot them on aerial satellite view",
  "12:35": "So lots and lots of fascinating",
  "12:38": "Projects.  So don't worry, it's only been one week. It doesn't mean everybody has to have had a project out yet",
  "12:44": "A lot of the folks who already have a project out have done a previous course, so they've got a bit of a head start",
  "12:50": "But we'll see today how you can definitely create your own classifier",
  "12:55": "this week.",
  "12:56": "So, from today after we dig a bit deeper into",
  "13:00": "really how to make these computer vision classifiers in particular, work well",
  "13:05": "We're then going to look at the same thing for text. We're then going to look at the same thing for tabular data",
  "13:12": "So they're kind of like more like spreadsheets and databases",
  "13:15": "Then we're going to look at 'collaborative filtering'",
  "13:19": "So we're going to recommendation systems that's going to take us into a topic called embeddings, which is basically a key",
  "13:26": "underlying platform behind these applications",
  "13:30": "That will take us back into more computer vision and then back into more NLP",
  "13:35": "so the idea here is that it turns out that it's it's much better for learning if you",
  "13:41": "Kind of see things multiple times. So rather than being like, okay that's computer vision",
  "13:45": "You won't see it again for the rest of the course",
  "13:47": "We're actually going to come back to the two key applications",
  "13:50": "NLP and computer vision a few weeks apart and that's going to force your brain to realize like 'oh I have to remember this'",
  "13:56": "It's not just something I can throw away",
  "14:01": "So",
  "14:03": "We are",
  "14:06": "You know for people who have more of a",
  "14:10": "hard sciences kind of background in particular",
  "14:14": "a lot of folks find this",
  "14:18": "Hey, 'here's some code - type it in start running it approach' rather than here's lots of theory approach",
  "14:25": "Confusing and surprising and odd at first and so for those of those of you I just wanted to remind you",
  "14:31": "you know, this basic tip, which is",
  "14:34": "Keep going.  You're not expected to remember everything. Yet.  You're not expected to understand everything. Yet.",
  "14:40": "You're not expected to know why everything works.  Yet.",
  "14:44": "You just want to be in a situation where you can: enter",
  "14:47": "the code and you can run it and you can get something happening and then you can start to",
  "14:53": "experiment and you kind of get a feel for what's going on and then,",
  "14:57": "Push on. Right?  Most of the people who have done the course and have gone on to be really successful watch the videos at least",
  "15:03": "three times.  So they kind of go through the whole lot and then go through it slowly",
  "15:08": "the second time, then they go through it really slowly the third time and I consistently hear them say 'I get a lot more out",
  "15:13": "of it each time I go through' so don't pause at lesson one and stop until you can continue",
  "15:21": "So, um this approach is based on a lot of a",
  "15:27": "research, academic research, into learning theory and one guy in particular",
  "15:31": "David Perkins from Harvard, has this really great analogy. He's a researcher into learning theory",
  "15:37": "He describes this approach of the whole game, which is basically if you're teaching a kid to play soccer you don't you know",
  "15:44": "first of all teach them about you know",
  "15:46": "how the friction between a ball and grass works and then teach them how to sew a",
  "15:51": "soccer ball with their bare hands and then teach them the mathematics of parabolas when you kick something in the air...",
  "15:57": "No.  They say \"Here's a ball. Let's watch some people playing soccer.\"",
  "16:01": "now we'll play soccer and",
  "16:03": "Then you you know gradually over the following years learn more and more so that you can get better and better at it",
  "16:09": "So this is kind of what we're trying to get you to do is to play soccer which in our case is to type code",
  "16:15": "And look at the inputs and look at the outputs",
  "16:19": "Okay",
  "16:21": "So let's dig into",
  "16:24": "Our first notebook, which is called \"lesson2-download\" and what we're going to do is we're going to see how to create",
  "16:34": "your own",
  "16:36": "Classifier with your own images, so it's going to be a lot like last week's pet detector",
  "16:42": "But it'll detect whatever you like. So it'll be like those some of those examples we just saw. How would you create your own",
  "16:49": "\"Panama Bus Detector\", from scratch?",
  "16:54": "So this is",
  "16:55": "inspired the approaches inspired by Adrian Rosebrock, who has a terrific website called \"pyimagesearch\" and",
  "17:03": "He has this nice explanation of how to create a data set using Google images",
  "17:09": "So that was definitely an inspiration for some of the techniques we use here. So, thank you to Adrian",
  "17:14": "And you should definitely check out his site. It's a really it's full of lots of good resources",
  "17:19": "So",
  "17:22": "So here we are so we are going to try to create a \"teddy bear\" detector",
  "17:31": "Thanks",
  "17:32": "We're going to try and make a teddy bear detector, and we're going to try and separate teddy bears",
  "17:37": "from black bears from grizzly bears.  Now,  this is very important",
  "17:42": "I have a three year old daughter and",
  "17:44": "She needs to know what she's dealing with.  In our house, you would be surprised at the number of monsters,",
  "17:51": "lions, and other terrifying",
  "17:53": "threats that are around, particularly around Halloween,",
  "17:55": "And so we always need to be on the lookout to make sure that the thing we're about to cuddle",
  "17:60": "Is in fact a genuine teddy bear, okay",
  "18:03": "So let's deal with that with that situation as best as we can",
  "18:06": "So our starting point is to find some pictures of teddy bears so we can learn",
  "18:12": "What they look like so I go to images.google.com",
  "18:16": "and",
  "18:17": "I type in \"teddy bear\"",
  "18:20": "and",
  "18:22": "I just scroll through until I kind of find a",
  "18:26": "goodly bunch of them.",
  "18:28": "and it's like, okay that",
  "18:30": "Looks like plenty of teddy bears to me. So then I'll go back to",
  "18:35": "Here.  So you can see it says \"search and scroll go to Google Images\" and search",
  "18:39": "and the next thing we need to do is to get a list of all of the URLs, there, and so to do that you",
  "18:45": "Back in your google images you hit 'ctrl- shift-J' or 'command-option-J'",
  "18:51": "And you paste this [highlighted text] into the window that appears.",
  "18:59": "So",
  "19:01": "I've got Windows so I go 'ctrl-shift-J'",
  "19:04": "paste in that code. So this is the JavaScript console, for those of you you haven't done any JavaScript before, I hit enter and",
  "19:12": "It downloads",
  "19:13": "my file for me.  So I would call this",
  "19:17": "\"teddies.txt\" and press Save. Okay, so I now have a",
  "19:24": "file of Teddies, or URLs of Teddies, so then I would repeat that process for",
  "19:31": "Black bears and for brown bears since that's a classifier I would want, and i would put each one in a file with an appropriate name.",
  "19:38": "So that's step one. So step two is we now need to",
  "19:42": "Download those URLs to our server. Because remember, when we're using Jupyter notebooks, it's not running on our computer",
  "19:50": "It's running on Sagemaker or Kressel or",
  "19:55": "Google Cloud or whatever",
  "19:57": "So to do that we start running some Jupyter cells. So let's grab the Fast.ai library and",
  "20:04": "Let's start with black bears. I've already got my black bears URL, so I click on this cell for black bears and I'll run it",
  "20:11": "See here how I've got three different cells, doing the same thing but different information?",
  "20:17": "This is this is one way I like to work with Jupyter notebook - it's something that a lot of kind of",
  "20:22": "People with a more strict scientific background are horrified by.  This is not reproducible research.",
  "20:27": "I actually click here and I run this cell to create a folder called \"black\" and a file called",
  "20:32": "\"urls_black\" for my black bears, I skip the next two cells and then I run this cell",
  "20:38": "to create",
  "20:40": "that folder, okay?  And then I go down to the next section and I run",
  "20:46": "the next cell which is",
  "20:49": "'download_images()' for",
  "20:52": "'black_bears', right?",
  "20:53": "So that's just going to download my black bears to that folder and then I'll go back and",
  "20:58": "I'll click on 'teddys', and I'll run that cell and",
  "21:01": "then scroll back down and",
  "21:03": "I'll run this cell and so that way I'm just going backwards and forwards to download each of the classes that I want",
  "21:09": "Very manual, but for me, I'm very iterative I'm very experimental, that works",
  "21:15": "well for me.  If you're better at kind of planning ahead than I am, you can, you know,",
  "21:19": "Write a proper loop or whatever and and do it that way. So but when you see",
  "21:24": "My notebooks and see things where there's kind of like configuration cells doing the same thing in different places",
  "21:31": "This is a strong sign that I didn't run this in order.  Right?",
  "21:35": "I clicked one place went to another around that went back went back went back and for me, I",
  "21:40": "Just I'm an experimentalist. I really like to",
  "21:44": "experiment in my notebook - I treat it like a lab journal. I try things out and I see what happens...",
  "21:49": "And so this is how my notebooks end up looking",
  "21:52": "it's a really controversial topic like for a lot of people they feel this is, like,",
  "21:57": "\"Wrong.\"  That you should only ever run things top to bottom.  Everything you do should be reproducible",
  "22:02": "For me, I don't think that's the best way of using human creativity. I think human creativity is best inspired",
  "22:09": "by trying things out, seeing what happens, and fiddling around.  So you can see how you go. See what works for you",
  "22:16": "So that will download the images to your server. It's going to use",
  "22:22": "multiple processes to do so and",
  "22:25": "One problem there is if is if something goes wrong",
  "22:29": "It's a bit hard to see what went wrong so you can see in the next section",
  "22:32": "There's a commented out section that says",
  "22:34": "\"max_workers = 0\" and that'll do it without spitting up a bunch of processes and will tell you the errors better.",
  "22:40": "So if things aren't downloading try using the second version.",
  "22:45": "Okay, so it takes so I you know grabbed a small number of each",
  "22:50": "and then the next thing that I found I needed to do was to remove the",
  "22:55": "Images that aren't actually images at all. \n And this happens all the time. There's always a few images in every batch that",
  "23:02": "are corrupted for whatever reason, you know Google Image tried to",
  "23:06": "Told us that this URL had an image, but actually it doesn't anymore",
  "23:10": "So I've got we've got this thing in the library called \"verify_images()\"",
  "23:14": "Which will check all of the images in a path",
  "23:16": "And will tell you if there's a problem if you say 'delete=True', it will actually delete it for you",
  "23:23": "Okay, so that's a really nice easy way to end up with a clean data set.",
  "23:28": "So at this point I now have",
  "23:30": "a bears folder containing a /grizzly folder and a /teddys folder and a /black folder in",
  "23:37": "Other words I have the basic structure we need to create an image data bunch to start doing some deep learning",
  "23:44": "So let's go ahead and do that. Now",
  "23:48": "Very often when you get when you download a data set from, like,",
  "23:53": "Kaggle or from some academic data set, there will often be a folder called",
  "23:58": "/train and a folder called /valid and a folder called /test right containing the different",
  "24:04": "data sets.  In this case, we don't have a separate validation set because we just grab these images from Google search, right?",
  "24:13": "But you still need a validation set",
  "24:16": "Otherwise, you don't know how well your model is going and we'll talk about more about this in a moment",
  "24:20": "So whenever you create a data bunch",
  "24:23": "If you don't have a separate training and validation set then you can just say okay",
  "24:28": "the training set is in the 'current' folder because by default it looks in a folder called /train and I want you to set aside",
  "24:35": "20 percent of the data, please. So this is going to create a validation set for you automatically and randomly.",
  "24:42": "You'll see that whenever I",
  "24:44": "create a validation set randomly, I always set my random seed to something fixed beforehand.",
  "24:50": "This means that every time I run this code, I'll get the same",
  "24:54": "validation set so in general",
  "24:58": "I'm not a fan of",
  "25:01": "making my",
  "25:03": "machine learning experiments reproducible;",
  "25:05": "i.e. ensuring I get exactly the same result every time.",
  "25:08": "The randomness is to me really important a really important part of planning out is your solution stable, you know",
  "25:14": "Is it going to work like each time you run it but what is important is that you always have the same?",
  "25:19": "Validation set. But otherwise when you're trying to decide has this hyper parameter change improved my model",
  "25:26": "But you've got a different set of data you're testing it on then you don't know maybe that set of data",
  "25:31": "It just happens to be a bit easier. Okay?",
  "25:33": "So that's why I always said the random seed here.",
  "25:38": "So we've now gone (let's run that cell), so we've now got a data bunch",
  "25:42": "And so you can look inside at the data.classes and you'll see",
  "25:47": "These are the folders that we created",
  "25:49": "So it knows that the classes or you know, so by classes we main all the possible labels black bear grizzly bear or teddy bear",
  "25:57": "We can run show batch and we can take a little look",
  "26:01": "And it tells us straight away that some of these are going to be a little bit tricky",
  "26:06": "So this is not a photo, for instance,",
  "26:11": "some of them kind of cropped funny",
  "26:15": "Some of them might be tricky like if you ended up with a black bear standing on top of a grizzly bear that might be",
  "26:19": "tough",
  "26:20": "Anyway, so you can kind of double check here, 'data.classes', here they are",
  "26:23": "They are (remember '.c' is the attribute which the classifiers tells us how many possible labels there are?).",
  "26:30": "We'll learn about some other more specific meanings of '.c' later.",
  "26:34": "We can see how many things are in our training set,",
  "26:37": "we can see how many things are in our validation set.",
  "26:43": "So we've got",
  "26:45": "473 training set, 141 validation set",
  "26:50": "So at that point we can go ahead you'll see all these commands are identical to the pet classifier from last week",
  "26:55": "we can create our",
  "26:57": "CNN (our convolutional neural network) using that data",
  "27:00": "I tend to default using a 'resnet34'",
  "27:04": "and let's print out the error rate each time and run 'fit_one_cycle()' four times and see how we go and",
  "27:12": "We have a 2% error rate",
  "27:14": "So that's pretty good. I",
  "27:16": "Personally, I mean sometimes it's easy for me to recognize a black bear from a grizzly bear",
  "27:20": "But sometimes it's a bit tricky. This one seems to be doing pretty well",
  "27:28": "Okay, so",
  "27:29": "After I kind of make some progress with my model and things looking good",
  "27:34": "I always like to save where I'm up to, to save me the 54 seconds of going back and doing it again and,",
  "27:39": "As very usual we unfreeze() the rest of our model. We're going to be learning more about what that means during the course",
  "27:46": "And then we run the 'learning rate finder' and plot",
  "27:50": "it (it tells you exactly what to type) and we take a look.  Now, we're going to be learning about learning rate, today, actually.",
  "27:58": "But for now, here's what you need to know: on the learning rate finder, what you're looking for is the strongest",
  "28:05": "downward slope",
  "28:07": "That's kind of sticking around for quite awhile, right?",
  "28:10": "So this one here looks more like a bump, but this looks like an actual downward slope, to me",
  "28:15": "So it's kind of like it's something you're going to have to practice with and get a feel for, like what",
  "28:21": "bit works so like if you're not sure is it this bit or this bit, try both learning rates and see which one",
  "28:28": "works better. Okay, but I'm",
  "28:30": "I've been doing this for a while and I'm pretty sure this looks like where it's really learning properly. So I would pick something",
  "28:37": "Okay here it's not so steep. So I would probably pick something back here",
  "28:41": "for my learning rate.",
  "28:44": "So you can see I picked",
  "28:46": "3x10^-5 so, you know somewhere around here. That sounds pretty good. So that's for my bottom learning rate",
  "28:53": "So my top learning rate I normally pick you know 1x10^-4 or 3x10^-4",
  "28:58": "It's kind of like I don't really think about it too much",
  "29:01": "That's a rule of thumb it always works pretty well",
  "29:04": "One of the things you'll realize is that",
  "29:08": "Most of these parameters don't actually matter that much, in detail.",
  "29:12": "If you just copy the numbers that I use each time",
  "29:16": "It'll the vast majority the time it'll just work fine and we'll see places where it doesn't today.",
  "29:22": "Okay, so we've got a 1.4% error rate after doing another couple of epochs. So that's looking great",
  "29:28": "So we've downloaded some images from Google Image Search",
  "29:32": "And created a classifier. We've got a 1.4% error rate. Let's save it.",
  "29:38": "And then as per usual",
  "29:40": "We can use the ClassificationInterpretation class to have a look at what's going on. And in this case, we made one mistake.",
  "29:48": "There was one black bear classified as grizzly bear.",
  "29:53": "So that's",
  "29:55": "That's a really good step. We come a long way",
  "29:58": "but",
  "29:60": "possibly you could do even better if your data set was less noisy like maybe",
  "30:06": "Google Image Search",
  "30:08": "Didn't give you exactly the right images all the time",
  "30:12": "So how do we fix that? And so we want to clean it up. And so",
  "30:16": "Combining a human expert with a computer learner is a really good idea almost",
  "30:22": "not no-nobody, but very very few people publish on this very very few people teach this but to me",
  "30:28": "It's like the most useful skill particularly for you, you know",
  "30:31": "Most of the people watching this are domain experts not computer science experts. And so this is where you can use your knowledge of",
  "30:40": "You know 'point mutations' in genomics or 'Panamanian buses' or whatever. So, let's see how that would work.",
  "30:47": "What I'm going to do is,",
  "30:49": "do you remember the .plot(top_losses) from last time where we saw the images which it was, like,",
  "30:54": "either the most wrong about or the least confident about we're going to look at those and decide which of those are",
  "31:01": "noisy like if you think about it, it's very unlikely that",
  "31:06": "If there is some mislabeled data that it's going to be predicted correctly and with high confidence.",
  "31:13": "That that's really unlikely to happen.",
  "31:15": "So we're going to focus on the on the ones which the model is saying either",
  "31:20": "'it's not confident of' or it was 'confident of but it was wrong about'.  They are the things which might be mislabeled.",
  "31:28": "So",
  "31:30": "a big shout-out to the San Francisco",
  "31:34": "Fast.ai Study Group who created this new widget this week called the FileDeleter.",
  "31:41": "So that's Zach and Jason and Francisco",
  "31:46": "Built this thing where we basically can take the top_losses() from that interpretation object",
  "31:52": "we just created, right, and then what we're going to do is we're going to say okay that returns",
  "31:57": "top losses... there's not just .plot(top_losses), but there's also just .top_losses() and .top_losses() returns two things",
  "32:04": "the 'losses' of the things that were the worst",
  "32:07": "and the",
  "32:08": "Indexes into the data set the things that were the worst and if you don't pass anything at all",
  "32:13": "It's going to actually return the entire data set, but sorted, so the first things will be the highest losses.",
  "32:21": "As we learned during the course or will keep seeing during the course, every data set",
  "32:26": "In Fast.ai has an X and a Y.  And the X",
  "32:30": "contains the things that are used to, in this case, get the images.",
  "32:34": "So this is the image file names and the Y's will be the labels",
  "32:38": "So if we grab the indexes and pass them into the data set",
  "32:42": "X this is going to give us the file names of the data set",
  "32:48": "Ordered by which ones had the highest loss",
  "32:51": "so which ones it was either confident and wrong about or not confident about.  And",
  "32:58": "so we can pass that to this new widget that they've created called the FileDeleter widget",
  "33:08": "So just to clarify, this top_loss_paths contains",
  "33:12": "all of the file names in our data set and when I say 'in our data set' and this particular one is in our",
  "33:18": "validation data set, so what this is going to do is it's going to clean up",
  "33:22": "mislabeled",
  "33:25": "Images or images that shouldn't be there",
  "33:29": "And we're going to remove them from the validation set so that our metrics will be more correct.",
  "33:34": "You then need to rerun these two steps replacing .valid_ds with .train_ds,",
  "33:38": "to clean up your training set to get the noise out of that as well. So it's a good practice to do both",
  "33:47": "We'll talk about test sets later as well. If you also have a test set you would then repeat the same thing.",
  "33:52": "So we run FileDeleter() passing in that sorted list of paths. And so what pops up is",
  "34:00": "Basically the same thing as plot_top_losses. So in other words, these are the ones which is either wrong about",
  "34:08": "Or the least confident about and so, not surprisingly,",
  "34:11": "this one here does not appear to be a teddy bear,",
  "34:15": "or a black bear or a brown bear.  Right? So this shouldn't be in our data set",
  "34:20": "So what I do is I whack on the Delete button",
  "34:24": "Okay, and all the rest do look, indeed, like bears and then so I can click confirm and it'll bring up another five",
  "34:32": "What's that?  That's not a bear is it?",
  "34:35": "So anybody know what that is?",
  "34:37": "I'm going to say that's not a bear. Delete. Confirm. Oh",
  "34:44": "Not there. Well, that's a teddy bear I'll leave that.  That's not really, I'll get rid of that one. \n Confirm.",
  "34:52": "Okay. So what I tend to do when I do this is I'll keep going Confirm until I get to a couple of screens all",
  "34:59": "The things that all look, okay, and that suggests to me that I've kind of got past the worst bits of the data",
  "35:05": "Okay, and that's it",
  "35:06": "And so now you can go back once you do it for the training set as well and retrain your model. So",
  "35:13": "I'll just note here that what our San Francisco study group did here was that they actually built a little app",
  "35:21": "inside",
  "35:22": "Jupyter notebook, which you might not have realized is possible, but not only is it possible, It's actually",
  "35:29": "Surprisingly straightforward and just like everything else you can hit double question mark to find out their secrets",
  "35:36": "So here is the source code. Okay, and",
  "35:39": "Really if you've done any",
  "35:41": "GUI programming before it'll look incredibly",
  "35:45": "normal, you know there's there's basically callbacks for what happens when you click on a button where you just do standard Python things and to",
  "35:53": "actually render it you just",
  "35:56": "use widgets and you can lay it out using standard boxes and whatever so",
  "36:01": "It's it this idea of creating",
  "36:05": "Applications inside notebooks is like it's really underused but it's super neat because it lets you create",
  "36:12": "tools for your fellow",
  "36:14": "practitioners to your fellow experimenters, right and you could definitely",
  "36:18": "envisage",
  "36:19": "Taking this a lot further. In fact by the time you're watching this on the MOOC",
  "36:23": "You will probably find that there's a whole lot more buttons here because we've already got a long list of to do that",
  "36:28": "We're going to add",
  "36:30": "to this particular thing",
  "36:34": "So so, that's it so I think like",
  "36:37": "I'd love for you to have to think about now that you know, it's possible",
  "36:42": "to write applications in your notebook, what are you going to write?  And if you google for 'ipywidgets'...",
  "36:50": "you can learn about",
  "36:52": "the little GUI framework",
  "36:55": "To find out what kind of widgets you can creation what they look like and how they work",
  "36:60": "and so forth and you'll find it's you know, it's actually a pretty",
  "37:04": "You know complete",
  "37:06": "GUI programming environment you can play with and this will all work nicely with your models and so forth",
  "37:13": "It's not a great way to",
  "37:16": "Productionize an application because it is sitting inside a notebook",
  "37:19": "This is really for things which are going to help other practitioners other",
  "37:25": "experimentalists and so forth for",
  "37:28": "productionizing things",
  "37:30": "You need to actually build a production web app, which we'll look at next.",
  "37:36": "Okay, so",
  "37:38": "After you have cleaned up your noisy images",
  "37:43": "You can then retrain your model and hopefully you'll find it's a little bit more accurate",
  "37:48": "one thing you might be interested to discover when you do this is",
  "37:51": "It actually doesn't matter, most of the time, very much, now,",
  "37:55": "on the whole, these models are pretty good at dealing with",
  "38:00": "moderate amounts of noisy data.  The problem would occur is if your data was not randomly noisy,",
  "38:08": "but biased noisy,",
  "38:10": "So I guess the main thing I'm saying is if you go through this process",
  "38:13": "of cleaning up your data and then rerun your model and It's like .001 % better,",
  "38:18": "that's normal. Okay, it's fine.  But it's still a good idea just to make sure that you don't have too much",
  "38:25": "noise in your data in case it is biased. So at this point",
  "38:29": "we're ready to put our model in production. And",
  "38:32": "this is where I hear a lot of people ask me about you know, which",
  "38:39": "mega Google Facebook",
  "38:42": "highly",
  "38:43": "distributed serving system they should use and how do they use a thousand GPUs at the same time and whatever else,",
  "38:51": "For the vast vast vast majority of things that you all do",
  "38:55": "You will want to actually run in production on a CPU",
  "38:59": "Not a GPU. Why is that?  Because the GPU is good at doing lots of things at the same time",
  "39:05": "But unless you have a very busy website. It's pretty unlikely that you're going to have 64",
  "39:11": "images to classify at the same time to put into a batch into a GPU",
  "39:15": "And if you did you've got to deal with all that",
  "39:17": "queuing, and running it all together, all of your users have to wait until that batch has got filled up and run.",
  "39:22": "It's a whole lot of hassle, right? And then if you want to scale that there's another whole lot of hassle",
  "39:28": "It's much easier if you just wrap one thing throw it at a CPU to get it done and it comes back again",
  "39:36": "so yes, it's going to take",
  "39:38": "You know, maybe 10 or 20 times longer",
  "39:41": "right, so maybe it'll take",
  "39:43": "0.2 seconds rather than",
  "39:45": "0.01 seconds, that's about the kind of times we talk about",
  "39:49": "But it's so easy to scale. All right, you can chuck it on any standard serving infrastructure. It's going to be cheap as hell",
  "39:56": "You can horizontally scale it really easily. Okay?",
  "39:59": "So most people I know who are running apps that aren't kind of at Google scale based on deep learning are using CPUs",
  "40:06": "And the term we use is inference, right?",
  "40:08": "So when you're running when you're not training a model, but you've got a trained model and you're getting to predict things,",
  "40:14": "we call that inference. So that's why we say here 'You probably want to use",
  "40:19": "CPUs for inference'.",
  "40:22": "So at inference time,",
  "40:24": "you've got your pre-trained model, you saved those weights, and how are you going to use them to create something like Simon relations?",
  "40:31": "a 'Cougar detector'? Well first thing you're going to need to know is what were the classes",
  "40:37": "that you trained with?  Right?  You need to not know not just what are they, but what were the order?",
  "40:42": "Okay, so you will actually need to like serialize that or just type them in or in some way",
  "40:49": "make sure you've got exactly the same",
  "40:51": "classes that you trained with.",
  "40:54": "If you don't have a GPU on your server,",
  "40:58": "it will use the CPU automatically.",
  "41:01": "If you want to test if you have a GPU machine and you want to test using a CPU",
  "41:05": "You can just uncomment this line and that tells First.ai",
  "41:09": "that you want to use",
  "41:11": "CPU, by passing it back to pytorch. So here's an example,",
  "41:16": "we don't have a cougar detector,",
  "41:18": "we have a 'teddy bear detector' and my daughter Claire is about to decide whether to cuddle his friend",
  "41:24": "Okay",
  "41:25": "so",
  "41:26": "what she does is she takes",
  "41:28": "Daddy's deep learning model and she gets a picture of this, and here's a picture that she's uploaded to the web app,",
  "41:34": "okay, and here's a picture of the",
  "41:37": "potentially cuddlesome object,",
  "41:39": "And so we're going to store that in a variable called 'img'",
  "41:41": "So open_image() is how you open an image in fastai, oddly enough,",
  "41:47": "Here is that list of classes that we saved earlier",
  "41:51": "And so as per usual we created a DataBunch,",
  "41:54": "but this time we're not going to create a DataBunch from a folder full of images,",
  "41:60": "we're going to create a special kind of DataBunch, which is one that's going to grab one single image at a time",
  "42:06": "So we're not actually passing it any data.",
  "42:09": "The only reason we pass it a path is so that it knows where to load our model from, right? That's just the path",
  "42:15": "that's the folder that the model is going to be in.",
  "42:18": "But what we do need to do is that we need to pass it the same information that we trained with. So the same",
  "42:23": "transforms, the same size, the same normalization. This is all stuff",
  "42:27": "we'll learn more about, but just make sure it's the same stuff that you used before.  And so now you've got a DataBunch",
  "42:32": "that actually doesn't have any data in it at all.",
  "42:35": "It's just something that knows how to transform",
  "42:38": "a new image in the same way that you trained with so that you can now do inference.",
  "42:43": "So you can now create a CNN with this kind of fake DataBunch. And again, you would use exactly the same",
  "42:50": "model that you trained with, you can now load in those saved weights, okay?  And so, this is the stuff that you do once,",
  "42:58": "just once when your web app's starting up, okay, and it takes you know, 0.1 of a second to run this code.  And",
  "43:04": "then, you just go learn.predict()",
  "43:07": "learn.predict(img), and it's lucky we did that because it is not a teddy bear. This is actually a black bear so, thankfully,",
  "43:15": "due to this",
  "43:18": "excellent deep learning model, my daughter will avoid having a very embarrassing black bear cuddle incident.",
  "43:26": "So,",
  "43:27": "what does this look like in production?",
  "43:29": "Well, I took Simon Willison's code and shamelessly stole it, made it probably a little bit worse,",
  "43:37": "And but basically it's going to look something like this. So Simon used a really cool web app toolkit called Starlette, if you've ever used",
  "43:46": "Flask, this will look extremely similar, but it's kind of a more",
  "43:50": "modern approach.  By 'modern' what I really mean is that",
  "43:53": "You can use 'await()'. It's basically means that you can wait for something that takes a while",
  "44:01": "Such as grabbing some data,",
  "44:04": "without using up a process.  So for things like 'I want to get a prediction' or 'I want to load up some data' or whatever",
  "44:10": "It's really great to be able to use this",
  "44:13": "modern Python 3 asynchronous stuff.",
  "44:16": "So Starlette would come highly recommended for creating your web app. And so yeah, you just create a",
  "44:23": "route as per usual in a web app, and in that",
  "44:29": "you say this is 'async' to ensure that it doesn't steal the process while it's waiting for things.",
  "44:35": "You open your image, you call .predict() and you return that response and then you can use, you know,",
  "44:43": "whatever, JavaScript client or whatever to to show it.  And that's it. That's basically the the main",
  "44:51": "contents of your web app.",
  "44:53": "So",
  "44:55": "Give it a go, right?  You know this week even if you've never created a web application before,",
  "45:02": "there's a lot of you know, nice little tutorials online, and kind of starter code",
  "45:08": "you know, if in doubt, why don't you try Starlette?  There's a",
  "45:12": "free hosting that you can use there's one called 'PythonAnywhere', for example.",
  "45:19": "The one that Simon's used (we'll mention that on the forum) it's something you can basically package it up as a docker",
  "45:24": "thing and shoot it off and it'll serve it up for you.",
  "45:27": "So it doesn't even need to cost you any money. And so all these",
  "45:32": "classifiers that you're creating, you can turn them into web applications. So I'll be really interested to see",
  "45:39": "what you're able to make of that.  That will be really fun.",
  "45:44": "Okay, so let's take a break we'll come back at",
  "45:49": "7:35.",
  "45:51": "See you then.",
  "45:56": "Okay.  So let's move on.",
  "46:07": "So I mentioned that",
  "46:10": "most of the time, the kind of rules of thumb I've shown you",
  "46:15": "will probably work and if you look at the 'Share Your Work' thread",
  "46:19": "you'll find most of the time people are posting things saying \"I downloaded these images, I tried this thing..\"",
  "46:26": "\"...They worked much better than expected.\"  Well, that's cool.",
  "46:29": "And then like 1 out of 20 says, like, \"Ah,...\"",
  "46:33": "\"I had a problem.\"",
  "46:35": "So let's have a talk about what happens when you have a problem.",
  "46:38": "And this is where we're gonna start getting into a little bit of theory",
  "46:41": "because in order to understand why we have these problems and how we fix them,",
  "46:45": "it really helps to know a little bit about what's going on. So first of all, let's look at examples of some problems.",
  "46:51": "The problems basically will be either",
  "46:55": "\"your learning rate is too high or low\" or \"your number of epochs is too high or low\".",
  "47:01": "So we're going to learn about what those mean and why they matter but first of all, because we're experimentalists,",
  "47:07": "let's try them.  All right?  So let's go with our teddy bear detector and let's make our learning rate",
  "47:14": "really high.  The default learning rate is 0.003 that works most of the time,",
  "47:21": "so what if we try a learning rate of 0.5? That's huge!  What happens?",
  "47:26": "Our validation loss",
  "47:28": "gets pretty damn high.",
  "47:31": "Remember, this is normally something that's underneath 1, right?",
  "47:35": "So if you see your validation loss do that, right?  Before we even learn what validation loss is, just",
  "47:41": "know this:  if it does that, your learning rate's too high.  That's all you need to know. Okay?",
  "47:47": "Make it lower.",
  "47:48": "Doesn't matter how many epochs you do. And if this happens,",
  "47:52": "there's no way to undo this you have to go back and create your neural net again and",
  "47:57": ".fit() from scratch with a lower learning rate. So that's \"Learning rate (LR) too high\".",
  "48:02": "\"Learning rate too low\"...",
  "48:05": "What if we use a learning rate not of",
  "48:08": "0.003 but 1e-5, so",
  "48:13": "0.00001, right?",
  "48:16": "So this is just I've just copied and pasted what happened when we trained before with a default error",
  "48:21": "Right now without default learning rate and within one epoch",
  "48:24": "we were down to a 2 or 3% error rate.  With this really low learning rate, our error rate does get better,",
  "48:32": "but very   very    slowly.",
  "48:35": "Right?  And you can plot it.  If you go to learn.recorder() is",
  "48:40": "an object which is going to keep track of lots of things",
  "48:43": "happening while you train.  You can call .plot_losses() to print to plot out the",
  "48:47": "validation and training loss and you can just see them just like gradually going down so slow, right?",
  "48:54": "So if you see that happening, then you have a learning rate which is   too    small.",
  "49:00": "Okay?  So bump it up by 10 or bump it up by 100 and try again.",
  "49:06": "The other thing you'll see if your learning rate is too small is that your training loss",
  "49:12": "will be higher than your validation loss.",
  "49:16": "You never want a model where your training loss is higher than your validation loss.",
  "49:23": "That always means you haven't fitted enough which means either your learning rate is too low",
  "49:30": "or your number of epochs is too low. So if you have a model like that,",
  "49:35": "train it some more or train it with a higher learning rate. Okay?",
  "49:43": "\"Too few epochs\".  So what if we train for just one epoch?",
  "49:49": "Our error rate certainly better than random,",
  "49:52": "5%.  But look at this.  The difference between training loss and validation loss.  The training loss is much higher than the validation loss. So",
  "50:01": "too few epochs and too low a learning rate look very similar, right?",
  "50:07": "And so you can just try running more epochs and if it's taking forever",
  "50:10": "you can try a higher learning rate.  Where we try a higher learning rate and the loss goes off to",
  "50:16": "100,000 million then put it back to where it was and try a few more epochs. That's the balance, right?",
  "50:21": "That's basically all you care about",
  "50:24": "99% of the time.  And this is only the one in 20 times that the defaults don't work for you.",
  "50:31": "Okay, \"Too many epochs\" (we're going to be talking more about this) create something called \"overfitting\".",
  "50:37": "If you train for too long, as we're going to learn about,",
  "50:39": "it will learn to recognize your particular teddy bears, but not teddy bears in general.",
  "50:46": "Here's the thing: Despite what you may have heard, it's very hard to overfit with deep learning.",
  "50:51": "So we were trying today to show you an example of overfitting and I turned off",
  "50:57": "And I turned off everything.",
  "51:00": "I turned (we're going to learn all about these terms soon), I turned off all the data augmentation, I turned off dropout,",
  "51:08": "I turned off weight decay, I tried to make it over fit as much as I can,",
  "51:11": "I trained it on a small-ish earning rate, I trained it for a really long time",
  "51:16": "and",
  "51:17": "like maybe I started to get it to overfit, maybe.  But",
  "51:24": "So the only thing that",
  "51:27": "tells you that you're overfitting is that the error rate improves for a while and",
  "51:32": "then starts getting worse again.",
  "51:36": "You will see a lot of people, even people that claim to understand machine learning, tell you that if your",
  "51:44": "training loss is",
  "51:46": "lower than your validation loss then you are overfitting.  As",
  "51:50": "you will learn today in more detail, and during the rest of the course, that is absolutely not true.",
  "51:55": "Any model is trained correctly will always have training loss lower than validation loss.  That is not a sign of overfitting.",
  "52:02": "That is not a sign you've done something wrong. That is a sign you have done something right.",
  "52:06": "Okay.",
  "52:08": "The sign that you are overfitting is that your error starts getting worse, because that's what you care about, right?",
  "52:14": "You want your model to have a low error?",
  "52:16": "So as long as your training and your model error is improving, you are not overfitting. How could you be?  Okay?",
  "52:24": "So there's basically the four possible, they're the main four things that can go wrong.",
  "52:28": "There are some other details that we will learn about during the rest of this course, but honestly, if you stopped listening now,",
  "52:35": "(please don't, that would be embarrassing) and you just, like \"Okay. I'm going to go and download images...\"",
  "52:41": "\"...I'm going to create CNNs with resnet34 or resnet50...\"",
  "52:45": "\"...I'm going to make sure that my learning rate and number of epochs is okay,...\"",
  "52:48": "\"and then I'm going to chuck them up in a in a Starlette Web API.\"",
  "52:53": "Most of the time, you're done. Okay?  At least for computer vision.",
  "52:58": "Hopefully you'll stick around because you want to learn about NLP and",
  "53:02": "collaborative filtering and tabular data and segmentation and stuff like that as well.",
  "53:11": "Let's now understand",
  "53:13": "what's actually going on? What does it mean?",
  "53:16": "'Loss' mean?  What does an 'epoch' mean? What does 'learning rate' mean?",
  "53:21": "Because for you to really understand these ideas you need to know what's going on",
  "53:25": "and so we're going to go all the way to the other side rather than creating a",
  "53:31": "state-of-the-art",
  "53:32": "'cougar detector', we're going to go back and create the simplest possible",
  "53:37": "linear model. Okay?  So",
  "53:41": "we're going to actually start seeing",
  "53:46": "We're actually going to start seeing a little bit of math.",
  "53:50": "Okay?  But don't be turned off. It's okay, right?  We're going to do a little bit of math,",
  "53:55": "but it's going to be totally fine, even if math's not your thing.",
  "53:59": "Because the first thing we're going to realize is that when we see a picture",
  "54:03": "Like this number eight. It's actually just a bunch of numbers.",
  "54:07": "It's a matrix of numbers.  For this grayscale one, it's a matrix of numbers, if it was a color image, it would be",
  "54:17": "have a third dimension.  So when you add an extra dimension, we call it a 'tensor' rather than a matrix.",
  "54:21": "It would be a 3D tensor of numbers: red, green, and blue.",
  "54:29": "So when we created that",
  "54:31": "teddy bear detector,",
  "54:33": "what we actually did was we created a mathematical function that took the numbers",
  "54:38": "from the images of the teddy bears and the mathematical function converted those numbers",
  "54:44": "into, in our case,",
  "54:47": "three numbers.  A number for the probability that it's a teddy, a probability that it's a grizzly, and the probability is a black bear",
  "54:54": "In this case,",
  "54:55": "there's some hypothetical function that's taking the pixels representing a handwritten digit and returning ten",
  "55:01": "numbers.  The probability for each",
  "55:05": "possible outcome:",
  "55:06": "the numbers from 0 to 9.  And so what you'll often see in",
  "55:13": "in our code and other deep learning code is that",
  "55:17": "you'll find a bunch of probabilities and then you'll find something called .max or .arg_max",
  "55:24": "attached to it a function called, and so what that function is doing is it's saying",
  "55:28": "find the highest number (the highest probability) and tell me what the index is.",
  "55:34": "So np.arg_max or torch.arg_max of this array would return",
  "55:41": "this number here. Okay, we return index \"8\". \n That makes sense? In fact,",
  "55:48": "Let's try it.",
  "55:49": "So",
  "55:51": "we know that the function to predict something is",
  "55:55": "called learn.predict()",
  "56:01": "Okay, so we can check: two question marks before it or after it to get the source code,",
  "56:07": "And here it is, right?",
  "56:10": "pred equals res (result).argmax() and",
  "56:15": "then, what is the class?  Well you just pass that into the classes array.",
  "56:19": "So like you should find that the source code in the fastai library can both kind of",
  "56:26": "strengthen your understanding of the concepts and make sure that you know, you know what's going on and and really help you here.",
  "56:33": "You've got a question. Come on over.",
  "56:39": "Q: \"Can we have a definition of the error rate being discussed and how it is calculated?  I assume it's cross validation error.\"",
  "56:48": "Sure",
  "56:49": "So one way to answer the question of 'How is error rate calculated?' would be to type",
  "56:56": "'error_rate??' (question mark) and look at the source code.  And it is",
  "57:02": "\"1 - accuracy\".  Fair enough.  And so then a question might be 'What is accuracy?\"",
  "57:08": "accuracy?? (question mark)",
  "57:11": "It is argmax.",
  "57:13": "we now know that means 'find out which particular thing it is' and then look at how often that equals the target.",
  "57:20": "So in other words the actual value and take the mean. So that's basically what it is.",
  "57:26": "And so then the question is, okay, well, what does that being applied to?",
  "57:30": "and",
  "57:31": "always in fastai,",
  "57:34": "metrics (so these things that we pass in, we call them metrics) are always going to be applied to the validation set.",
  "57:42": "Okay",
  "57:43": "So anytime you put a metric here, it'll be applied to the validation set because that's your best practice, right?",
  "57:49": "That's like, that's what you always want to do, is make sure that you're checking your",
  "57:54": "performance on data that your model hasn't seen and we'll be learning more about the validation set shortly.",
  "58:02": "Remember, you can also type",
  "58:05": "doc(term-to-look-up)",
  "58:06": "If the source code is not what you want",
  "58:08": "which it might not well be, you actually want the documentation.  That will both give you a summary of the types in and",
  "58:16": "out of the function and a link to the full",
  "58:21": "documentation where you can find out all about",
  "58:25": "how metrics work, and what other metrics there are and so forth.",
  "58:30": "And generally speaking you'll also find links to more information.",
  "58:36": "Where, for example, you will find complete runs through and sample code and so forth showing you how to use all these things",
  "58:42": "So don't forget that the doc() function is your friend. Okay?  And",
  "58:49": "also in the documentation both in the doc function and in the documentation, you'll see a source link. This is like ??,",
  "58:57": "but what the source link does is it takes you into the exact line of code in github.",
  "59:04": "So you can see exactly how that's implemented, and what else is around it so lots of good stuff there.",
  "59:12": "Q. Why were you using 3s for your learning rates earlier with 3e-5 and 3e-4?",
  "59:22": "We found that",
  "59:24": "3e-3 is just a really good default learning rate.  It works most of the time.",
  "59:31": "For your initial fine-tuning,",
  "59:34": "before you unfreeze.",
  "59:36": "And then I tend to kind of just multiply from there. So I generally find then that the the next stage",
  "59:43": "I will pick ten times lower than that",
  "59:45": "for the second part of the slice and whatever the LR_finder() found for the first part of the slice.",
  "59:52": "The second part of the slice doesn't come from the LR_finder(),",
  "59:55": "it's just a rule of thumb which is like 10 times less than your your first part which defaults to 3e-3,",
  "60:02": "and then the first part of the slice is what comes out of the LR_finder() and we'll be learning a lot more about these",
  "60:08": "learning rate details",
  "60:10": "both today and in the coming lessons.",
  "60:12": "But yeah for now all you need to remember is that in your you know, your basic approach looked like this it was",
  "60:20": "learn.fit_one_cycle(),",
  "60:23": "some number of epochs (I often pick four) and some learning rate which defaults to 3e-3.",
  "60:32": "I'll just type it up fully so you can see, and then we do that for a bit and then we unfreeze it,",
  "60:38": "right?  And then we learn some more",
  "60:42": "and so this is a bit where I just take whatever I did last time and",
  "60:47": "divide it by 10,",
  "60:49": "and then I also...",
  "60:52": "Right?  Like that?  And then I have to put one more number in",
  "60:56": "here...and that's the number that I get from the learning_rate_finder a bit where it's got the strongest slope.",
  "61:01": "So that's kind of the",
  "61:03": "Kind of \"don't have to think about it, don't really have to know what's going on\" Rule of Thumb that works most of the time.",
  "61:12": "But let's now dig in and actually understand it more completely.",
  "61:18": "So we're going to create this mathematical function that takes the numbers that represent the pixels and spits out probabilities for each possible [?]",
  "61:27": "And by the way, a lot of the stuff that we're using here, we are stealing from other people who are awesome,",
  "61:32": "and so we are putting their details here.",
  "61:35": "So like, please check out their work because they've got great work that we are highlighting in our course.",
  "61:42": "I",
  "61:43": "really like this idea of this little",
  "61:45": "animated gif of the numbers. So thank you to Adam Geitgey for creating that.  And",
  "61:51": "I guess that was probably on",
  "61:53": "Quora by the looks of this...Medium, oh, yes, it was - that terrific Medium post",
  "61:58": "I remember. I've had a whole series of Medium posts",
  "62:03": "So",
  "62:06": "So, let's look and see",
  "62:09": "how we create one of these functions.",
  "62:13": "And",
  "62:15": "let's start with the simplest function I know,",
  "62:21": "\"y = ax + b\". Okay. That's a line, right?",
  "62:33": "That's a line.",
  "62:35": "And the gradient of the line is here and the intercept of the line",
  "62:41": "is here? Okay, so hopefully,",
  "62:44": "when we said that you need to know high school math to do this course",
  "62:47": "these are the things we're assuming that you remember.  If we do kind of mention some math thing",
  "62:53": "which I'm assuming you remember and you don't remember it, don't freak out, right?",
  "62:58": "Happens to all of us.",
  "63:00": "Khan Academy is actually terrific. It's not just for school kids.  Go to Khan Academy,",
  "63:05": "find the concept you need a refresher on and he explains things really. Well, so strongly recommend",
  "63:12": "checking that out.",
  "63:15": "You know, remember I'm just a philosophy student, right?",
  "63:17": "So I, all the time, am trying to either remind myself about something or I never learnt something and so",
  "63:23": "we have the whole Internet to teach us these things.",
  "63:26": "So I'm going to rewrite this slightly",
  "63:31": "y = a1 x + a2",
  "63:38": "So let's just replace b with a2, just give it a different name. Okay.",
  "63:43": "So there's another way of saying the same thing.  Another way of saying that would be if I could multiply a2",
  "63:52": "by the number 1, okay, this still is the same thing, okay?",
  "63:57": "and",
  "63:59": "So now at this point I'm actually going to say let's not put the number 1 there but let's put an x1 here",
  "64:07": "And an x2 here and I'll say x2 equals 1 okay?",
  "64:14": "So far, this is, you know, this is pretty early high school math",
  "64:17": "This is multiplying by 1 which I think we can handle, okay?  So these two are equivalent,",
  "64:24": "with a bit of renaming.",
  "64:26": "Now, in",
  "64:28": "machine learning,",
  "64:30": "we don't just have one equation, we've got lots, right?  So if we've got some data that",
  "64:37": "represents",
  "64:40": "the temperature versus the number of ice creams sold,",
  "64:51": "then we kind of have lots of dots.",
  "64:54": "And,",
  "64:55": "so, each one of those dots, we might hypothesize,",
  "64:59": "you know, is based on this formula",
  "65:02": "\"y = a1x1 +  a2x2\"",
  "65:04": "all right?  And so basically there's lots of",
  "65:08": "(so this is our Y),",
  "65:09": "(this is our X)...",
  "65:10": "there's lots of values of y so we can stick a little \"i\" here and",
  "65:14": "There's lots of values of x so we can stick a little \"x\" here, okay?",
  "65:18": "So the way we kind of do that is a lot like numpy",
  "65:22": "indexing, right?  But rather than things in square brackets with pytorch indexing,  we kind of put them",
  "65:28": "down here in",
  "65:32": "our",
  "65:34": "kind of in the subscript of our equation. Ok?  So this is now saying there's actually lots of these different y(i)s",
  "65:41": "based on lots of different",
  "65:43": "x(i1) and x(i2), ok?  But notice there's only this is still only one of each of these.",
  "65:49": "So these things here are",
  "65:51": "called the",
  "65:53": "\"coefficients\", or the \"parameters\".",
  "65:56": "So",
  "65:58": "this is our linear equation, and this is still, we're going to say that",
  "66:04": "every x(i2) is equal to 1, ok?  Why did I do it that way?",
  "66:10": "Because I want to do linear algebra. Why do I want to do in linear algebra? Well one reason is because",
  "66:17": "Rachel teaches the world's best linear algebra course.",
  "66:20": "So if you're interested check out 'Computational Linear Algebra for Coders', so it's a good opportunity for me to throw in a pitch for this",
  "66:30": "free course, which we make no money, but never mind",
  "66:33": "But more to the point right now, it's going to make life much easier,",
  "66:37": "right?  Because I hate writing loops. I hate writing code, right?",
  "66:42": "I just want the computer to do everything for me.  At anytime",
  "66:46": "you see like these little \"i\" subscripts, that sounds like you're going to have to do loops and all kind of stuff,",
  "66:50": "but, what you might remember, from school, is that when you've got like two things being multiplied together,",
  "66:57": "two things being multiplied together, and then they get added up, that's called a",
  "67:02": "\"dot product\",",
  "67:03": "and",
  "67:06": "then if you do that for lots and lots of different numbers \"i\",",
  "67:10": "then that's called a \"matrix product\".",
  "67:14": "So, in fact, this whole thing can be written like this.",
  "67:17": "Rather than lots of different y(i)s, we can say there's one vector,",
  "67:21": "called 'y',",
  "67:22": "which is equal to",
  "67:25": "one matrix called \"X\" times one vector",
  "67:31": "called \"a\".",
  "67:33": "Now at this point, I know a lot of you don't remember that.  So that's fine, we have a picture",
  "67:41": "to show you.",
  "67:43": "I don't know who created this.  So now I do, somebody called Andre Stouts,  created this fantastic thing called \"matrixmultiplication.xyz\" and",
  "67:51": "here we have a matrix by a vector and we're going to do a \"matrix vector product\".  Go!  Pshoo...",
  "68:02": "That times that times that, plus plus plus.  That",
  "68:05": "times that times that, plus plus plus.  That times that times that, plus plus plus.",
  "68:10": "Finished!  That is what matrix vector multiplication does.",
  "68:15": "In other words,",
  "68:17": "It's just that.",
  "68:19": "Except his version is much less messy.",
  "68:24": "Okay.  So.  This is actually an excellent spot to have a little break and find out what questions we have coming through our students.",
  "68:32": "What are they asking, Rachel?",
  "68:35": "Q. When generating new image data set, how do you know how many images are enough?",
  "68:41": "What are ways to measure \"enough\"?",
  "68:43": "Yeah, that's a great question. So, another possible problem you have is you don't have enough data.",
  "68:49": "How do you know if you don't have enough data?",
  "68:52": "Because you found a good learning rate,",
  "68:54": "(because if you make it higher, then it goes off into massive losses, if you make it lower it goes really slowly)...",
  "68:60": "so you've got a good learning rate, and",
  "69:02": "then you train for such a long time that your error starts getting worse,",
  "69:08": "Okay?  So, you know that you've trained for long enough.  And you're still not happy with the accuracy.",
  "69:12": "It's not good enough for the, you know,",
  "69:16": "the 'Teddy-bear cuddling level' of safety you want.  So, if that happens",
  "69:21": "there's a number of things you can do",
  "69:23": "and we'll learn about some of them during, er, pretty much all of them, during this course, but one of the easiest ones is:",
  "69:30": "Get more data.",
  "69:31": "If you get more data, then you can train for longer, get a higher accuracy, lower error rate - without overfitting.",
  "69:41": "Unfortunately, there's no shortcut. I wish there was.  I wish there was some way to know ahead of time, how much data you need.",
  "69:46": "But I will say this; most of the time you need less data than you think.  So",
  "69:51": "organizations very commonly spend too much time gathering data getting more data than it turned out",
  "69:56": "they actually needed.  So get a small amount first and see how you go.",
  "70:00": "Q. What do you do if you have unbalanced classes such as 200 Grizzlies and 50 Teddies?  A. Uh,",
  "70:08": "nothing.  Try it.  It works.  A lot of people ask this question about how do I deal with unbalanced data?",
  "70:14": "I've done lots of",
  "70:17": "analysis with unbalanced data over the last couple of years and I just can't make it not work. It always works. So",
  "70:25": "there's a there's actually a paper,",
  "70:27": "that said, like, if you want to get it slightly better then the best thing to do is to take that",
  "70:32": "uncommon class and just make a few copies of it (that's called over sampling). But,",
  "70:38": "like, I haven't found a situation in practice where I needed to do that. I've found it always just works fine, for me.",
  "70:47": "Q. Once you unfreeze and retrain with one cycle again,",
  "70:51": "if your training loss is still lower than your validation loss (likely underfitting),",
  "70:56": "do you retrain it unfrozen again",
  "70:58": "(which will technically be more than one cycle) or do you redo everything with a longer epoch for the cycle?",
  "71:05": "Hey, you guys asked me that last week!  My answer's still the same:  I don't know. I would find,",
  "71:12": "if you do another cycle, then it'll kind of maybe generalize a little bit better if you start again,",
  "71:19": "do twice as long, it's kind of annoying;",
  "71:21": "Depends how patient you are.  It won't make much difference, you know?  For me personally, I normally just train a few more cycles.",
  "71:28": "But,",
  "71:30": "yeah, it doesn't make much difference. \n Most of the time.",
  "71:38": "Q. So showing the code sample where you were creating a CNN with resnet34 for the",
  "71:45": "'Grizzly-Teddy' classifier,",
  "71:48": "it says this requires resnet34,",
  "71:51": "which I find surprising.  I had assumed that the model created by .save(), which is about 85",
  "71:57": "megabytes on disk, would be able to run without also needing a copy of resnet34.",
  "72:08": "A.Yeah, I understand.  We're going to be learning all about this shortly.",
  "72:16": "You don't...",
  "72:18": "There's no 'copy' of resnet34.  resnet34 is actually what we call an 'architecture' -",
  "72:24": "we're going to be learning a lot about this. It's a functional form.  Just like this is a 'linear",
  "72:28": "functional form' - it doesn't take up any room, it doesn't contain anything - it's just a function.  resnett34 is just a function.",
  "72:35": "It doesn't contain anything, it doesn't store anything. I think the confusion here is that",
  "72:41": "we often use a 'pre-trained' neural net that's been learned on ImageNet. In this case,",
  "72:48": "we don't need to use a pre-trained neural net.  And actually,",
  "72:56": "to entirely avoid that even getting created you can actually pass",
  "73:03": "\"pretrained = False\" and that'll ensure that nothing even gets loaded which will save you another",
  "73:09": "0.2 seconds, I guess.",
  "73:12": "So, yeah.  But we'll be learning a lot more about this, so don't worry",
  "73:14": "if this is a bit unclear.",
  "73:15": "But the basic idea is this this thing here is is basically equivalent of saying \"is it a line?\" ?",
  "73:21": "Or \"is it a quadratic?\" or \"is it a reciprocal?\" This is just a function, this is the \"resnet34 function\" -",
  "73:29": "It's a mathematical function. It has no...",
  "73:31": "doesn't take any storage, it doesn't have any numbers, doesn't have to be loaded.",
  "73:36": "As opposed to a",
  "73:38": "pre-trained model and so that's why",
  "73:43": "when we used, when we did it at inference time the thing that took space is...",
  "73:51": "This bit.  Which is where we load our parameters",
  "73:54": "which is basically saying, as we're ready to find out, what are the values of \"a\" and",
  "73:60": "\"b\"?  We have to store",
  "74:02": "those numbers.  But for resnet34, you don't just store two numbers, you store a few million.",
  "74:10": "Or a few tens of millions of numbers.",
  "74:13": "So, why did we do all this? Well, it's because I wanted to be able to",
  "74:20": "write it out like this. And the nice thing if we can write it out like this, is that we can now",
  "74:26": "Do that in",
  "74:27": "PyTorch, with no loops,",
  "74:31": "single line of code, and it's also going to run",
  "74:34": "faster.  PyTorch really doesn't like loops, right? It really wants you to send it a whole equation to do all at once,",
  "74:41": "which means you really want to try and specify things in these kind of linear algebra ways.",
  "74:48": "So let's go and take a look because what we're going to try and do then is we're going to try and take",
  "74:54": "this, we're going to call it an 'architecture', it's like the tiniest world's tiniest neural network.",
  "74:60": "It's got two parameters, you know, a1 and a2, we're going to try and fit this architecture to some data.",
  "75:06": "So let's jump into a notebook and generate some dots right and see if we can get it to fit a",
  "75:14": "line",
  "75:16": "somehow.  And the 'somehow' is going to be using something called",
  "75:20": "S. G. D.",
  "75:24": "What is s S.G.D.? Well, there's two types of SGD. The first one is where I said, in Lesson 1,",
  "75:31": "\"Hey, you should all try building these models and try and come up with something cool.\"",
  "75:35": "And you guys all experimented and found really good stuff. So that's where the 'S' would be Student. That would be Student Gradient Descent.",
  "75:42": "So that's version one of SGD.  Version two of SGD,",
  "75:46": "which is what I'm going to talk about today, is",
  "75:48": "where we're going to have a computer try lots of things and try and come up with a really good function and that will be",
  "75:53": "called 'Stochastic Gradient Descent'.",
  "75:56": "So,",
  "75:57": "the other one that you hear a lot on Twitter is",
  "76:01": "'Stochastic Grad-student Descent', so that's the other one that you hear.",
  "76:08": "So,",
  "76:09": "we're going to jump into",
  "76:11": "\"Lesson 2:",
  "76:12": "SGD\".  And,",
  "76:14": "so we're going to kind of go bottom up rather than top down. We're going to create the simplest possible model",
  "76:21": "we can, which is going to be a linear model,",
  "76:23": "and the first thing that we need is we need some data.  And so we're going to generate",
  "76:28": "some data.  The data we're going to generate looks like this. So this might represent temperature and this rate represent number of ice creams",
  "76:35": "we sell or something like that, but we're just going to create some synthetic data that we know is following a line.  And",
  "76:41": "so, as we build this we're actually going to learn a little bit about PyTorch, as well.",
  "76:48": "So basically the way we're going to generate this data",
  "76:51": "is by creating",
  "76:53": "some coefficients.  a1 will be 3 and a2 will be 2.  And",
  "76:60": "we're going to create some...",
  "77:03": "like we've looked at before, basically a column of numbers",
  "77:08": "for each axis, and a whole bunch of ones. \n And then we're going to do this:  x@a",
  "77:13": "What is \"x@a\"?  x@a, in python, means a",
  "77:18": "matrix product",
  "77:20": "between x and a.  It actually is even more general than that.",
  "77:24": "It can be a vector-vector product, a matrix-vector product, a vector-matrix product or a matrix-matrix product.",
  "77:30": "And then actually in PyTorch, specifically, it can mean even more general things where we get into higher rank tensors,",
  "77:36": "which we will learn all about very soon. Right?  But this is basically the key",
  "77:42": "thing that's going to go on in all of our deep learning.  The vast majority of the time",
  "77:47": "our computers are going to be basically doing this: multiplying numbers together and adding them up, which is a surprisingly useful thing to do.",
  "77:58": "Ok, so",
  "77:60": "we basically are going to generate some data by creating a line",
  "78:04": "and then we're going to add some random numbers to it.",
  "78:06": "But let's go back and see how we created \"x\" and \"a\".",
  "78:09": "So I mentioned that you know, we've basically got these two coefficients, 3 and 2,",
  "78:15": "and you'll see that we've wrapped it in this function called",
  "78:19": "\"tensor()\". You might have heard this word 'tensor' before.  Who's heard the word tensor before?  About 2//3 of you. Okay, so",
  "78:27": "it's one of these words that sounds scary and",
  "78:31": "apparently, if you're a physicist, it actually is scary,",
  "78:35": "but in the world of deep learning it's actually not scary at all.  Tensor means 'array'.",
  "78:41": "Okay?  It means array.  So specifically it's an array of a regular shape, right?",
  "78:46": "So it's not an array where row 1 has two things and row 3 has three things and row 4 has one thing what you",
  "78:51": "call a 'jagged' array. That's not a tensor.  A tensor is any",
  "78:55": "array, which has a",
  "78:57": "'rectangular' or 'cube' or whatever...",
  "79:00": "you know, a shape where every element every row is the same length, and then every column is the same length",
  "79:07": "so a 4x3 matrix would be a tensor.  A",
  "79:10": "vector of length 4 would be a tensor.  A",
  "79:14": "3D array of length",
  "79:16": "3 x 4 x 6 would be a tensor. That's all a tensor is. Okay?  And so",
  "79:24": "we have these all the time.  For example, an image is",
  "79:28": "a three dimensional tensor.",
  "79:31": "It's got number of rows by number of columns by number of channels; normally red green blue.",
  "79:38": "So for example, a kind of a VGA texture would be 640",
  "79:43": "by 480 by 3 or actually...",
  "79:47": "we do things backwards, so when people talk about images they normally go width by height",
  "79:52": "but when we talk mathematically we always go a number of rows by number of columns",
  "79:56": "So it'd actually be 480 by 640 by 3",
  "80:01": "That will catch you out",
  "80:03": "We don't say 'dimensions' though, with tensors, we use one of two words: We either say 'rank' or",
  "80:09": "or 'axes'.  'Rank' specifically means how many axes are there? How many dimensions are there?",
  "80:15": "So an image is generally a \"rank 3",
  "80:19": "tensor\".",
  "80:21": "So what we've created here is",
  "80:23": "a \"rank 1 tensor\" or",
  "80:27": "also known as a 'vector', right?  But like, in math",
  "80:33": "people come up with slightly different words or actually no; they come up with very different words for slightly different concepts.",
  "80:39": "Why is a one dimensional array a 'vector' and a two dimensional array's a 'matrix' and then a three dimensional array...",
  "80:46": "Does that even have a name?",
  "80:48": "Not really.  It doesn't have a name.  Like, it doesn't make any sense. We also you know with computers",
  "80:54": "we try to have some simple consistent naming conventions. They're all called 'tensors'.  Rank",
  "80:58": "1 tensor, rank 2 tensor, rank 3 tensor.",
  "81:01": "You can certainly have a rank 4 tensor",
  "81:03": "If you've got 64 images then that would be a rank 4 tensor of 64",
  "81:10": "x 480 x 640 x 3,",
  "81:12": "for example. So tensors are very simple. They just mean arrays.  And",
  "81:18": "so, in PyTorch, you say tensor and you pass in some numbers and you get back, in this case,",
  "81:24": "just a list.  I got back a",
  "81:26": "'vector', okay?",
  "81:28": "So this, then, represents our",
  "81:32": "coefficients: the slope and the intercept of our line.  And",
  "81:37": "so, because remember,",
  "81:38": "we're not actually going to have a special case of \"ax + b\"",
  "81:43": "instead, we're going to say there's always this second x value which is always 1 (you can see it here, always 1),",
  "81:50": "which allows us just to do a simple",
  "81:52": "'matrix vector product'. Ok, so that's 'a' and",
  "81:57": "then we wanted to generate this",
  "82:01": "'x array' of data which is going to have we're going to put random numbers in the first column and a whole bunch of ones",
  "82:07": "in the second column.  So to do that, we basically say to PyTorch: \"create a",
  "82:14": "rank 2 tensor,",
  "82:19": "Actually no, sorry, let's say that again.",
  "82:22": "We see to PyTorch: \"we want to create a tensor of",
  "82:27": "'n x 2'.",
  "82:30": "So since we passed in a total of 2 things we get a rank 2 tensor.",
  "82:34": "The number of rows will be 'n' and the number of columns will be 2.  And",
  "82:40": "in there, every single thing in it will be a 1. \n That's what torch.ones() means.  And",
  "82:46": "then, this is really important, you can index into that,",
  "82:52": "just like you can index into a list in Python,",
  "82:54": "but you can put a colon (:)",
  "82:57": "anywhere.  And a colon means - \"every single value on",
  "83:01": "that axis\".  Or \"every single value on that dimension\".  So this here means every single row.  And",
  "83:08": "then this here means column 0.  So this is every row of column 0, I",
  "83:15": "want you to grab a uniform,",
  "83:17": "random number.  And",
  "83:20": "here's another very important concept: in PyTorch,",
  "83:24": "anytime you've got a function that ends in an underscore, it means \"don't return to me that uniform random number but replace",
  "83:32": "whatever this is being called on, with the result of this function\".  So this takes column 0 and",
  "83:39": "replaces it with a uniform random number between -1 and 1.  So",
  "83:45": "there's a lot to unpack there, right?  But the good news is those two lines of code,",
  "83:52": "plus this one (which we're coming to), cover",
  "83:56": "95% of what you need to know about PyTorch.  How to create an array,",
  "84:01": "how to change things in an array, and how to do matrix operations on an array, okay?",
  "84:06": "So there's a lot to unpack but these small number of concepts are incredibly powerful.",
  "84:13": "So I can now print out the first",
  "84:16": "5",
  "84:18": "rows, okay?  So",
  "84:21": "\":5\" is standard",
  "84:24": "python",
  "84:25": "'slicing' syntax, to say 'the first five rows'. So here are the first five rows, two columns",
  "84:30": "looking like my random numbers, and my ones.",
  "84:35": "So now I can do a matrix product of that x",
  "84:39": "by my a,",
  "84:41": "add in some random numbers to add a bit of noise, and",
  "84:46": "then I can do a scatter plot.  And I'm not really interested in my scatter plot in this column of ones,",
  "84:51": "right?  There just there to make my linear",
  "84:55": "function more convenient, so I'm just going to plot my",
  "84:59": "0-index column",
  "85:01": "against my \"y\"s and",
  "85:03": "there it is.",
  "85:06": "\"plt\" is what we",
  "85:09": "universally use to refer to",
  "85:11": "the plotting library 'matplotlib'.  And that's what most people use for most of their plotting in",
  "85:18": "python.  In scientific python we use matplotlib.",
  "85:22": "It's certainly a library, you'll want to get familiar with because being able to plot things is really important.",
  "85:29": "There are lots of other plotting packages.",
  "85:33": "Lots of them, the other packages, are better at certain things than matplotlib, but like matplotlib can do everything",
  "85:42": "reasonably well.",
  "85:44": "Sometimes it's a little awkward, but you know, for me, I do pretty much everything in matplotlib",
  "85:50": "because there's really nothing it can't do (even though some libraries can do other things a little bit better or a little bit prettier).",
  "85:59": "But it's really powerful, so once you know matplotlib, you can do everything.",
  "86:03": "So here I'm asking matplotlib to give me a scatterplot with my x's against my y's and",
  "86:09": "there it is, okay?  So this is my",
  "86:12": "my dummy data representing like, you know, of temperature and ice cream sales",
  "86:18": "So ,now what we're going to do is we're going to pretend we were given this data and we don't know that",
  "86:24": "the values of our coefficients are 3 and 2. So we're going to pretend that we never knew that we have to figure them out,",
  "86:31": "okay?  So how would we figure them out? How would we draw a line to fit to this data? And",
  "86:38": "why would that even be interesting?",
  "86:41": "Well, we're going to look at more about why it's interesting in just a moment,",
  "86:44": "but the basic idea is this: if we can find (this is going to be kind of",
  "86:49": "perhaps, really surprising) but if we can find a way to find those two parameters to fit that line",
  "86:56": "to those (how many points were there? - 'n' was",
  "87:01": "100) if we can find a way to fit that line to those 100 points,",
  "87:06": "we can also fit",
  "87:08": "these arbitrary functions that convert from pixel values to probabilities.",
  "87:14": "It'll turn out that there's techniques that we that we're going to learn to find these two numbers,",
  "87:21": "works equally well for",
  "87:23": "the 50 million numbers in resnet34.",
  "87:27": "So we're actually going to use an almost identical approach.  So that's (this is the bit that I found in previous classes,",
  "87:35": "people have the most trouble digesting),",
  "87:38": "like, I often find even after week 4 or week 5, people will come up to me and say \"I don't get it,",
  "87:44": "how do we actually train these models?\" - and I'll say \"It's SGD. It's that thing",
  "87:50": "we throw in the notebook with the 2 numbers\". It's like \"Yeah, but",
  "87:52": "but we're fitting a neural network\".  So \"I know, and we can't print the 50 million numbers anymore,",
  "87:59": "but it is literally,",
  "88:01": "identically, doing the same thing\".  And the reason this is hard to digest is that the human brain has a lot of trouble",
  "88:08": "conceptualizing of what an equation with 50 million numbers looks like and can do. \n So you just kind of, for now,",
  "88:14": "will have to take my word for it.  It can do things like",
  "88:19": "recognize Teddy Bears.",
  "88:21": "And all these functions turn out to be very powerful.  Now",
  "88:23": "we're going to learn a little bit more in just a moment, about how to make them extra powerful, but for now,",
  "88:29": "the thing we're going to learn to fit these two numbers is the same thing that we've just been using to fit 50 million numbers.",
  "88:36": "Okay, so we want to find",
  "88:39": "what PyTorch calls 'parameters'.  Or in statistics, you'll often hear called 'coefficients'.  These values a1 and a2.",
  "88:47": "We want to find these parameters",
  "88:50": "such that the line that they create",
  "88:54": "minimizes the error",
  "88:56": "between that line and the points.",
  "89:01": "So in other words,",
  "89:06": "you know, if we created,",
  "89:10": "you know, if the a1 and a2 we came up with resulted in this line,",
  "89:16": "then we'd look and we'd see like how far away is that line from each point? I would say",
  "89:21": "\"Oh, that's quite a long way\". And so maybe there was some other a1 or a2 which resulted in",
  "89:29": "this line and they would say, like, \"oh, how far away is each of those points\"?  And then eventually we come up with",
  "89:38": "Blue",
  "89:41": "We come up with this line and it's like, \"Oh, in this case, each of those is actually very close\".",
  "89:46": "All right?",
  "89:47": "So you can see how in each case we can say how far away is the line at each spot away from its point and",
  "89:54": "then we can take the average of all those and",
  "89:57": "that's called the 'loss'.",
  "89:59": "And that is the value of our loss, right?  So you need some mathematical function that",
  "90:04": "can basically say how far away is this line from those points?",
  "90:10": "For this kind of problem, which is called a 'regression' problem ,a problem where your",
  "90:16": "dependent variable",
  "90:19": "Is",
  "90:21": "'continuous', so rather than being",
  "90:23": "\"Grizzlies\" or \"Teddies\", it's like some number between",
  "90:27": "-1 and 6, this is called a regression problem.",
  "90:29": "And for regression the most common loss function is called 'mean squared error', which pretty much everybody calls",
  "90:37": "'MSE'.  You may also see RMSE just 'Root Mean Squared Error'. And so the mean squared error is a loss,",
  "90:43": "it's the difference between some prediction that you've made,",
  "90:47": "okay, which you know is like the value of the line, and the actual",
  "90:52": "number of ice cream sales.",
  "90:54": "And so, in the mathematics of this,",
  "90:57": "people normally refer to the actual, they normally call it \"y\" and the prediction, they normally call it",
  "91:03": "\"y hat\", as in they they write it",
  "91:07": "like that.",
  "91:10": "And so",
  "91:12": "what I try to do like when we're writing something like, you know, mean squared error equation,",
  "91:18": "there's no point writing ice cream here and temperature here because we wanted to apply it to anything.  So we tend to use these like",
  "91:25": "mathematical placeholders.",
  "91:28": "So the value of mean squared error is simply the difference between those two,",
  "91:34": "squared!  All right?  And then we can take the mean.  Because, remember,",
  "91:39": "that is actually a 'vector' or what we now call it, a \"rank 1 tensor\" and that is actually a rank 1 tensor,",
  "91:47": "so it's the value of the number of ice cream sales at each place. And so when we subtract",
  "91:53": "one vector from another vector,",
  "91:56": "(and we're going to be learning a lot more about this), but it does something called element-wise arithmetic in other words",
  "92:00": "It subtracts each each one from each other,",
  "92:03": "and so we end up with a vector of differences, and",
  "92:06": "then if we take the square of that, it squares everything in that vector.  And so then we can take the mean of that",
  "92:12": "to find the average",
  "92:15": "square of the differences between the actuals and the predictions.  So, if you're",
  "92:22": "more comfortable with mathematical notation what we just wrote there was the",
  "92:29": "\"sum of...\" (which way round did we do it?)",
  "92:33": "y hat minus...",
  "92:36": "y...",
  "92:38": "squared, over...",
  "92:41": "n\", right?  So that equation is the same as",
  "92:46": "that equation. So one of the things I'll note here is, I don't think this is,",
  "92:54": "you know, more",
  "92:57": "complicated or unwieldy than this, right?",
  "93:01": "But the benefit of this is you can experiment with it like once you've defined it,",
  "93:07": "you can use it you can send things into it and get stuff out of it and see how it works, alright?  So, for me,",
  "93:13": "most of the time I prefer to explain things with code rather than with math.",
  "93:19": "Right?  Because I can actually...they're the same, they're doing, in this case at least, in",
  "93:24": "all the cases we'll look at, they're exactly the same, they're just different notations for the same thing. But one of the notations is executable,",
  "93:32": "it's something that you can experiment with, and one of them is",
  "93:36": "abstract, so that's why I'm generally going to show code.",
  "93:40": "So the good news is, if you're a coder,",
  "93:44": "with not much of a math background,",
  "93:47": "actually, you do have a math background because code is math.  Right?",
  "93:51": "Now if you've got more of a math background and less of a code background,",
  "93:55": "then actually a lot of the stuff that you learned from math is going to translate very directly",
  "93:59": "into code, and now you can start to experiment really with your math.",
  "94:04": "Okay, so this is a 'loss function'.  This is something that tells us how good our line is.",
  "94:08": "So now, we have to kind of come up with:",
  "94:12": "\"What is the line that fits through here?\"  Remember, we don't know (we're going to pretend we don't know)",
  "94:17": "so what you actually have to do is you have to guess.",
  "94:21": "You actually have to come up with a guess:",
  "94:23": "what are the values of a1 and a2?  So let's say we guess that a1 and a2 are both 1.",
  "94:29": "So this is our tensor.  'a'",
  "94:32": "is (1.0, 1.0), right?",
  "94:35": "So here is how we create that",
  "94:37": "tensor.  And I wanted to write it this way because you'll see this all the time.",
  "94:42": "Like, written out it should be \"1.0...\" (sorry...it should be -1)...",
  "94:50": "Written out fully it would be \"-1.0...",
  "94:53": "1.0\". Like that's written out fully.",
  "94:56": "We can't write it without the point,",
  "94:59": "because that's now an 'int',",
  "95:02": "not a floating point. So that's going to \"spit the dummy\" if you try to do calculations with that in neural nets, all right?",
  "95:11": "I'm lazy, I'm far too lazy to type \".0\" every time.  python knows perfectly",
  "95:15": "well that if you add a dot next to any of these numbers,",
  "95:19": "then the whole thing is now floats, right?  So that's why you'll often see it written this way, particularly by lazy people like me.",
  "95:27": "Okay, so 'a'",
  "95:30": "is",
  "95:31": "a tensor.  You can see it's floating-point - you see like, even PyTorch is lazy,",
  "95:36": "they just put a \".\" they don't bother with a 0, right?",
  "95:38": "But if you want to actually see exactly what it is. You can write \".type()\"",
  "95:43": "and",
  "95:45": "you can see it's a 'float' tensor, okay?",
  "95:51": "And so now we can calculate our predictions with this, like, random guess",
  "95:57": "x@a (matrix product of x and a), and we",
  "96:01": "can now calculate the mean squared error of our predictions and their actuals and that's our loss.",
  "96:08": "Okay, so for this regression, our loss is 8.9. \n And",
  "96:13": "so we can now plot a",
  "96:15": "scatter plot of x against y and we can plot the scatter plot of x against y-hat (our predictions) and",
  "96:23": "there they are.",
  "96:24": "Okay, so this is the (1 , -1) line",
  "96:29": "...sorry, the (-1,  1) line and here's actuals. So that's not great, not surprising, it's just a guess.",
  "96:35": "so SGD,",
  "96:38": "or \"gradient descent\" more generally (and anybody who's done any",
  "96:42": "engineering or probably computer science at school will have done plenty of this, like Newton's method what all the stuff that you did...",
  "96:50": "university -  if you didn't, don't worry, we're going to learn it now)...",
  "96:53": "It's basically about taking this guess and trying to make it a little bit better.",
  "96:59": "So, how do we make it a little bit better?",
  "97:01": "Well, there's only two numbers right and the two numbers are",
  "97:06": "and the two numbers are the",
  "97:09": "intercept of that orange line and the gradient of that orange line.",
  "97:12": "So what we're going to do with gradient descent is we're going to simply say:",
  "97:17": "\"What if we change those two numbers a little bit, what if we made the intercept a little bit higher...?\"",
  "97:23": "or a",
  "97:24": "little bit lower?",
  "97:27": "What if we made the gradient a little bit more positive or",
  "97:33": "a little bit more negative?",
  "97:35": "So there's like four possibilities.  And then we can just calculate the loss",
  "97:41": "for each of those four possibilities and see what see what worked.  Did lifting it up or down make it better?",
  "97:47": "Did tilting it more positive or more negative make it better?  And then all we do is we say, okay,",
  "97:52": "well, whichever one of those made it better",
  "97:54": "that's what we're going to do.  And",
  "97:57": "that's it. Right?  But here's the cool thing for those of you that remember calculus -",
  "98:02": "you don't actually have to move it up and down and round about,",
  "98:07": "you can actually calculate the 'derivative'. \n The derivative is the thing that tells you...",
  "98:12": "Would moving it up or down make it better or would rotating it this way or that way make it better?",
  "98:17": "Okay, so the good news is if you didn't do calculus or you don't remember calculus,",
  "98:22": "I just told you everything you need to know about it, right?  Which is that it tells you how changing one thing",
  "98:30": "changes the function, right?  That's what the derivative is.",
  "98:35": "Kind of, not quite strictly speaking right, close enough, also called the 'gradient'.",
  "98:39": "Okay, so the gradient or the derivative, tells you how changing a1, up or down, would change",
  "98:46": "our MSE, how changing a2 up or down will change your MSE and this does it more quickly.",
  "98:52": "Does it more quickly than actually moving it up and down? Okay?",
  "98:56": "So,",
  "98:59": "in school,",
  "99:01": "unfortunately, they forced us to sit there and calculate these derivatives by hand.",
  "99:06": "We have computers!  Computers can do that for us.  We are NOT going to calculate them by hand.",
  "99:12": "Instead,",
  "99:14": "we're going to call",
  "99:17": "\".grad\".",
  "99:19": "On our computer that will calculate the gradient for us.",
  "99:23": "So here's what we're going to do -",
  "99:25": "we're going to create a loop, we're going to loop through 100 times and we're going to call a function called .update().",
  "99:32": "That function is going to calculate y-hat",
  "99:36": "(our prediction),",
  "99:38": "It is going to calculate loss",
  "99:41": "(our mean squared error).",
  "99:43": "From time to time it will print that out so we can see how we're going.",
  "99:48": "It will then calculate the gradient and in PyTorch calculating the gradient is done by using a method called .backward().",
  "99:56": "So you'll see something really interesting which is, mean squared error",
  "100:00": "was just a simple",
  "100:03": "standard mathematical function,",
  "100:06": "PyTorch, for us,",
  "100:09": "keeps track of how it was calculated and lets us calculate the derivatives.",
  "100:14": "So if you do a mathematical operation on a tensor in PyTorch, you can call .backward() to calculate the derivative.",
  "100:22": "What happens to that derivative?  That gets stuck inside an attribute called .grad.",
  "100:28": "So I'm going to take my coefficients 'a' and I am going to subtract from them",
  "100:35": "my gradient.  And",
  "100:37": "this underscore here... Why?  Because that's going to do it in place.  So it's going to actually update those coefficients a",
  "100:46": "to subtract",
  "100:48": "the gradients from them, right?  So, why do we subtract?   Well because the gradient tells us if I",
  "100:55": "move the whole thing",
  "100:58": "downwards, the loss goes up.  If I move the whole thing upwards, the loss goes down.",
  "101:02": "So I want to like do the opposite of the thing that makes it go up, right?",
  "101:07": "So because our loss, we want to loss to be small.  So that's why we have to subtract.",
  "101:12": "And then there's something here called \"lr\".",
  "101:16": "\"lr\" is our",
  "101:20": "learning rate.  And so literally all it is is the thing that we multiply by the gradient.",
  "101:28": "Why is there any 'lr' at all? Let me show you why.",
  "101:36": "Let's take a really simple example.",
  "101:44": "A quadratic.",
  "101:46": "All right, and let's say your algorithm's job was to find where that quadratic was at its lowest point.",
  "101:52": "And so, well, how could it do this? Well, just like what we're doing now, the starting point would just be to pick",
  "101:59": "some x value at random.  And",
  "102:02": "then, pop up here to find out what the value of y is.",
  "102:06": "Okay?  That's its starting point. And so then it can calculate the gradient and the gradient is simply the slope.",
  "102:14": "Right?  It tells you",
  "102:16": "moving in which direction is going to make you go down.  And so the gradient tells you you have to go this way.",
  "102:23": "So, if",
  "102:25": "the gradient was really big, you might jump this way a very long way.",
  "102:31": "So you might jump all the way over to...",
  "102:37": "Here.  Maybe even here.  Right?  And so if you jumped over to there...",
  "102:46": "Then that's actually not going to be very helpful because then, you see, well where does that take us to?  Oh!",
  "102:52": "It's now worse.",
  "102:54": "Right? We jumped too far.",
  "102:58": "So we don't want to jump too far, so maybe we should just jump a little bit.",
  "103:05": "Maybe to here.  And",
  "103:07": "the good news is that is actually a little bit closer.",
  "103:10": "And so then we'll just do another little jump;  see what the gradient is and do another little jump.",
  "103:15": "That takes us to here.  And another little jump.  That takes us to here.",
  "103:19": "Here. Yeah, right. So in other words,",
  "103:22": "we find our gradient to tell us kind of what direction to go and like,  do we have to go a long way or not",
  "103:28": "too far?",
  "103:29": "But then we multiply it by some number, less than one,",
  "103:33": "so we don't jump too far.  And",
  "103:36": "so, hopefully at this point, this might be reminding you of something.  Which is 'what happened",
  "103:44": "when our learning rate was too high'?",
  "103:49": "So do you see why that happened now?  Our learning rate was too high,",
  "103:53": "meant that we jumped",
  "103:55": "all the way past the right answer further than we started with and it got worse and worse and worse.",
  "104:04": "So that's what a 'learning rate too high' does.",
  "104:11": "On the other hand, if our learning rate is too low",
  "104:16": "then you just take tiny little steps and",
  "104:20": "so, eventually you're going to get there but you're doing lots and lots of calculations along the way.",
  "104:24": "So you really want to find something where it's kind of either like this",
  "104:30": "Or maybe it's kind of a little bit backwards and forwards, maybe it's kind of like this...",
  "104:36": "Something like that, you know, you want something that kind of gets in there quickly,",
  "104:40": "but not so quickly it jumps out and diverges.  Not so slowly that it takes lots of steps.",
  "104:46": "So that's why we need a good learning rate. And so that's all it does.",
  "104:52": "So if you look inside the source code of any deep learning library, you will find",
  "104:57": "this.  You will find something that says coefficients.subtract(learning rate) * gradient.  And",
  "105:05": "we'll learn about some minor...not minor... We'll learn about some easy but important optimizations we can do to make this go faster.",
  "105:14": "But that's basically it.",
  "105:16": "There's a couple of other little minor issues that we don't need to talk about now one involving zeroing out the",
  "105:21": "gradients and another involving making sure that you turn gradient calculation off when you do the SGD update.  If",
  "105:28": "you're interested we can discuss them on the forum or",
  "105:32": "you can do our",
  "105:34": "\"Introduction to Machine Learning\" course, which covers",
  "105:38": "all the mechanics of this in more detail.",
  "105:42": "But this is the basic idea.",
  "105:44": "So if we run update() 100 times,",
  "105:46": "printing out the loss from time to time you can see it starts at 8.9,",
  "105:51": "and it goes down down down down down down down.  And so we can then print out scatter plots and",
  "105:57": "there it is.",
  "105:59": "That's it.  Believe it or not, that's gradient descent.",
  "106:04": "So we just need to start with a function that's a bit more complex than",
  "106:09": "x@a",
  "106:12": "But as long as we have a function that can represent things like 'is this a teddy bear?',",
  "106:16": "we now have a way to fit it.",
  "106:19": "Okay?  So let's now take a look at this as an animation and",
  "106:25": "this is one of the nice things that you can do",
  "106:28": "with...",
  "106:32": "This is one of the nice things that you can do with matplotlib is you can take any plot and turn it into an animation.",
  "106:39": "That and so you can now actually see it updating each step. So let's see what we did here.",
  "106:44": "We simply said, as before, create a scatter plot,",
  "106:50": "but then rather than having a loop, we used matplotlib FuncAnimation()",
  "106:56": "so call 100 times,",
  "106:60": "this function.  And this function just called that update() that we created earlier and then updated the 'y'",
  "107:07": "data in our line.  And so did that 100 times...",
  "107:11": "waiting 20 milliseconds after each one ,and there it is. Right?  So you might think that, like,",
  "107:18": "visualizing your algorithms with animations is some",
  "107:22": "amazing and complex thing to do, but actually now, you know",
  "107:25": "It's 1 2 3 4 5 6 7 8 9 10 11 lines of code.",
  "107:30": "Okay?  So I think that is pretty damn cool.",
  "107:36": "So that is SGD",
  "107:38": "visualized.  And so we can't visualize as",
  "107:41": "conveniently what updating 50 million parameters in a resnet34 looks like,",
  "107:46": "but it's basically doing the same thing, okay?  And so studying these simple versions is actually a great way to get an intuition.",
  "107:52": "So you should try running this notebook with a really big learning rate, with a really small",
  "107:58": "learning rate, and see what this animation looks like, right, and try get a feel for it.",
  "108:02": "Maybe you can even try a 3d plot. I haven't tried that yet, but I'm sure it would work fine too.   So",
  "108:10": "the only difference between",
  "108:12": "Stochastic Gradient Descent and this is something called 'minibatches'.",
  "108:16": "You'll see what we did here was we calculated the value of the loss on the whole data set on every iteration.",
  "108:23": "But if your data set is one and a half million images in ImageNet,",
  "108:27": "that's going to be really slow, right?  Just to do a single",
  "108:30": "update of your parameters you've got to calculate the loss on one and a half million images.",
  "108:35": "You wouldn't want to do that.",
  "108:37": "So what we do is we grab",
  "108:40": "64 images or so at a time,",
  "108:42": "at random, and",
  "108:44": "we calculate the loss on those 64 images, and",
  "108:47": "we update our weights.  And then we grab another 64 random images. We update the weights. So in other words, the loop",
  "108:55": "basically looks exactly the same, but at this point here - so it'd basically be y",
  "109:01": "square bracket and some",
  "109:03": "random indexes here, you know, and some",
  "109:07": "random indexes here and",
  "109:10": "we'd basically do the same thing and well actually, sorry, it would be",
  "109:17": "there, right, so some random indexes on our x and some random indexes on our y to do a minibatch at a time,",
  "109:23": "and that would be the basic difference.  And, so, once you add those,",
  "109:28": "you know, grab a random few points each time,",
  "109:31": "those random few points accord your minibatch and that approach is called SGD, or Stochastic Gradient Descent.",
  "109:39": "Okay, so",
  "109:41": "there's quite a bit of vocab we've just covered, right? So let's just",
  "109:47": "remind ourselves:",
  "109:49": "the 'learning rate' is a thing that we multiply our gradient by, to decide how much to update the weights by.",
  "109:58": "An 'epoch' is one complete run through all of our",
  "110:03": "data points (all of our images).  So for the non-stochastic gradient descent",
  "110:08": "we just did, every single loop, we did the entire data set.  But if you've got a data set with a",
  "110:15": "thousand images and your mini-batch size is 100 then it would take you ten iterations",
  "110:20": "to see every image once, so that would be one 'epoch'.",
  "110:25": "Epochs are important because if you do lots of epochs,",
  "110:30": "then you're looking at your images lots of times, and so every time you see an image",
  "110:35": "there's a bigger chance of overfitting.  So we generally don't want to do too many epochs.",
  "110:41": "A 'minibatch' is just a random bunch of points that you use to update your weights.",
  "110:48": "SGD is just gradient descent using minibatches.",
  "110:54": "Architecture and model kind of mean the same thing.  In this case, our architecture is",
  "111:02": "y = Xa",
  "111:05": "All right?  The architecture is the mathematical function that you're fitting the parameters to.",
  "111:10": "And we're going to learn either",
  "111:13": "today or next week,",
  "111:15": "what the mathematical function of things like resnet34, actually is.",
  "111:20": "But it's basically",
  "111:21": "pretty much what you've just seen.  It's a bunch of matrix products.",
  "111:27": "'Parameters', also known as coefficients, also known as weights,",
  "111:32": "are the numbers that you're updating.  And",
  "111:35": "then 'loss function' is the thing that's telling you how far away or how close you are to the correct answer.",
  "111:41": "Any questions?",
  "111:45": "All right.",
  "111:46": "So, these models, these predictors, these Teddy Bear Classifiers, are functions that take pixel values and",
  "111:53": "return probability.",
  "111:55": "They start with some",
  "111:57": "functional form, like",
  "111:59": "y = Xa, and they fit the parameters, 'a',",
  "112:04": "using SGD, to try and do the best to calculate your predictions.",
  "112:10": "So far we've learned how to do regression, which is a single number.",
  "112:16": "Next week, we'll learn how to do the same thing for classification where we have multiple numbers.  But it's basically the same.",
  "112:27": "In the process,",
  "112:28": "we had to do some math.",
  "112:30": "We had to do some linear algebra",
  "112:32": "and we had to do some calculus.  And a lot of people get a bit scared at that point and",
  "112:39": "tell us \"I am NOT a math person\".",
  "112:42": "If that is you, that's totally okay,",
  "112:46": "but you're wrong. You are a math person.",
  "112:48": "In fact,",
  "112:49": "it turns out that when in the actual academic",
  "112:52": "research around this, there are not math people and non-math people. It turns out to be entirely a result of",
  "112:60": "culture and expectations.  So you should check out Rachel's talk",
  "113:06": "\"There's No Such Thing As Not a Math Person\",",
  "113:10": "where she will introduce you to some of that academic research.  And so if you think of yourself as not a math person",
  "113:15": "you should watch this so that you learn that you're wrong,",
  "113:19": "that your thoughts are actually there because somebody has told you 'you're not a math person',",
  "113:25": "but there's actually no academic research to suggest that there is such a thing.",
  "113:30": "there are some cultures, like Romania and China, where the 'not a math person' concept never even appeared.",
  "113:39": "It's almost unheard of in some cultures for somebody to say \"I'm not a math person\" because that just never entered",
  "113:47": "that cultural identity.  So,",
  "113:51": "don't freak out if words like 'derivative' and 'gradient' and 'matrix product' are things that you're kind of scared of,",
  "113:59": "it's something you can learn. It's something you'll be okay with.  Okay?",
  "114:05": "So the last thing that we're going to close with today...",
  "114:10": "Oh, I just got a message from Simon Willison.",
  "114:16": "Ah!  Simon's telling me he's actually not that special, lots of people won medals. So,",
  "114:25": "That's the worst part about Simon. Not only is he really smart he's also really modest which I think it's just awful.",
  "114:34": "I mean if you're going to be that smart, at least be a horrible human being and, you know, make it okay.",
  "114:42": "Okay, so,",
  "114:43": "the last thing I want to close with is the idea of (and we're going to look at this more next week)",
  "114:48": "underfitting and overfitting.",
  "114:53": "We just fit a line",
  "114:55": "to our data.  But imagine that our data wasn't actually line 'shaped', right? And so if we try to fit something which was, like",
  "115:04": "\"constant + constant * x\", a line to it, then it's never going to fit very well.",
  "115:09": "Right?  No matter how much we change these two",
  "115:12": "coefficients, it's never going to get really close.",
  "115:16": "On the other hand, we could fit some much bigger equation,",
  "115:20": "so in this case, it's a higher degree polynomial, with lots of lots of wiggly bits like so.  Right?",
  "115:26": "But if we did that,",
  "115:27": "it's very unlikely we go and look at some other place to find out the temperature that it is and how much ice cream they're",
  "115:33": "selling and that will get a good result,",
  "115:35": "because, like, the wiggles are far too wiggly.",
  "115:38": "So this is called 'overfitting'.",
  "115:41": "We're looking for some mathematical function that fits \"just right\",",
  "115:45": "to stay with the teddy bear analogy.  So",
  "115:51": "you might think, if you have a statistics background, the way to make things fit \"just right\"",
  "115:56": "is to have exactly the right number of parameters.  To use a mathematical",
  "116:00": "function that",
  "116:01": "doesn't have too many parameters in it.  It turns out that's actually completely not the right way to think about it.",
  "116:08": "There are other ways to make sure that we don't overfit, and in general, this is called 'regularization'.",
  "116:14": "Regularization are all the techniques to make sure that when we",
  "116:19": "train our model, that it's going to work",
  "116:21": "not only well on the data its seen but on the data it hasn't seen yet.",
  "116:28": "So, the most important thing",
  "116:30": "to know when you've trained a model, is actually 'how well does it work on",
  "116:36": "data that it hasn't been trained with'?",
  "116:38": "And so as we're going to learn a lot about next week, that's why we have this thing called a 'validation set'.",
  "116:44": "So what happens with a validation set, is",
  "116:48": "that we do our",
  "116:51": "minibatch SGD training loop with one set of data (with one set of teddy bears, grizzlies, \n black bears) and",
  "116:58": "then when we're done, we",
  "116:60": "check the loss function and the accuracy",
  "117:03": "to see how good is it on a bunch of images which were not included in the training. And so, if we do that,",
  "117:09": "then if we have something which is too wiggly,",
  "117:12": "it'll tell us.  \"Oh, your loss function and your error is really bad\", because on the bears that it hasn't been trained with,",
  "117:18": "the wiggly bits are in the wrong spot.  Where if it was underfitting, it would also tell us",
  "117:24": "that your validation set's really bad. So, like,",
  "117:29": "even for people that",
  "117:31": "don't go through this course and don't learn about the details of deep learning, like if you've got",
  "117:38": "managers or colleagues or whatever, at work, who are kind of wanting to, like, learn about AI,",
  "117:43": "the only thing that you really need to be teaching them is about the idea of a validation set.",
  "117:47": "Because that's the thing they can then use to figure out, you know, if somebody's selling them snake oil or not, you know,",
  "117:53": "they're like, hold back some data and then they get told, like, \"oh",
  "117:56": "here's a model that we're going to roll out\" and then you say \"okay, fine...",
  "117:59": "I'm just going to check it on this held out data to see whether it generalizes.\"",
  "118:04": "There's a lot of details to get right when you design your validation set. We will talk about them,",
  "118:11": "briefly, next week, but a more full version would be in Rachel's",
  "118:17": "piece on the fast.ai blog called \"How (and why) to create a good validation set\".",
  "118:22": "And this is also one of the things we go into in a lot of detail in the 'Intro to Machine Learning' course.",
  "118:27": "So we're going to try and give you enough to get by,",
  "118:30": "for this course, but it is certainly something that's worth deeper study as well.  Any questions or comments before we wrap up?",
  "118:39": "Okay, good.  All right, well, thanks everybody. I hope you have a great time building your web applications. See you next week."
}
