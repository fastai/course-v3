welcome to lesson 12 well we're moving along and this is an exciting lesson because it's where we're going to wrap up all the pieces both for computer vision and for NLP and you might be surprised to hear they're going to wrap up all the pieces to NLP because we haven't really done any NLP yet but actually everything we've done is equally applicable to NLP so there's very little to do to get a state-of-the-art result on IMDb sentiment analysis from scratch so that's what we're going to do well before we do let's finally finish off this slide we've been going through for three lessons now I promised not promised that we would get something stayed with yet on imagenet turns out we did so you're going to see that today so we're going to finish off mix up label smoothing and resinates okay so let's do it before we look at the new stuff oh nine bean learner I've made a couple of minor changes that I thought you might be interested in it's kind of like ask you refactor things so remember last week we refactored the learner to get rid of that or four separate runner so there's just no one thing made a lot of our code a lot easier that they'll still this concept left behind that when you started fitting you had to tell each callback what it's learner or Runner was I've moved that because we don't you know they're all totally attached now I've moved that to the init and so now you can call add Seabees to add a whole bunch of call backs or add CB to add one call back and that happens automatically at the start of trail that's a very minor thing more interesting was when I did this little reformatting exercise where I took all these callbacks that used to be on the line underneath the thing before them and line them up over here and suddenly realized that now I can answer all the questions I have in my head about our callback system which is what exactly are the steps in the training loop what exactly are the callbacks that you can use in the training loop step goes with which callback which steps don't have a callback are there any callbacks that don't have a step so like it's one of one of these interesting things where I really don't like the idea of kind of automating your formatting and creating rules for formatting when like something like this can just like as soon as I did this I understood my code better and for me understanding my code is the only way to make it work because debugging machine learning code is awful so you've got to make sure that the thing you write makes sense it's going to be simple it's gonna be really simple so this is really simple then more interestingly we used to create the optimizer in in it and you could actually pass in an already created optimizer I removed that and the only thing you know you can pass in as an optimization function so something that will create an optimizer which is what we've always been doing anyway and by doing that we can now create their optimizer when we start fitting and that turns out to be really important because when we do things like discriminative learning rates and gradual unfreezing and layer groups and stuff we can change things and then when we fit it'll all just work so that's a more significant it's like one line of code but it's conceptually a very significant change okay so that's some minor changes to 9b and now let's move on to mix up and local smoothing so I'm really excited about the stuff we saw at the end of the last lesson where we saw how we can use the GPU to do data augmentation fully randomized fully GPU accelerated data augmentation using just plain PI torch operations I think that's a big win but it's quite possible we don't need that kind of data augmentation anymore because in our experimentation with this data augmentation called mix-up we found we can remove most other data augmentation and get amazingly good results so it's just a kind of a simplicity resolved and also you when you use mix-up you can train for a really long time and get really good results so let me show you mix up and in terms of the results you can get what happened in the bag of tricks paper was they when they turned mixup on they also started training for two hundred epochs as instead of ninety sorry instead of 120 so be a bit careful when you interpret their paper table when it goes from label smoothing 94.1 to mix up with that distillation ninety four point six they're also nearly doubling the number of epochs they do but you can kind of get a sense that you can get big decrease in error the other thing they mentioned in the paper is distillation I'm not going to talk about that because it's a thing where you pre train some much bigger model like a resonant one five two and then you try and train something that predicts the output of that and to me the idea of like training a really big model to train a smaller model it's interesting but it's not exactly training in the way I normally think about it so we're not looking at distillation it would be an interesting assignment if somebody wanted to try adding it to the notebooks though you have all the information I think all those skills you need to do that now alright so mix up we stop by grabbing imagenet data set and we grab the Baker GB and resize and turn it into a float tensor this is just our quick and dirty resize really doing this for testing purposes split it up create a data bunch all the normal stuff and what we're going to do is we're going to take an image like this and an image like this and we're going to combine them we're going to take point three times this image plus point seven times this image and this is what it's going to look like unfortunately Silva and I have different orderings of file names on our things so I wrote it's a French one and attention that actually silver clearly doesn't have French horn or dentures but you get the idea it's a except of two different images so we're going to create a dreidel orientation where every time we predict something we got a pretty predicting a mix of two things okay so we're going to both take the linear combination point three and point seven of the two images but then we're going to have to do that for the labels as well right there's no point predicting the one hot encoded output of this breed of doggy where there's also a bit of a best pump so we're also going to have we're not going to have one hot encoded output we're going to have a point seven encoded doggy and a point three encoded gas pump right so that's the basic idea so the mix up paper was super cool well there people are talking about things that aren't deep learning I guess that's their priorities so the paper is a pretty nice easy read by paper standards and I would definitely suggest you check it out so I've told you what we're going to do implementation wise we have to decide what number to use here is at point three or point one or point five or what and this is the data augmentation method so the answer is will randomize it but we're not going to randomize it from naught to one uniform or not to 0.5 uniform but instead we're going to randomize it using shapes like this in