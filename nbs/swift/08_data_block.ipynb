{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data block foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_07_batchnorm\")' FastaiNotebook_07_batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow\n",
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_07_batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image ItemList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Imagenette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, we need to download Imagenette and untar it. What follows is very close to what we did for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public let dataPath = Path.home/\".fastai\"/\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func downloadImagenette(path: Path = dataPath, sz:String=\"-160\") -> Path {\n",
    "    let url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette\\(sz).tgz\"\n",
    "    let fname = \"imagenette\\(sz)\"\n",
    "    let file = path/fname\n",
    "    try! path.mkdir(.p)\n",
    "    if !file.exists {\n",
    "        downloadFile(url, dest:(path/\"\\(fname).tgz\").string)\n",
    "        _ = \"/bin/tar\".shell(\"-xzf\", (path/\"\\(fname).tgz\").string, \"-C\", path.string)\n",
    "    }\n",
    "    return file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let path = downloadImagenette(sz:\"-320\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at `path.ls()`, we see it returns a list of entries, which are structures with a `kind` and a `path` attribute. The `kind` is an enum that can be `file` or `directory`. `path` then points to the corresponding location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in path.ls() { print(\"\\(e.path) (\\(e.kind == .directory ? \"directory\": \"file\"))\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in (path/\"val\").ls() { print(\"\\(e.path) (\\(e.kind == .directory ? \"directory\": \"file\"))\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look inside a class folder (the first class is tench):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let pathTench = path/\"val\"/\"n01440764\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let imgFn = Path.home/\".fastai/data/imagenette-320/val/n01440764/ILSVRC2012_val_00006697.JPEG\"\n",
    "imgFn.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `tf.data` to read and resize our images in parallel. `tf.data` needs to operate on tensors, so we convert our `Path` image filename to that format. We can then apply the extensions that we defined previously in 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let decodedImg = StringTensor(readFile: imgFn.string).decodeJpeg(channels: 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decodedImg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By converting this image to numpy, we can use `plt` to plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func show_img<T:NumpyScalarCompatible>(_ img: Tensor<T>, _ w: Int = 7, _ h: Int = 5) {\n",
    "    show_img(img.makeNumpyArray(), w, h)\n",
    "}\n",
    "\n",
    "public func show_img(_ img: PythonObject, _ w: Int = 7, _ h: Int = 5) {\n",
    "    plt.figure(figsize: [w, h])\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(decodedImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have donloaded the data, we need to be able to recursively grab all the filenames in the imagenette folder. The following function walks recursively through the folder and adds the filenames that have the right extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func fetchFiles(path: Path, recurse: Bool = false, extensions: [String]? = nil) -> [Path] {\n",
    "    var res: [Path] = []\n",
    "    for p in try! path.ls(){\n",
    "        if p.kind == .directory && recurse { \n",
    "            res += fetchFiles(path: p.path, recurse: recurse, extensions: extensions)\n",
    "        } else if extensions == nil || extensions!.contains(p.path.extension.lowercased()) {\n",
    "            res.append(p.path)\n",
    "        }\n",
    "    }\n",
    "    return res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't have a generic `open_image` function like in python here, but will be using a specific decode function (here for jpegs, but there is one for gifs or pngs). That's why we limit ourselves to jpeg exensions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time { let fNames = fetchFiles(path: path, recurse: true, extensions: [\"jpeg\", \"jpg\"]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let fNames = fetchFiles(path: path, recurse: true, extensions: [\"jpeg\", \"jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames.count == 13394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset` can handle all the transforms that go on a `Tensor`, including opening an image and resizing it since it takes `StringTensor`. That makes the `tfms` attribute of `ItemList` irrelevant, so `ItemList` is just an array of `Item` with a path (if get method seems useful later, we can add it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct ItemList<Item>{\n",
    "    public var items: [Item]\n",
    "    public let path: Path\n",
    "    \n",
    "    public init(items: [Item], path: Path){\n",
    "        (self.items,self.path) = (items,path)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension ItemList where Item == Path {\n",
    "    init(fromFolder path: Path, extensions: [String], recurse: Bool = true) {\n",
    "        self.init(items: fetchFiles(path: path, recurse: recurse, extensions: extensions),\n",
    "                  path:  path)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct SplitData<Item>{\n",
    "    public let train: ItemList<Item>\n",
    "    public let valid: ItemList<Item>\n",
    "    public var path: Path { return train.path }\n",
    "    \n",
    "    public init(train: ItemList<Item>, valid: ItemList<Item>){\n",
    "        (self.train, self.valid) = (train, valid)\n",
    "    }\n",
    "    \n",
    "    public init(_ il: ItemList<Item>, fromFunc: (Item) -> Bool){\n",
    "        self.init(train: ItemList(items: il.items.filter { !fromFunc($0) }, path: il.path),\n",
    "                  valid: ItemList(items: il.items.filter {  fromFunc($0) }, path: il.path))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func grandParentSplitter(fName: Path, valid: String = \"valid\") -> Bool{\n",
    "    return fName.parent.parent.basename() == valid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sd = SplitData(il) { grandParentSplitter(fName: $0, valid: \"val\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public protocol Processor {\n",
    "    associatedtype Input\n",
    "    associatedtype Output\n",
    "    \n",
    "    mutating func initState(items: [Input])\n",
    "    func process1(item: Input) -> Output\n",
    "    func deprocess1(item: Output) -> Input\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension Processor {\n",
    "    func process(items: [Input]) -> [Output] {\n",
    "        return items.map { process1(item: $0) }\n",
    "    }\n",
    "    \n",
    "    func deprocess(items: [Output]) -> [Input] {\n",
    "        return items.map { deprocess1(item: $0) }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct NoopProcessor<Item>: Processor {\n",
    "    public init() {}\n",
    "   \n",
    "    public mutating func initState(items: [Item]) {}\n",
    "    \n",
    "    public func process1  (item: Item) -> Item { return item }\n",
    "    public func deprocess1(item: Item) -> Item { return item }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct CategoryProcessor: Processor {\n",
    "    public init() {}\n",
    "    public var vocab: [String]? = nil\n",
    "    public var reverseMap: [String: Int32]? = nil\n",
    "    \n",
    "    public mutating func initState(items: [String]) {\n",
    "        vocab = Array(Set(items)).sorted()\n",
    "        reverseMap = [:]\n",
    "        for (i,x) in vocab!.enumerated() { reverseMap![x] = Int32(i) }\n",
    "    }\n",
    "    \n",
    "    public func process1  (item: String) -> Int32 { return reverseMap![item]! }\n",
    "    public func deprocess1(item: Int32)  -> String { return vocab![Int(item)] }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we build the datasets, we don't need to return a tupe (item, label) but to have the tensor(s) with the items and the tensor(s) with the labels separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct LabeledItemList<PI,PL> where PI: Processor, PL: Processor{\n",
    "    public var items: [PI.Output]\n",
    "    public var labels: [PL.Output]\n",
    "    public let path: Path\n",
    "    public var procItem: PI\n",
    "    public var procLabel: PL\n",
    "    \n",
    "    public init(rawItems: [PI.Input], rawLabels: [PL.Input], path: Path, procItem: PI, procLabel: PL){\n",
    "        (self.procItem,self.procLabel,self.path) = (procItem,procLabel,path)\n",
    "        self.items = procItem.process(items: rawItems)\n",
    "        self.labels = procLabel.process(items: rawLabels)\n",
    "    }\n",
    "    \n",
    "    public init(_ il: ItemList<PI.Input>, fromFunc: (PI.Input) -> PL.Input, procItem: PI, procLabel: PL){\n",
    "        self.init(rawItems:  il.items,\n",
    "                  rawLabels: il.items.map{ fromFunc($0)},\n",
    "                  path:      il.path,\n",
    "                  procItem:  procItem,\n",
    "                  procLabel: procLabel)\n",
    "    }\n",
    "    \n",
    "    public func rawItem (_ idx: Int) -> PI.Input { return procItem.deprocess1 (item: items[idx])  }\n",
    "    public func rawLabel(_ idx: Int) -> PL.Input { return procLabel.deprocess1(item: labels[idx]) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct SplitLabeledData<PI,PL> where PI: Processor, PL: Processor{\n",
    "    public let train: LabeledItemList<PI,PL>\n",
    "    public let valid: LabeledItemList<PI,PL>\n",
    "    public var path: Path { return train.path }\n",
    "    \n",
    "    public init(train: LabeledItemList<PI,PL>, valid: LabeledItemList<PI,PL>){\n",
    "        (self.train, self.valid) = (train, valid)\n",
    "    }\n",
    "    \n",
    "    public init(_ sd: SplitData<PI.Input>, fromFunc: (PI.Input) -> PL.Input, procItem: inout PI, procLabel: inout PL){\n",
    "        procItem.initState(items: sd.train.items)\n",
    "        let trainLabels = sd.train.items.map{ fromFunc($0) }\n",
    "        procLabel.initState(items: trainLabels)\n",
    "        self.init(train: LabeledItemList(rawItems: sd.train.items, rawLabels: trainLabels, path: sd.path, \n",
    "                                         procItem: procItem, procLabel: procLabel),\n",
    "                  valid: LabeledItemList(sd.valid, fromFunc: fromFunc, procItem: procItem, procLabel: procLabel))\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Make a labeled data without an input processor, by defaulting to a noop processor.\n",
    "public func makeLabeledData<T, PL: Processor>(_ sd: SplitData<T>, fromFunc: (T) -> PL.Input, procLabel: inout PL) \n",
    " -> SplitLabeledData<NoopProcessor<T>, PL> {\n",
    "    var pi = NoopProcessor<T>()\n",
    "    return SplitLabeledData(sd, fromFunc: fromFunc, procItem: &pi, procLabel: &procLabel)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func parentLabeler(_ fName: Path) -> String { return fName.parent.basename() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (procItem,procLabel) = (NoopProcessor<Path>(),CategoryProcessor())\n",
    "let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sld.train.labels[0])\n",
    "print(sld.train.rawLabel(0))\n",
    "print(sld.train.procLabel.vocab!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go in a Dataset, our array of items and array of labels need to be converted to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct LabeledElement<I: TensorGroup, L: TensorGroup>: TensorGroup {\n",
    "    public var xb: I\n",
    "    public var yb: L    \n",
    "    \n",
    "    public init(xb: I, yb: L) {\n",
    "        (self.xb, self.yb) = (xb, yb)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension SplitLabeledData {\n",
    "    func toDataBunch<XB, YB> (\n",
    "        itemToTensor: ([PI.Output]) -> XB, labelToTensor: ([PL.Output]) -> YB, bs: Int = 64\n",
    "    ) -> DataBunch<LabeledElement<XB, YB>> where XB: TensorGroup, YB: TensorGroup {\n",
    "        let trainDs = Dataset<LabeledElement<XB, YB>>(\n",
    "            elements: LabeledElement(xb: itemToTensor(train.items), yb: labelToTensor(train.labels)))\n",
    "        let validDs = Dataset<LabeledElement<XB, YB>>(\n",
    "            elements: LabeledElement(xb: itemToTensor(valid.items), yb: labelToTensor(valid.labels)))\n",
    "        return DataBunch(train: trainDs, valid: validDs, \n",
    "                         trainLen: train.items.count, validLen: valid.items.count,\n",
    "                         bs: bs)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func pathsToTensor(_ paths: [Path]) -> StringTensor { return StringTensor(paths.map{ $0.string })}\n",
    "public func intsToTensor(_ items: [Int32]) -> Tensor<Int32> { return Tensor<Int32>(items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataset = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We directly plug in to the dataset the transforms we want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func transformData<I,TI,L>(\n",
    "    _ data: DataBunch<LabeledElement<I,L>>, \n",
    "    nWorkers:Int=4,\n",
    "    tfmItem: (I) -> TI\n",
    ") -> DataBunch<DataBatch<TI,L>> \n",
    "where I: TensorGroup, TI: TensorGroup & Differentiable, L: TensorGroup{\n",
    "    return DataBunch(train: data.train.innerDs.map(parallelCallCount: nWorkers){ DataBatch(xb: tfmItem($0.xb), yb: $0.yb) },\n",
    "                     valid: data.valid.innerDs.map(parallelCallCount: nWorkers){ DataBatch(xb: tfmItem($0.xb), yb: $0.yb) },\n",
    "                     trainLen: data.train.dsCount, \n",
    "                     validLen: data.valid.dsCount,\n",
    "                     bs: data.train.bs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func openAndResize(fname: StringTensor, size: Int) -> TF{\n",
    "    let decodedImg = StringTensor(readFile: fname).decodeJpeg(channels: 3)\n",
    "    let resizedImg = Tensor<Float>(Raw.resizeBilinear(\n",
    "        images: Tensor<UInt8>([decodedImg]), \n",
    "        size: Tensor<Int32>([Int32(size), Int32(size)]))) / 255.0\n",
    "    return resizedImg.reshaped(to: TensorShape(size, size, 3))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let tfmData = transformData(dataset) { openAndResize(fname: $0, size: 128) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension FADataset {\n",
    "    func oneBatch() -> Element? {\n",
    "        for batch in ds { return batch }\n",
    "        return nil\n",
    "    }\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batch = tfmData.train.oneBatch()!\n",
    "batch.xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func showImages(_ xb: TF, labels: [String]? = nil) {\n",
    "    let (rows,cols) = (3,3)\n",
    "    plt.figure(figsize: [9, 9])\n",
    "    for i in 0..<(rows * cols) {\n",
    "        let img = plt.subplot(rows, cols, i + 1)\n",
    "        img.axis(\"off\")\n",
    "        let x = xb[i].makeNumpyArray()\n",
    "        img.imshow(x)\n",
    "        if labels != nil { img.set_title(labels![i]) }\n",
    "        if (i + 1) >= (rows * cols) { break }\n",
    "    }\n",
    "    plt.show()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let labels = batch.yb.scalars.map { sld.train.procLabel.vocab![Int($0)] }\n",
    "showImages(batch.xb, labels: labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summarize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])\n",
    "let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})\n",
    "var (procItem,procLabel) = (NoopProcessor<Path>(), CategoryProcessor())\n",
    "let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)\n",
    "var rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor, bs: 256)\n",
    "var data = transformData(rawData) { openAndResize(fname: $0, size: 224) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// tf.data reads the whole file into memory if we shuffle!\n",
    "data.train.shuffle = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time { let _ = data.train.oneBatch() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func allBatches() -> (Int,TF) {\n",
    "    var m = TF(zeros: [224, 224, 3])\n",
    "    var c: Int = 0\n",
    "    for batch in data.train.ds { \n",
    "        m += batch.xb.mean(squeezingAxes: 0) \n",
    "        c += 1\n",
    "    }\n",
    "    return (c,m)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time {let (c,m) = allBatches()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])\n",
    "let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})\n",
    "var (procItem,procLabel) = (NoopProcessor<Path>(), CategoryProcessor())\n",
    "let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)\n",
    "var rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)\n",
    "let data = transformData(rawData) { openAndResize(fname: $0, size: 128) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export \n",
    "public let imagenetStats = (mean: TF([0.485, 0.456, 0.406]), std: TF([0.229, 0.224, 0.225]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func prevPow2(_ x: Int) -> Int { \n",
    "    var res = 1\n",
    "    while res <= x { res *= 2 }\n",
    "    return res / 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO(TF-619): Remove this, and use FABatchNorm/ConvBN from 07_batchnorm instead.\n",
    "\n",
    "public struct ThisModuleFABatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative var momentum, epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative let runningMean, runningVariance: Reference<Tensor<Scalar>>\n",
    "    // Trainable parameters\n",
    "    public var scale, offset: Tensor<Scalar>\n",
    "    \n",
    "    public init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [featureCount])\n",
    "        self.offset = Tensor(zeros: [featureCount])\n",
    "        self.runningMean = Reference(Tensor(0))\n",
    "        self.runningVariance = Reference(Tensor(1))\n",
    "    }\n",
    "    \n",
    "    public init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func forwardTraining(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = input.mean(alongAxes: [0, 1, 2])\n",
    "        let variance = input.variance(alongAxes: [0, 1, 2])\n",
    "        runningMean.value += (mean - runningMean.value) * (1 - momentum)\n",
    "        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func forwardInference(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = runningMean.value\n",
    "        let variance = runningVariance.value\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}\n",
    "\n",
    "public struct ThisModuleConvBN<Scalar: TensorFlowFloatingPoint>: FALayer {\n",
    "    public typealias Input = Tensor<Scalar>\n",
    "    public typealias Output = Tensor<Scalar>\n",
    "    public var conv: FANoBiasConv2D<Scalar>\n",
    "    public var norm: ThisModuleFABatchNorm<Scalar>\n",
    "    \n",
    "    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1){\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)\n",
    "        self.norm = ThisModuleFABatchNorm(featureCount: cOut, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return norm.forward(conv.forward(input))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct CNNModel: Layer {\n",
    "    public var convs: [ThisModuleConvBN<Float>]\n",
    "    public var pool = FAGlobalAvgPool2D<Float>()\n",
    "    public var linear: FADense<Float>\n",
    "    \n",
    "    public init(channelIn: Int, nOut: Int, filters: [Int]){\n",
    "        convs = []\n",
    "        let (l1,l2) = (channelIn, prevPow2(channelIn * 9))\n",
    "        convs = [ThisModuleConvBN(l1,   l2,   stride: 1),\n",
    "                 ThisModuleConvBN(l2,   l2*2, stride: 2),\n",
    "                 ThisModuleConvBN(l2*2, l2*4, stride: 2)]\n",
    "        let allFilters = [l2*4] + filters\n",
    "        for i in 0..<filters.count { convs.append(ThisModuleConvBN(allFilters[i], allFilters[i+1], stride: 2)) }\n",
    "        linear = FADense<Float>(filters.last!, nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func callAsFunction(_ input: TF) -> TF {\n",
    "        // TODO: Work around https://bugs.swift.org/browse/TF-606\n",
    "        return linear.forward(pool.forward(convs(input)))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CNNModel) -> SGD<CNNModel> { return SGD(for: model, learningRate: 0.1) }\n",
    "func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NotebookExport\n",
    "let exporter = NotebookExport(Path.cwd/\"08_data_block.ipynb\")\n",
    "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
