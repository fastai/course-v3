{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_08a_heterogeneous_dictionary\")' FastaiNotebook_08a_heterogeneous_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_08a_heterogeneous_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let path = downloadImagenette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])\n",
    "let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})\n",
    "var procLabel = CategoryProcessor()\n",
    "let sld = makeLabeledData(sd, fromFunc: parentLabeler, procLabel: &procLabel)\n",
    "let rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)\n",
    "let data = transformData(rawData, tfmItem: { openAndResize(fname: $0, size: 128) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO(TF-619): Remove this, and use CNNModel from 08 instead.\n",
    "\n",
    "public struct ThisModuleFABatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative var momentum, epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative let runningMean, runningVariance: Reference<Tensor<Scalar>>\n",
    "    // Trainable parameters\n",
    "    public var scale, offset: Tensor<Scalar>\n",
    "    \n",
    "    public init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [featureCount])\n",
    "        self.offset = Tensor(zeros: [featureCount])\n",
    "        self.runningMean = Reference(Tensor(0))\n",
    "        self.runningVariance = Reference(Tensor(1))\n",
    "    }\n",
    "    \n",
    "    public init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func forwardTraining(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = input.mean(alongAxes: [0, 1, 2])\n",
    "        let variance = input.variance(alongAxes: [0, 1, 2])\n",
    "        runningMean.value += (mean - runningMean.value) * (1 - momentum)\n",
    "        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func forwardInference(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = runningMean.value\n",
    "        let variance = runningVariance.value\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}\n",
    "\n",
    "public struct ThisModuleConvBN<Scalar: TensorFlowFloatingPoint>: FALayer {\n",
    "    public typealias Input = Tensor<Scalar>\n",
    "    public typealias Output = Tensor<Scalar>\n",
    "    public var conv: FANoBiasConv2D<Scalar>\n",
    "    public var norm: ThisModuleFABatchNorm<Scalar>\n",
    "    \n",
    "    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1){\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)\n",
    "        self.norm = ThisModuleFABatchNorm(featureCount: cOut, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return norm.forward(conv.forward(input))\n",
    "    }\n",
    "}\n",
    "\n",
    "public struct CNNModel: Layer {\n",
    "    public var convs: [ThisModuleConvBN<Float>]\n",
    "    public var pool = FAGlobalAvgPool2D<Float>()\n",
    "    public var linear: FADense<Float>\n",
    "    \n",
    "    public init(channelIn: Int, nOut: Int, filters: [Int]){\n",
    "        convs = []\n",
    "        let (l1,l2) = (channelIn, prevPow2(channelIn * 9))\n",
    "        convs = [ThisModuleConvBN(l1,   l2,   stride: 1),\n",
    "                 ThisModuleConvBN(l2,   l2*2, stride: 2),\n",
    "                 ThisModuleConvBN(l2*2, l2*4, stride: 2)]\n",
    "        let allFilters = [l2*4] + filters\n",
    "        for i in 0..<filters.count { convs.append(ThisModuleConvBN(allFilters[i], allFilters[i+1], stride: 2)) }\n",
    "        linear = FADense<Float>(filters.last!, nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func callAsFunction(_ input: TF) -> TF {\n",
    "        // TODO: Work around https://bugs.swift.org/browse/TF-606\n",
    "        return linear.forward(pool.forward(convs(input)))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we create this structure to contain the names of our hyper-parameters. This will give us some tab completion and typo-proof way of handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct HyperParams {\n",
    "    public static let lr = \"learningRate\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the python version, we create `statDelegates` that will be responsible for computing/updating statistics in the state (like the moving average of gradients) and `stepDelegates` that will be responsible for performing a part of the update of the weights. \n",
    "\n",
    "In PyTorch we created a basic class with functions that needed to be implemented. In swift this is what protocols are for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public protocol StatDelegate {\n",
    "    var name: String {get}\n",
    "    var defaultHPs: [String:Float] {get}\n",
    "    \n",
    "    func update(_ state: inout [String:TF], p: TF, ùõÅp: TF, hps: inout [String:Float])\n",
    "}\n",
    "\n",
    "public protocol StepDelegate {\n",
    "    var defaultHPs: [String:Float] {get}\n",
    "    \n",
    "    func update(_ p: inout TF, ùõÅp: inout TF, state: [String:TF], hps: inout [String:Float])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are helper functions to merge dictionaries that we'll use in the `StatefulOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func mergeDicts(_ dicts: inout [[String:Float]], with newDict: [String:Float]) {\n",
    "    for i in dicts.indices { \n",
    "        dicts[i].merge(newDict) { (_, new) in new } \n",
    "    }\n",
    "}\n",
    "\n",
    "public func mergeDicts(_ dicts: inout [[String:Float]], with newDicts: [[String:Float]]) {\n",
    "    for i in dicts.indices { \n",
    "        dicts[i].merge(newDicts[i]) { (_, new) in new } \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those two extensions are there to initialize dicts easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "extension Dictionary where Value == Int{\n",
    "    public init(mapFromArrays arrays: [[Key]]){\n",
    "        self.init(uniqueKeysWithValues: arrays.enumerated().flatMap { i, arr in arr.map { ($0, i) } })\n",
    "    }\n",
    "}\n",
    "\n",
    "extension Dictionary {\n",
    "    public init(constant: Value, keys: [Key]){\n",
    "        self.init(uniqueKeysWithValues: keys.map { ($0, constant) })\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the initial state of our StatefulOptimizer. It's a dictionary keyPath (see below) to dictionary that maps names to tensor of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func initState<Model: Layer>(for model: Model, names: [String]) \n",
    "-> [WritableKeyPath<Model.AllDifferentiableVariables, TF>: [String:TF]] {\n",
    "    return [WritableKeyPath<Model.AllDifferentiableVariables, TF>: [String:TF]](\n",
    "        constant: [String: TF](constant: TF(0), keys: names),\n",
    "        keys: model.variables.keyPaths)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can define the main `StatefulOptimizer`. It takes a model, hyperparameters for each parameter group, some `steppers` and `stats` and a `splitArray` that defines our different parameter groups.\n",
    "\n",
    "To understand how this work, you need to know a little bit about keyPaths. This is a tool in swift to access any elements in a nested structure like our models: one model typically has a few attributes that are modules which in turn contain other modules and so forth until we reach the primitives layers like `Conv2d` or `Dense`. KeyPaths will allow us to index in that nested structure the objects of a particular type. \n",
    "\n",
    "For instance, the shortcut `keyPaths` you can apply to any `Layer` will find all the tensors of floats. If we apply it to a `Model.AllDifferentiableVariables` object, we will find all the parameters of the model (since `model.allDifferentiableVariables` only contain the trainable parameters).\n",
    "\n",
    "That's why the inner loop of our `StatefulOptimizer` is over `variables.keyPaths`. The same keyPath index will give us the gradients. Then we create a `state` to be a dictionary of such keyPaths to `[String:TF]` and the `splitArray` we provide is an array of different keyPaths (each giving us a parameter group) from which we build a `splitDict` that maps our keyPaths to the index of the corresponding group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class StatefulOptimizer<Model: Layer>\n",
    "    where Model.AllDifferentiableVariables == Model.TangentVector {\n",
    "    public typealias ModelKeyPath = WritableKeyPath<Model.AllDifferentiableVariables, TF>\n",
    "    public typealias SplitDict = [ModelKeyPath: Int]\n",
    "    public var hpGroups: [[String:Float]]\n",
    "    public var splitDict: SplitDict\n",
    "    public var states: [ModelKeyPath: [String: TF]]\n",
    "    public var stats: [StatDelegate]\n",
    "    public var steppers: [StepDelegate]\n",
    "    public init(        \n",
    "        for model: __shared Model,\n",
    "        steppers: [StepDelegate],\n",
    "        stats: [StatDelegate],\n",
    "        hpGroups: [[String:Float]],\n",
    "        splitArray: [[ModelKeyPath]]\n",
    "    ) {\n",
    "        self.hpGroups = Array(repeating: [:], count: hpGroups.count)\n",
    "        (self.steppers,self.stats) = (steppers,stats)\n",
    "        self.splitDict = SplitDict(mapFromArrays: splitArray)\n",
    "        states = [:]\n",
    "        steppers.forEach { mergeDicts(&self.hpGroups, with: $0.defaultHPs) }\n",
    "        stats.forEach    { mergeDicts(&self.hpGroups, with: $0.defaultHPs) }\n",
    "        states = initState(for: model, names: stats.map { $0.name })\n",
    "        mergeDicts(&self.hpGroups, with: hpGroups)\n",
    "    }\n",
    "        \n",
    "    public func update(\n",
    "        _ variables: inout Model.AllDifferentiableVariables,\n",
    "        along direction: Model.TangentVector\n",
    "    ) {\n",
    "        for kp in variables.keyPaths {\n",
    "            var ùõÅp = direction[keyPath: kp]\n",
    "            var hps = hpGroups[splitDict[kp]!]\n",
    "            stats.forEach() { $0.update(&states[kp]!, p: variables[keyPath: kp], ùõÅp: ùõÅp, hps: &hps) }\n",
    "            steppers.forEach() { $0.update(&variables[keyPath: kp], ùõÅp: &ùõÅp, state: states[kp]!, hps: &hps) }\n",
    "            hpGroups[splitDict[kp]!] = hps\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make `StatefulOptimizer` conform to the `Optimizer` protocol, we need to add a `learningRate` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "extension StatefulOptimizer: Optimizer{\n",
    "    public var learningRate: Float {\n",
    "        get { return hpGroups.last![HyperParams.lr]! } \n",
    "        set { \n",
    "            for i in hpGroups.indices {self.hpGroups[i][HyperParams.lr] = newValue }\n",
    "        }\n",
    "    }\n",
    "    //For discriminative learning rates\n",
    "    public var learningRates: [Float] {\n",
    "        get { return hpGroups.map { $0[HyperParams.lr]! } }\n",
    "        set { \n",
    "            for i in hpGroups.indices {self.hpGroups[i][HyperParams.lr] = newValue[i] } \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we don't have any parameter groups, we just use one with all the `keyPaths`. This convenience init automatically does that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "extension StatefulOptimizer{\n",
    "    public convenience init (for model: __shared Model,\n",
    "                             steppers: [StepDelegate],\n",
    "                             stats: [StatDelegate],\n",
    "                             hps: [String:Float]) {\n",
    "        self.init(for: model,\n",
    "                  steppers: steppers,\n",
    "                  stats: stats,\n",
    "                  hpGroups: [hps],\n",
    "                  splitArray: [model.variables.keyPaths])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define `steppers` and `stats`. Let's begin with basic SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct SGDStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.lr: 3e-3] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, ùõÅp: inout TF, state: [String:TF], hps: inout [String:Float]) {\n",
    "        p -= ùõÅp * hps[HyperParams.lr]!\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check all is working and train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var hps: [String:Float] = [HyperParams.lr: 0.01]\n",
    "func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {\n",
    "    return StatefulOptimizer(for: model, steppers: [SGDStep()], stats: [], hps: hps)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "var recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can add weight decay and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension HyperParams {\n",
    "    static let wd = \"weightDecay\"\n",
    "}\n",
    "\n",
    "public struct WeightDecay: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.wd: 0] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, ùõÅp: inout TF, state: [String:TF], hps: inout [String:Float]) {\n",
    "        p *= 1 - hps[HyperParams.lr]! * hps[HyperParams.wd]!\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct L2Regularization: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.wd: 0] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, ùõÅp: inout TF, state: [String:TF], hps: inout [String:Float]) {\n",
    "        ùõÅp += hps[HyperParams.wd]! * p\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is SGD with momentum. For this we need a statistic that keeps track of the moving average of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "//Expandable enum to have tab completes/typo-proof for state variable names.\n",
    "public struct StateKeys {\n",
    "    public static let avgGrad = \"averageGrad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension HyperParams {\n",
    "    static let mom = \"momentum\"\n",
    "    static let momDamp = \"dampening\"\n",
    "}\n",
    "\n",
    "public struct AverageGrad: StatDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.mom: 0.9] }\n",
    "    public let dampened: Bool\n",
    "    public init(dampened: Bool = false) { self.dampened = dampened }\n",
    "    public var name: String { return StateKeys.avgGrad }\n",
    "    public func update(_ state: inout [String: TF], p: TF, ùõÅp: TF, hps: inout [String:Float]) {\n",
    "        state[StateKeys.avgGrad]! *= hps[HyperParams.mom]!\n",
    "        hps[HyperParams.momDamp] = 1.0 - (dampened ? hps[HyperParams.mom]! : 0.0)\n",
    "        state[StateKeys.avgGrad]! += hps[HyperParams.momDamp]! * ùõÅp\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct MomentumStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] = [:]\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, ùõÅp: inout TF, state: [String: TF], hps: inout [String:Float]) {\n",
    "        p -= state[StateKeys.avgGrad]! * hps[HyperParams.lr]!\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check it trains properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let hps: [String:Float] = [HyperParams.lr: 0.01]\n",
    "func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {\n",
    "    return StatefulOptimizer(for: model, steppers: [MomentumStep()], stats: [AverageGrad()], hps: hps)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "var recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameters have taken the default values provided (except for learning rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.opt.hpGroups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is Adam. For that we need to keep track of the averages of the gradients squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension HyperParams {\n",
    "    static let ¬≤mom = \"momentumSquares\"\n",
    "    static let ¬≤momDamp = \"dampeningSquares\"\n",
    "}\n",
    "\n",
    "public extension StateKeys {\n",
    "    static let avgSqr = \"averageSquaredGrad\"\n",
    "}\n",
    "\n",
    "public struct AverageSquaredGrad: StatDelegate {\n",
    "    let dampened: Bool\n",
    "    public init(dampened: Bool = true) { self.dampened = dampened }\n",
    "    public var name: String { return StateKeys.avgSqr }\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.¬≤mom: 0.99] }\n",
    "    public func update(_ state: inout [String: TF], p: TF, ùõÅp: TF, hps: inout [String:Float]) {\n",
    "        state[StateKeys.avgSqr]! *= hps[HyperParams.¬≤mom]!\n",
    "        hps[HyperParams.¬≤momDamp] = 1.0 - (dampened ? hps[HyperParams.¬≤mom]! : 0.0)\n",
    "        state[StateKeys.avgSqr]! += hps[HyperParams.¬≤momDamp]! * ùõÅp.squared()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also need to keep track of the number of iterations we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension StateKeys {\n",
    "    static let step = \"stepCount\"\n",
    "}\n",
    "\n",
    "public struct StepCount: StatDelegate {\n",
    "    public var name: String { return StateKeys.step }\n",
    "    public var defaultHPs: [String:Float] = [:]\n",
    "    public init() {}\n",
    "    public func update(_ state: inout [String: TF], p: TF, ùõÅp: TF, hps: inout [String:Float]) {\n",
    "        state[StateKeys.step]! += 1.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "//public struct Epsilon: HetDictKey { public static var defaultValue: Float = 1e-5 }\n",
    "public extension HyperParams {\n",
    "    static let eps = \"epsilon\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct AdamStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.eps: 1e-5] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, ùõÅp: inout TF, state: [String: TF], hps: inout [String:Float]) {\n",
    "        let step = state[StateKeys.step]!\n",
    "        let (mom,damp) = (hps[HyperParams.mom]!,hps[HyperParams.momDamp]!)\n",
    "        let debias1 = damp * (1 - pow(mom, step)) / (1 - mom)\n",
    "        let num = debias1 * state[StateKeys.avgGrad]!\n",
    "        \n",
    "        let (¬≤mom,¬≤damp) = (hps[HyperParams.¬≤mom]!,hps[HyperParams.¬≤momDamp]!)\n",
    "        let debias2 = ¬≤damp * (1 - pow(¬≤mom, step)) / (1 - ¬≤mom)\n",
    "        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + hps[HyperParams.eps]!\n",
    "        \n",
    "        p -= hps[HyperParams.lr]! * num / denom\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's check it's all training properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {\n",
    "    return StatefulOptimizer(\n",
    "        for: model,\n",
    "        steppers: [AdamStep()], \n",
    "        stats: [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()], \n",
    "        hps: [HyperParams.lr: 1e-3])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the values of the hyper-parameters have been set properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.opt.hpGroups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Lamb optimizer is as easy as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct LambStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.eps: 1e-6, HyperParams.wd: 0.0] }\n",
    "    public func update(_ p: inout TF, ùõÅp: inout TF, state: [String: TF], hps: inout [String:Float]) {\n",
    "        let stepCount = state[StateKeys.step]!\n",
    "        let (mom,damp) = (hps[HyperParams.mom]!,hps[HyperParams.momDamp]!)\n",
    "        let debias1 = damp * (1 - pow(mom, stepCount)) / (1 - mom)\n",
    "        let num = debias1 * state[StateKeys.avgGrad]!\n",
    "        \n",
    "        let (¬≤mom,¬≤damp) = (hps[HyperParams.¬≤mom]!,hps[HyperParams.¬≤momDamp]!)\n",
    "        let debias2 = ¬≤damp * (1 - pow(¬≤mom, stepCount)) / (1 - ¬≤mom)\n",
    "        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + hps[HyperParams.eps]!\n",
    "        \n",
    "        let step = num / denom + hps[HyperParams.wd]! * p\n",
    "        let r1 = sqrt((p * p).mean())\n",
    "        let r2 = sqrt((step * step).mean())\n",
    "        let factor = min(r1 / r2, Float(10.0))\n",
    "        p -= hps[HyperParams.lr]! * factor * step\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making convenience functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily create our optimizers, we have two convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func sgdOpt<Model>(lr: Float, mom: Float = 0.9, wd: Float = 0.0, dampening: Bool = false\n",
    "                         ) -> ((Model) -> StatefulOptimizer<Model>) {\n",
    "    var steppers: [StepDelegate] = (mom != 0) ? [MomentumStep()] : [SGDStep()]\n",
    "    if wd != 0 { steppers.append(WeightDecay()) }\n",
    "    let stats = (mom != 0) ? [AverageGrad(dampened: dampening)] : []\n",
    "    var hps: [String: Float] = [HyperParams.lr: lr]\n",
    "    if mom != 0 { hps[HyperParams.mom] = mom }\n",
    "    if wd != 0  { hps[HyperParams.wd ] = wd  }\n",
    "    return {model in \n",
    "        return StatefulOptimizer(for: model, steppers: steppers, stats: stats, hps: hps)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func adamOpt<Model>(lr: Float, mom: Float = 0.9, beta: Float=0.99, wd: Float = 0.0, eps: Float = 1e-5\n",
    "                         ) -> ((Model) -> StatefulOptimizer<Model>) {\n",
    "    var steppers: [StepDelegate] = [AdamStep()]\n",
    "    if wd != 0 { steppers.append(WeightDecay()) }\n",
    "    let stats: [StatDelegate] = [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()]\n",
    "    var hps: [String: Float] = [HyperParams.lr: lr]\n",
    "    hps[HyperParams.mom] = mom\n",
    "    hps[HyperParams.¬≤mom] = beta\n",
    "    hps[HyperParams.eps] = eps\n",
    "    if wd != 0  { hps[HyperParams.wd ] = wd  }\n",
    "    return {model in \n",
    "        return StatefulOptimizer(for: model, steppers: steppers, stats: stats, hps: hps)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule the hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is that we need to schedule our hyper-parameters. The following function allows us to schedule any of them, as long as they are present in the `hpGroups` dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension StatefulOptimizer {\n",
    "    func setParam(_ hp: String, _ val: Float) {\n",
    "        for i in 0..<hpGroups.count { hpGroups[i][hp] = val }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner where Opt.Scalar: BinaryFloatingPoint, \n",
    "    Opt.Model.AllDifferentiableVariables == Opt.Model.TangentVector{\n",
    "    public class ParamScheduler: Delegate {\n",
    "        public override var order: Int { return 1 }\n",
    "        public typealias ScheduleFunc = (Float) -> Float\n",
    "\n",
    "        // A learning rate schedule from step to float.\n",
    "        public var scheduler: ScheduleFunc\n",
    "        public let hp: String\n",
    "        \n",
    "        public init(scheduler: @escaping (Float) -> Float, hp: String) {\n",
    "            (self.scheduler,self.hp) = (scheduler,hp)\n",
    "        }\n",
    "        \n",
    "        override public func batchWillStart(learner: Learner) {\n",
    "            let val = scheduler(learner.pctEpochs/Float(learner.epochCount))\n",
    "            (learner.opt as! StatefulOptimizer<Opt.Model>).setParam(hp, val)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public func makeParamScheduler(_ scheduler: @escaping (Float) -> Float, hp: String) -> ParamScheduler {\n",
    "        return ParamScheduler(scheduler: scheduler, hp: hp)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a helper function to schedule a 1cycle policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export \n",
    "public func oneCycleSchedulers(_ lrMax: Float, pctStart:Float=0.25, divStart: Float = 10, divEnd: Float = 1e5, \n",
    "                               moms: (Float,Float,Float) = (0.95,0.85,0.95)) \n",
    "-> ((Float) -> Float, (Float) -> Float){\n",
    "    let lrSched = combineSchedules(\n",
    "        pcts: [pctStart, 1-pctStart], \n",
    "        schedules: [makeAnnealer(start: lrMax/divStart, end: lrMax, schedule: cosineSchedule),\n",
    "                    makeAnnealer(start: lrMax, end: lrMax/divEnd, schedule: cosineSchedule)])\n",
    "    let momSched = combineSchedules(\n",
    "        pcts: [pctStart, 1-pctStart], \n",
    "        schedules: [makeAnnealer(start: moms.0, end: moms.1, schedule: cosineSchedule),\n",
    "                    makeAnnealer(start: moms.1, end: moms.2, schedule: cosineSchedule)])\n",
    "    return (lrSched, momSched)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner where Opt.Scalar: BinaryFloatingPoint, \n",
    "    Opt.Model.AllDifferentiableVariables == Opt.Model.TangentVector{\n",
    "\n",
    "    public func addOneCycleDelegates(_ lrMax: Float, pctStart:Float=0.25, divStart: Float = 10, divEnd: Float = 1e5, \n",
    "                               moms: (Float,Float,Float) = (0.95,0.85,0.95)) {\n",
    "        let scheds = oneCycleSchedulers(lrMax, pctStart: pctStart, divStart: divStart, divEnd: divEnd, moms: moms)\n",
    "        addDelegates([makeParamScheduler(scheds.0 , hp: HyperParams.lr), \n",
    "                      makeParamScheduler(scheds.1 , hp: HyperParams.mom)])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check it's all training properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optFunc: (CNNModel) -> StatefulOptimizer<CNNModel> = adamOpt(lr: 1e-3, mom: 0.9, beta: 0.99, wd: 1e-2, eps: 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.addOneCycleDelegates(1e-3)\n",
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.plotLRs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train at differential learning rates (or freeze part of the models) we need to pass to our optimizer arrays of KeyPaths (which will define our layer groups). For instance, we can begin with the firt 9 keyPaths (which corresponds to the first three ConvBNs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = modelInit()\n",
    "let splitArray = [Array(model.variables.keyPaths[0..<9]), Array(model.variables.keyPaths[9...])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let hpGroups: [[String: Float]] = [[HyperParams.lr: 0], [HyperParams.lr: 0.1]]\n",
    "func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {\n",
    "    return StatefulOptimizer(for: model, steppers: [SGDStep()], stats: [], hpGroups: hpGroups, splitArray: splitArray)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First parameter shouldn't change since the corresponding layer group as a LR of 0., second should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.convs[0].norm.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.convs[3].norm.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.convs[0].norm.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.convs[3].norm.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get those keyPaths is to use the keyPaths to certain layers then append the keyPaths of all the parameters inside. This function takes a model, a layer and the keyPath that points from `model.variables` to `layer.variables` and returns the keyPaths of all the parameters of that layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public func parameterKeyPaths<M1, M2>(\n",
    "    _ model: M1,\n",
    "    _ kp: WritableKeyPath<M1.AllDifferentiableVariables, M2.AllDifferentiableVariables>,\n",
    "    _ layer: M2) -> [WritableKeyPath<M1.AllDifferentiableVariables, TF>]\n",
    "where M1: Layer, M2: Layer {\n",
    "    return model.variables[keyPath: kp].keyPaths.map { kp.appending(path: $0) }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a keyPath directly, we use \\ commands. Here is the keyPath to the array of convs, which lets us easily get the keyPaths for all the body of our CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let kp = \\(CNNModel.AllDifferentiableVariables).convs\n",
    "let conv = model.convs\n",
    "let bodyKeyPaths = parameterKeyPaths(model, kp, conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could split body and head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let splitArray = [bodyKeyPaths, model.variables.keyPaths.filter { return !bodyKeyPaths.contains($0) }]\n",
    "splitArray.map { $0.count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to refine this a bit and split our body between the first 4 convs and the last 3 we can proceed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let x = [1,2,3]\n",
    "let y = [4,5,6]\n",
    "zip(x,y).map { print($0, $1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let deepBody = (0..<4).map { parameterKeyPaths(\n",
    "    model, \n",
    "    \\(CNNModel.AllDifferentiableVariables).convs.base[$0], \n",
    "    model.convs[$0]\n",
    ") }.reduce([], +)\n",
    "\n",
    "let upperBody = (4..<7).map { parameterKeyPaths(\n",
    "    model, \n",
    "    \\(CNNModel.AllDifferentiableVariables).convs.base[$0], \n",
    "    model.convs[$0]\n",
    ") }.reduce([], +)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let splitArray = [deepBody, upperBody, model.variables.keyPaths.filter { return !bodyKeyPaths.contains($0) }]\n",
    "splitArray.map { $0.count }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we want a parameter group will all the batchnorm layers. KeyPaths allow us to get all the batchnorm layers by saying `(to: FABatchNorm<Float>.self)`. The `.keyPaths` method we have been using is jsut a shortcut for `recursivelyAllWritableKeyPaths(to: TF.self)`, which grabs all the keypaths to all the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let bns = model.recursivelyAllWritableKeyPaths(to: FABatchNorm<Float>.self).map { model[keyPath: $0] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need the keypaths from model.variables to those batchnorms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let bnKeyPaths = model.variables.recursivelyAllWritableKeyPaths(to: FABatchNorm<Float>.AllDifferentiableVariables.self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let bnParameters = zip(bnKeyPaths, bns).map { parameterKeyPaths(model, $0, $1) }.reduce([], +)\n",
    "bnParameters.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NotebookExport\n",
    "let exporter = NotebookExport(Path.cwd/\"09_optimizer.ipynb\")\n",
    "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
