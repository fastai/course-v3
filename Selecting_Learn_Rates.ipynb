{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Selecting Learn Rates.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jav0927/course-v3/blob/master/Selecting_Learn_Rates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGTcRsGQboTH",
        "colab_type": "text"
      },
      "source": [
        "## Selecting Learn Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jEtNc7jbkne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_unfreeze(learner):\n",
        "    '''\n",
        "    Determines whether the next-to-last layer in the model is set to unfreeze or freeze\n",
        "    '''\n",
        "    c = 0\n",
        "    for each in list(learner.model.body[-1][0].parameters()):\n",
        "      if each.requires_grad: c += 1   \n",
        "    if c == len(list(learner.model.body[-1][0].parameters())):\n",
        "      return True \n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "  \n",
        "  \n",
        "def find_optimal_lr(learner, noise=5, show_df=None, show_min_values=False):\n",
        "    '''\n",
        "    Parameters\n",
        "    \n",
        "      learner:  The learner (mandatory)\n",
        "      \n",
        "      (Optional)\n",
        "      noise:   Filtering parameter, set to 5. Suggest no to modify this value\n",
        "      \n",
        "      show_df: 'head' - Show the top 50 rows, \n",
        "               'tail' - Show the tail 50 rows\n",
        "    \n",
        "      show_min_values: True  - Display all values, min, and max \n",
        "                       False - Display min_loss and max_grad values\n",
        "    \n",
        "    Returns:\n",
        "      optimun_lr - if freeze = True\n",
        "      Suggested Best Slice - if freeze = False\n",
        "      \n",
        "    Author:  J. Adolfo Villalobos @ 2019  \n",
        "    '''\n",
        "    \n",
        "    # Get loss values, corresponding gradients, and lr values from model.recorder\n",
        "    loss = np.array(learner.recorder.losses)\n",
        "    loss_grad = np.gradient(loss)   \n",
        "    # Transform lrs list to np array\n",
        "    lrs = np.array(learner.recorder.lrs, dtype='float32')\n",
        "    \n",
        "    # Create a DataFrame with the data\n",
        "    data = {'loss': loss.T, 'loss_grad': loss_grad.T, 'lrs': lrs.T}\n",
        "    df = pd.DataFrame(data, columns=['loss', 'loss_grad', 'lrs', 'min_loss', 'max_loss', 'min_grad', 'max_grad'])\n",
        "      \n",
        "    # Populate \"min\" and \"max\" columns for loss and gradient values filtering the noise with argrelextrema.     \n",
        "    from scipy.signal import argrelextrema\n",
        "    \n",
        "    #********\n",
        "    # IMPORTANT: n filters noise (sharp spikes in the data). Higher n value filters noise more aggressively. \n",
        "    # n = 5 seems to work best\n",
        "    n=noise    \n",
        "    #********\n",
        "    \n",
        "    df.min_loss = df.iloc[argrelextrema(df.loss.values, np.less_equal, order=n)[0]]['loss']\n",
        "    df.max_loss = df.iloc[argrelextrema(df.loss.values, np.greater_equal, order=n)[0]]['loss']\n",
        "    df.min_grad = df.iloc[argrelextrema(df.loss_grad.values, np.less_equal, order=n)[0]]['loss_grad']\n",
        "    df.max_grad = df.iloc[argrelextrema(df.loss_grad.values, np.greater_equal, order=n)[0]]['loss_grad']\n",
        "\n",
        "    # Optional: Display dataframe if show_df=True\n",
        "    if show_df == 'head': print(df.head(50)) \n",
        "    elif show_df == 'tail': print(df.tail(50))     \n",
        "        \n",
        "    # Plot losses and loss gradients against lr values\n",
        "    plt.figure(figsize=[8, 5])\n",
        "    #figs, ax = plt.subplots(1,1)\n",
        "    ax = plt.gca()\n",
        "    color_loss = 'blue'\n",
        "    color_grad = 'orange'\n",
        "    color_green = 'green'\n",
        "    color_red = 'red'\n",
        "\n",
        "    ax.xaxis.grid(True)\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Learn Rate Finder')\n",
        "    ax.tick_params(axis='y', labelcolor=color_loss)\n",
        "    ax.semilogx(df.lrs, df.loss, c=color_loss )\n",
        "    \n",
        "    # Define variable vertical size of the plot window, depending on the graph shape\n",
        "    u_limit = max(df.loss.loc[(df.lrs < 0.1)].max(), 250)*2    \n",
        "    ax.set_ylim([-200, u_limit])\n",
        "   \n",
        "    # Plot resulting line graphs\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.set_ylabel('loss', color= color_grad)\n",
        "    ax2.semilogx(df.lrs, df.loss_grad, c = color_grad)\n",
        "    ax2.tick_params(axis='y', labelcolor = color_grad)\n",
        "    \n",
        "    # plot inflection points\n",
        "    ax.scatter(df.lrs, df.min_loss, c = color_red)    \n",
        "    ax2.scatter(df.lrs, df.min_grad, c = color_red)    \n",
        "    if show_min_values:\n",
        "      ax.scatter(df.lrs, df.max_loss, c = color_green)\n",
        "      ax2.scatter(df.lrs, df.max_grad, c = color_green) \n",
        "    \n",
        "    # Legends\n",
        "    plt.LogFormatter(labelOnlyBase=False)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=3, fancybox=True, shadow=True)\n",
        "    ax2.legend(loc='upper center', bbox_to_anchor=(0.5, 0.9), ncol=3, fancybox=True, shadow=True)\n",
        "    plt.show()\n",
        "    \n",
        "    # Display resulting lr values, format varies depending of the state of the model's \n",
        "    # next-to-last layer ggroup: set to freeze or unfreeze    \n",
        "    if is_unfreeze(learn):\n",
        "      # Yellow min_grad graph\n",
        "      rev_tru_idx = df.min_grad.notna()[::-1]   \n",
        "      optimun_lr_upper_bound_g = df.lrs.iloc[rev_tru_idx.idxmax()] \n",
        "      rev_tru_idx[rev_tru_idx.idxmax()] = np.NaN      \n",
        "      optimun_lr_lower_bound_1_g = df.lrs.iloc[rev_tru_idx.idxmax()]\n",
        "      rev_tru_idx[rev_tru_idx.idxmax()] = np.NaN      \n",
        "      optimun_lr_lower_bound_2_g = df.lrs.iloc[rev_tru_idx.idxmax()] \n",
        "      \n",
        "      # Blue loss graph\n",
        "      rev_tru_idx_loss = df.min_loss.notna()[::-1]   \n",
        "      optimun_lr_upper_bound_l = df.lrs.iloc[rev_tru_idx_loss.idxmax()] \n",
        "      rev_tru_idx_loss[rev_tru_idx_loss.idxmax()] = np.NaN      \n",
        "      optimun_lr_lower_bound_1_l = df.lrs.iloc[rev_tru_idx_loss.idxmax()]\n",
        "      rev_tru_idx_loss[rev_tru_idx_loss.idxmax()] = np.NaN      \n",
        "      optimun_lr_lower_bound_2_l = df.lrs.iloc[rev_tru_idx_loss.idxmax()] \n",
        "      \n",
        "      # Print results and return choices of lr slice\n",
        "      print('Model set to: \"unfreeze\" or \"freeze_to:\"')      \n",
        "      data = {'*Gradient - Orange Graph*' : [optimun_lr_upper_bound_g, optimun_lr_lower_bound_1_g, optimun_lr_lower_bound_2_g], \n",
        "              '*Loss - Blue Graph*' : [optimun_lr_upper_bound_l, optimun_lr_lower_bound_1_l, optimun_lr_lower_bound_2_l]}\n",
        "      prdf = pd.DataFrame(data, index = ['First choice lr:', 'Second choice lr:', 'Third choice lr:' ])\n",
        "      pd.options.display.float_format = '{:.10E}'.format\n",
        "      #prdf.style.applymap('color: %s' % color_grad, subset=['*Gradient - Orange Graph*'])\n",
        "      print(prdf)\n",
        "       \n",
        "      return optimun_lr_lower_bound_1_g, optimun_lr_upper_bound_g\n",
        "      \n",
        "    else:\n",
        "        \n",
        "      optimun_lr_upper_bound = df.lrs.iloc[df.min_grad.notna()[::-1].idxmax()]\n",
        "      optimun_lr_lower_bound = df.lrs.iloc[df.min_loss.notna()[::-1].idxmax()]/10\n",
        "      # Print results and return optimal lr\n",
        "      print('Model set to \"freeze\":')\n",
        "      print('  Optimun lr: {:.10E} '.format(optimun_lr_upper_bound))\n",
        "      print('  Min loss divided by 10: {:.10E}'.format(optimun_lr_lower_bound))\n",
        "      return optimun_lr_upper_bound \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}