{
"welcome to lesson seven the last lesson",
"of part one this will be a pretty",
"intense lesson and so don't let that",
"bother you because partly what I want to",
"do is to kind of give you enough things",
"to think about to keep you busy until",
"platt - and so in fact some of the",
"things we cover today I'm not going to",
"tell you about some of the details I've",
"just point out a few things where I'll",
"say like okay that we're not talking",
"about yet that not and we're not talking",
"about that",
"and so then come back in part two to get",
"the details on some of these extra extra",
"pieces right so well you know today will",
"be a lot of material pretty quickly you",
"might require a few viewings to fully",
"understand at all a few experiments and",
"so forth and that's kind of intentional",
"and trying to give you stuff to to keep",
"you amused for a couple of months wanted",
"to start by showing some core work done",
"by a couple of students Reshma and in",
"patters Eero one who have developed an",
"Android and an iOS app and so check out",
"Reshma's post on the forum about that",
"because they have a demonstration of how",
"to create a both Android and iOS apps",
"that are actually on the Play Store and",
"on the Apple App Store so that's pretty",
"cool first first ones I know of that are",
"on the App Store's that are using first",
"AI and let me also say a huge thank you",
"to Rushmore for all of the work she does",
"both for the fast AI community and the",
"machine learning community or generally",
"and also the women in machine learning",
"community in particular she does a lot",
"of fantastic work including providing",
"lots of fantastic documentation and",
"tutorials and community organizing and",
"so many other things so thank you rush",
"me and congrats on getting this app out",
"there we have lots of less than",
"seven-minute bucks today as you see and",
"we're going to start with the one",
"so the first notebook we're going to",
"look at is lesson seven ResNet amnesty",
"and what I want to do is look at some of",
"the stuff we started talking about last",
"week around convolutions and",
"convolutional neural networks and start",
"building on top of them to create a",
"fairly modern deep learning architecture",
"largely from scratch when I say from",
"scratch I'm not going to re-implement",
"things we already know how to implement",
"but kind of use the pre-existing ply",
"torch bits of those so we're going to",
"use the EM list data set which so URLs",
"that amnesty has the whole emne status",
"set often we've done stuff with a subset",
"of it",
"so in there there's a training folder",
"and a testing folder and as I read this",
"in I'm going to show some more details",
"about pieces of the data blocks API so",
"that you see how to kind of see what's",
"going on normally with the date betablox",
"API we've kind of said bla bla bla bla",
"and done it all in one cell but let's do",
"them one cell at a time so first thing",
"you say is what kind of item list do you",
"have so in this case it's an item list",
"of images and then where are you getting",
"the list of file names from in this case",
"by looking in a folder recursively and",
"that's where it's coming from you can",
"pass in arguments that end up going to",
"pillow because pillow or PIL is the",
"thing that actually opens that for us",
"and in this case these are black and",
"white rather than RGB so you have to use",
"pillows convert mode equals L for more",
"details refer to the Python imaging",
"library documentation to see what",
"they're convert modes are but this one",
"is going to be a grayscale which is what",
"M lists is so inside an item list is an",
"item's attribute and the items attribute",
"is kind of the thing that you gave it",
"it's the thing that it's going to use to",
"create your items in this case the thing",
"you gave it really is a list of file",
"names that's what it got from the folder",
"okay when you show images normally it",
"shows them in RGB and so in this case we",
"want to use a binary color map so in",
"first AI you can set a default color map",
"for more information about C map and",
"color maps",
"to the matplotlib documentation and so",
"this will set the default color map for",
"a faster I okay so our image item list",
"contains 70,000 items and it's a bunch",
"of images that are 1 by 28 by 28",
"remember that pipe torch puts channel",
"first so there one channel 28 for 28 you",
"might think we're way out there just 28",
"by 28 matrices rather than a 1 by 28 by",
"28 rank 3 tensor it's just easier that",
"way all the comp 2d stuff and so forth",
"works on rank 3 tensors so you want to",
"you want to include that unit axis at",
"the start and so first day I will do",
"that for you even when it's reading one",
"channel images so the dot items",
"attribute contains the things that's",
"kind of red to build the image which in",
"this case is the file name but if you",
"just index into an item list directly",
"you'll get the actual image object okay",
"and so the actual image object has a",
"show method and so there's there's the",
"image so once you've got an image item",
"list you then split it into training",
"versus validation you nearly always want",
"validation if you don't you can actually",
"use the dot nose split method to create",
"a kind of empty validation set you can't",
"skip it entirely you have to say how to",
"split and one of the options is no split",
"right and so remember that's always the",
"order first create your item list then",
"decide how to split in this case we're",
"gonna do it based on folders in this",
"case the the validation folder for EM",
"list is called testing so in kind of",
"fast AI parlance we use the same kind of",
"parlance that kaggle does which is the",
"training set is what you train on the",
"validation set has labels and you do it",
"for testing that your models working the",
"test set doesn't have labels and you use",
"it for doing inference or submitting to",
"a competition or sending it off to",
"somebody who's held out those labels for",
"you know event or testing or whatever",
"okay so just because a folder in your",
"data set is called testing doesn't mean",
"it's a test set right this one has",
"labels so it's a validation set",
"okay so if you want to do inference on",
"lots you know lots of things at a time",
"rather than one thing at a time you want",
"to use the test equals in in fast AI to",
"say this is stuff which has no labels",
"and I'm just using for inference okay so",
"my split data is a training set and a",
"validation set as you can see so inside",
"the training set there's a folder for",
"each image for each class so now we can",
"take that split data and say label from",
"folder so first you create the Animus",
"then you spit it then you label it and",
"so you can see now we have an X and a Y",
"and the Y are category objects category",
"object is just a class basically so if",
"you index into a label list such as lol",
"trained as a label list you will get",
"back an independent variable independent",
"variable X & Y so this case the X will",
"be an image object which I can show and",
"the Y will be a category object which I",
"can read that's the number it's the",
"number 8 category and there's the 8 next",
"thing we can do is to add transforms in",
"this case we're not going to use the",
"normal get transforms function because",
"we're doing digit recognition and digit",
"recognition like you wouldn't want to",
"flip it left right",
"that would change the meaning of it you",
"wouldn't want to rotate it too much that",
"would change the meaning of it also",
"because these images are so small kind",
"of doing zooms and stuff is going to",
"make them so fuzzy used to be unreadable",
"so normally for small images of digits",
"like this you just add a bit of random",
"padding so I'll use the random padding",
"function which actually returns two",
"transforms so a bit that does the",
"padding and the D that does the random",
"crop so you have to use star to say put",
"both these transforms in this list so",
"now we can call transform this empty",
"array here is referring to the",
"validation set transforms so no",
"transforms for the validation set now",
"we've got a transformed",
"list we can pick a batch size and choose",
"data bunch we can choose normalize in",
"this case we're not using a pre-trained",
"model so there's no reason to use",
"imagenet stats here and so if you call",
"normalize like this without passing in",
"stats it will all grab a batch of data",
"at random and use that to decide what",
"normalization stats to use that's a good",
"idea",
"if you're not using a pre-trained model",
"okay so we've got a dead data bunch and",
"so in that data bunch is a data set",
"which we've seen already but what is",
"interesting is that the training data",
"set now has data augmentation because",
"you've got transforms so plot multi is a",
"faster Oh function that we're all plot",
"the result of calling some function for",
"each of this row by column grid so in",
"this case my function is just grab and",
"grab the first image from the training",
"set and because each time you grab",
"something from the training set it's",
"going to load it from disk and it's",
"going to transform it on the fly right",
"so people sometimes ask like how many",
"transformed versions of the image do you",
"create and the answer is kind of",
"infinite each time we grab one thing",
"from the data set we do a random",
"transform on the fly",
"okay so potentially you everyone will",
"look a little bit different so you can",
"see here if we plot the result of that",
"lots of times we get",
"eights in slightly different positions",
"because we did random padding you can",
"always grab a batch of data then from",
"the data bunch because remember a data",
"bunch has data loaders and data loaders",
"things that you grab a batch at a time",
"and so you can then grab our X patch and",
"a Y batch look at their shape batch size",
"by channel by row by column all fast a a",
"data bunches have a show batch which",
"will show you what's in it in some",
"sensible way okay so that's a quick walk",
"through with the data block API stuff to",
"grab our data so let's start out",
"creating a simple CNN",
"simple confident so the input is 28 by",
"28 so let's define I like to define when",
"I'm creating architectures a function",
"which kind of does the things that I do",
"again and again and again I don't want",
"to call it with the same arguments",
"because I'll forget I'll make a mistake",
"so in this case all of my convolutions",
"are going to be kernel size 3 stride 2",
"padding 1 so let's just create a simple",
"function to do a cons with those",
"parameters so you try to have a",
"convolution it's skipping over one pixel",
"so it's doing jumping jumping two steps",
"each time so that means that each time",
"we have a convolution it's going to have",
"the grid size so I've put a comment here",
"showing what the new grid size is after",
"each one so after the first convolution",
"we have one channel coming in because",
"it's remember it's a grayscale image",
"with one channel and then how many",
"channels coming out whatever you like",
"right so remember you always get to pick",
"how many filters you create regardless",
"of whether it's a fully connected layer",
"in which case it's just the the width of",
"the matrix you're multiplying by or in",
"this case with a 2d cons it's just how",
"many how many filters do you want so I",
"picked 8 and so after this it's dried 2",
"so the 28 by 28 image is now a 14 by 14",
"feature map with 8 channels so",
"specifically therefore it's an 8 by 14",
"by 14 tensor of activations then we'll",
"do batch norm then would devalue so the",
"number of input filters to the next con",
"has to equal the number of output",
"filters from the previous conf and we",
"can just keep increasing the number of",
"channels because we're doing stride to",
"it's going to keep decreasing the grid",
"size notice here it goes from 7 to 4",
"because if you're doing a stride to",
"convey over 7 it's going to be kind of",
"math dot ceiling of 7/2 patch norm rail",
"you confer now down to 2 by 2 personal",
"really akan we're now down to 1 by 1",
"right so after this",
"we have a batch side of the picture map",
"of let's say ten by one by one does that",
"make sense we've got a grid size of one",
"now so it's not a vector of length 10",
"its a rank 3 tensor of 10 by 1 by 1 so",
"our loss functions expect generally a",
"vector not a rank 3 tensor so you can",
"check flatten at the end and flatten",
"just means remove any unit axes so that",
"will make it now just a vector of length",
"10 which is what we always expect so",
"that's how we can create a CNN so then",
"we can return that into a learner by",
"passing in the data and the model and",
"the loss function and if optionally some",
"metrics so we're going to use",
"cross-entropy as usual so we can then",
"call own dot summary and confirm after",
"that first cond we're down to 14 by 14",
"and after the second column 7 by 7 and 4",
"by 4 2 by 2 1 by 1 the flatten comes out",
"calling it a lambda but that as you can",
"see it gets rid of the 1 by 1 then it's",
"now just a length 10 vector for each",
"item in the bench so 128 by 10 matrix of",
"the whole mini batch so just to confirm",
"that this is working ok we can grab that",
"mini batch of X that we created earlier",
"there's so many vetch of X pop it onto",
"the GPU and call the model directly",
"remember any PI torch module we can",
"pretend as a function and that gives us",
"back as we hoped a 128 by 10 resolved ok",
"so that's how you can directly get some",
"predictions out now find fit one cycle",
"and bang we already have a 98.6%",
"accurate confident and this is trained",
"from scratch of course it's not",
"pre-trained we literally",
"created our own architecture about the",
"simplest possible architecture you can",
"imagine 18 seconds to train so that's",
"how easy it is to create a pretty",
"accurate digit detector",
"so let's refactor that a little rather",
"than saying clowns metronome really all",
"the time",
"first day I already has something called",
"con underscore lair which lets you",
"create cons batch normal you",
"accommodations and it has various other",
"options to do other tweaks to it but the",
"basic version is just exactly what I",
"just showed you so we can refactor that",
"like so so that's exactly the same",
"euronet and so you know let's just try",
"it a little bit longer and it's actually",
"99.1 percent accurate if we train it for",
"all over minute so that's cool so how",
"can we improve this well what we really",
"want to do is create a deeper Network",
"and it's a very easy way to create a",
"deeper Network would be after every",
"stride to cons add a stride one cons",
"because the straight one comes doesn't",
"change the feature map size at all so",
"you can add as many as you like right",
"but there's a problem there's a problem",
"and the problem was pointed out in this",
"paper very very very influential paper",
"called deep learning deep residual",
"learning for image recognition by coming",
"her and colleagues at then at Microsoft",
"Research and they did something",
"interesting they said let's look at the",
"training error",
"so forget generalization even let's just",
"look at the training error of a network",
"train on so far 10 and let's try one",
"network of 20 layers just basic 3x3",
"funds it's just basically the same",
"network I just showed you but without",
"batch norm let's try train to 20 layer",
"one and a 56 layer one on the training",
"set so the 56 layer one has a lot more",
"parameters it's got a lot more of these",
"trade one comes in the middle so the one",
"with more parameters should seriously",
"over fit right so you would expect",
"the 56 layer one to zip down to zero ish",
"training error pretty quickly and that",
"is not what happens it is worse than the",
"shallower network so when you see",
"something weird happen really good",
"researchers don't go oh no it's not",
"working",
"they go that's interesting so cutting",
"her said that's interesting what's going",
"on and he said I don't know but what I",
"do know is this I could take this 56",
"layer Network and make a new version of",
"it which is identical but has to be at",
"least as good as the 20 layer network",
"and here's how every two convolutions",
"I'm going to add together the input to",
"those two convolutions add it together",
"with the result of those two",
"convolutions so in other words he's",
"saying instead of saying output equals",
"con two of cons one of X instead he's",
"saying output equals x plus con of two",
"of cons one of s so that fifty six",
"layers worth of convolutions in in that",
"his theory was has to be at least as",
"good as the twenty layer version because",
"it could always just set com2 and cons",
"one to a bunch of zero waits for",
"everything except for the first 20",
"layers because because they're X the",
"input could just go straight through so",
"this thing here is as you see called an",
"identity connection it's the identity",
"function nothing happens at all it's",
"also known as a skip connection so that",
"was a theory right that's what the paper",
"describes as the intuition behind this",
"is what would happen if we created",
"something",
"which has to train at least as well as a",
"20 layer neural network because it kind",
"of contains that 28 layer neural network",
"is literally a path you can just skip",
"over all the convolutions and so what",
"happens and what happened was he won",
"imagenet that year he easily won",
"imagenet that year and in fact you know",
"even today you know we had that",
"record-breaking result on image net",
"speed training ourselves you know in the",
"last year we used this to you know res",
"net has been revolutionary and anytime",
"here's a trick if you're interested in",
"doing some research in the whole",
"research anytime you find some model for",
"anything with recite medical image",
"segmentation or you know some kind of",
"gain or whatever you know and it was",
"written a couple of years ago they might",
"have forgotten to put rest nets in res",
"block res blocks this is what we",
"normally call a res block they might",
"have forgotten what res blocks in so",
"replace their convolutional path with a",
"bunch of res blocks and you'll almost",
"always get better results faster it's a",
"good trick",
"so at Europe's which Rachel and I and",
"David or just came back from and Sylvia",
"we saw a new presentation where they",
"actually figured out how to visualize",
"the loss surface of a neural net which",
"is really cool this is a fantastic paper",
"and anybody who's watching this lesson 7",
"is at a point where they will understand",
"most of the most important concepts in",
"this paper you could read this now you",
"won't necessarily get all of it but I'm",
"sure you'll find it again enough to find",
"an interesting and so the the big",
"picture was this one here's what happens",
"if you if you draw a picture we're kind",
"of X&Y",
"here are two projections of the of the",
"white space and Z is the loss and so as",
"you move through the white space rest at",
"a 56 layer neural network without skip",
"connections is very very bumpy",
"and that's why this got nowhere because",
"it just got stuck in all these hills and",
"bellies the exact same network with",
"identity connections with skip",
"connections has this lost landscape",
"right so that's it's kind of interesting",
"how how how her recognized back in 2015",
"you know this shouldn't happen here's a",
"way that must fix it and it took three",
"years before people were able to say oh",
"this is kind of why it fixed it it kind",
"of reminds me of the batch normal",
"discussion we had a couple of weeks ago",
"people realizing a little bit after the",
"fact sometimes what's what's going on",
"and why it helps so in our code we can",
"create a res block in just the way I",
"described we create a tenant module we",
"create two con Flair's remember a con",
"flair is kind of 2d that's normal you so",
"I come to derail your veteran or so",
"creator of those and then in forward we",
"go conf one of ex-cons two of that and",
"then add X there's a res Bach function",
"already in fast AI so you can just call",
"res block instead and you just pass in",
"something saying how many filters do you",
"want so yeah so there's the wrist block",
"that I defined in a notebook and so with",
"that look with that res block we can now",
"take every one of those I've just copied",
"the previous CNN and after every con to",
"except the last one I added a res block",
"so this is great now got three times as",
"many layers so it should be able to do",
"more compute right but it shouldn't be",
"any harder to optimize so what happens",
"well let's just refactor it one more",
"time since I go come to res block so",
"many times let's just pop that into a",
"little mini sequential model here and so",
"I can refactor that like so",
"like keep refactoring your architectures",
"if you're trying novel architectures",
"because you'll make less mistakes very",
"few people do this most research codes",
"you look at is is clunky as all hell and",
"people often make mistakes in that way",
"so don't don't do that be you know",
"you're all coders so use your coding",
"skills to make life easier",
"okay so there's my ResNet ish",
"architecture and ela find as usual fit",
"for a while and I get ninety nine point",
"five four so that's interesting because",
"we've trained this literally from",
"scratch with an architecture we built",
"from scratch I didn't look at this",
"architecture anywhere it's just the",
"first thing that came to mind but in",
"terms of where that puts us point four",
"five percent error is around about the",
"state of the art for this data set as of",
"three or four years ago now you know",
"today M just is considered a kind of",
"trivially easy data set so I'm not",
"saying like well we've broken some",
"records here people have got beyond",
"0.45% error but what I'm saying is that",
"you know we can't you know this kind of",
"resonate is a genuinely extremely useful",
"network still today and this is this is",
"really all we use in our first image net",
"training still and one of the reasons as",
"well is that it's so popular so the the",
"vendors of the library spend a lot of",
"time optimizing it so things tend to",
"work fast where are some more modern",
"style architectures using things like",
"separable or group convolutions tend not",
"to actually train very quickly in",
"practice if you look at the definition",
"of res block in the FASTA code you'll",
"see it looks a little bit different to",
"this and that's because i've created",
"something called a merge layer and a",
"merge layer is something which in the",
"forward just keeps dense for a moment",
"the forward says X plus X",
"a ridge so you can see that some coming",
"ResNet ish going on here what is X dot",
"orig well if you create a special kind",
"of sequential model called a sequential",
"e^x so this is like the fastes",
"sequential extended it's just like a",
"normal sequential model but we store the",
"input in X dot orig right and so this",
"this here is the quench really ex-con",
"flour corn flour merge layer will do",
"exactly the same that's this okay so you",
"can create your own variations of ResNet",
"blocks very easily with just sequential",
"ax and merge layer so there's something",
"else here which is when you create your",
"merge layer you can optionally set dense",
"equals true what happens if you do well",
"if you do it doesn't go X plus X dot",
"image that goes cat X comma X dot orig",
"in other words rather than putting a",
"plus in this connection it does a",
"concatenate so that's pretty interesting",
"because what happens is that you have",
"your your input coming into your res",
"block and once you use concatenate",
"instead of plus it's not called a res",
"block anymore it's called a dense block",
"and it's not quite a ResNet any what",
"more is called a dense net so the dense",
"net was invented about a year after the",
"ResNet and if you read the dense net",
"paper it can sound incredibly complex",
"and different but actually it's",
"literally identical but plus here is",
"replaced with with cat so you have your",
"input coming into your dense block right",
"and you've got a kind of few",
"convolutions in here and then you've got",
"some output coming out and then you've",
"got your identity connection and",
"remember it doesn't plus it con cats so",
"this is the channel access it gets a",
"little bit bigger all right and then so",
"we do another dense block right and at",
"the end of that we have you know all of",
"this coming in drop sorry we have okay",
"so at the end of that we have you know",
"the result of the convolution as per",
"usual but this time",
"identity bloc is that big right so you",
"can see that what happens is that with",
"dense blocks it's getting bigger and",
"bigger and bigger and kind of",
"interestingly the exact input is still",
"here right so that actually no matter",
"how deep you get the original input",
"pixels are still there and the original",
"layer 1 features are still there in the",
"original layer of two features are still",
"there so as you can imagine dense Nets",
"are very memory intensive there are ways",
"to manage this the best from time to",
"time you can have a regular convolution",
"that squishes your channels back down",
"but they are memory intensive but they",
"have very few parameters so for dealing",
"with small datasets",
"you should definitely experiment with",
"dense blocks and dense Nets they tend to",
"work really well on small datasets also",
"because it's possible to kind of keep",
"those original input pixels all the way",
"down the path they work really well for",
"segmentation right because for",
"segmentation you know you kind of want",
"to be able to reconstruct the original",
"resolution of your picture so having all",
"of those original pixels still there is",
"super helpful so so that's that's",
"residents and the main one of the main",
"reasons other than fact that rez nets",
"are awesome to tell you about them is",
"that these skipped connections are",
"useful in other places as well and there",
"it's particularly useful in other places",
"and other ways of designing",
"architectures for segmentation so in",
"building this lesson I always kind of I",
"keep trying to take old papers and",
"saying like imagining like what would",
"that person have done if they had access",
"to all the modern techniques we have now",
"and I try to kind of rebuild them in a",
"more modern style so I've been really",
"rebuilding this next architecture going",
"to look at called a unit in a more",
"modern style recently and got to the",
"point now I keep showing you this",
"semantic segmentation paper with the",
"state of the art for camford which was",
"91.5 this week I got it up to 94.1 using",
"the architecture I'm about to show you",
"so we just we keep pushing this further",
"and further and further and it's really",
"was all about you know adding all of the",
"modern tricks many of which I'll show",
"you today some of which we will see in",
"part two so what we're going to do to",
"get there is we're going to use this",
"unit so we've used a unit before I've",
"improved it a bit since then so we've",
"used a unit before we used it when we",
"did the camp fit segmentation but we",
"didn't understand what it was doing so",
"we're now in a position where we can",
"understand what I was doing and so the",
"first thing we need to do is kind of",
"understand the basic idea of how you can",
"do segmentation so if we go back to our",
"canvas notebook in our camping notebook",
"you'll remember that basically what we",
"were doing is we were taking these",
"photos and adding a class to every",
"single pixel and somebody go data touch",
"show batch for something which is a",
"segmentation item list it will",
"automatically show you these color-coded",
"pixels so here's the thing like in order",
"to color code this as a pedestrian you",
"know but this as a bicyclist it needs to",
"know what it is there needs to actually",
"know that's what a pedestrian looks like",
"and it needs to know that's exactly",
"where the pedestrian is and this is the",
"arm of the pedestrian and not part of",
"their shopping basket it needs to really",
"understand a lot about this picture to",
"do this task and it really does through",
"this task like when you looked at the",
"results of our top model it's it's you",
"know I can't see a single pixel by",
"looking at it by eye I know there's a",
"few wrong but I can't see",
"ones that are wrong is that accurate so",
"how does it do that so the way that",
"we're doing it to get these really",
"really good results is not surprisingly",
"using pre-training so we start with a",
"ResNet 34 and you can see that here unit",
"learner data comma model start ResNet 34",
"and if you don't say pre-trained equals",
"false by default you get pre-trained",
"equals true because why not so we start",
"with a resonate 34 which starts with a",
"big image so in this case this is from",
"the unit paper now they're images they",
"started with one channel by 572 by 572",
"this is for medical imaging segmentation",
"so after your stride to cons you and",
"they're doubling the number of channels",
"to 128 and they're having the size so",
"they're now down to 280 by 280 in this",
"original unit paper they didn't add any",
"padding so they lost the pixel on each",
"side each time they did a con that's why",
"you're losing these two but so basically",
"half the size and then half the size and",
"then half the size and then half the",
"size until they're down to 28 by 28 with",
"1024 channels right so that's that's",
"what the unit's down the sampling path",
"this is called the down sampling path",
"look like ours is just a ResNet 34 so",
"you can see it here learn dot summary",
"right this is literally a resinate 34 so",
"you can see that the size keeps having",
"channels keep going up and so forth",
"okay so eventually you've got down to a",
"point where if you use a unit",
"architecture it's 28 by 28 with 1,024",
"channels with the resonant architecture",
"with a 224 pixel input it would be 512",
"channels by 7 by 7 so it's a pretty",
"small grid size on this feature map",
"somehow we've got to end up with",
"something which is the same size as our",
"original picture so how do we do that",
"how do you do computation which",
"increases the grid size well we don't we",
"don't have a way to do that in our",
"current bag of tricks",
"we can use a stride one conf to do",
"computation and keeps grid size or a",
"stride to con to do computation and half",
"the grid size so how do we double the",
"grid size we do a stride 1/2 conf also",
"known as a deconvolution also known as a",
"transposed convolution there is a",
"fantastic paper called a guide to",
"convolution arithmetic fatigue learning",
"that shows a great picture of exactly",
"what does a 3x3 kernel stride 1/2 cons",
"look like and it's literally this if you",
"have a 2x2 input so the blue squares are",
"the 2x2 input you add not only 2 pixels",
"of padding all around the outside but",
"you also add a pixel of padding between",
"every pixel and so now if we put this",
"3x3 kernel here and then here and then",
"here usually other 3x3 kernels just",
"moving across it in the usual way you",
"will end up going from a 2x2 output to a",
"5x5 output so if you only added one",
"pixel of padding around the outside you",
"would add up end up with a 3x3 output",
"right so that's very 4x4 so this is how",
"you can increase the resolution this was",
"the way people did it until maybe a year",
"or two ago that's another trick for",
"improving things you find online because",
"this is actually a dumb way to do it and",
"it's kind of obvious it's a dumb way to",
"do it for a couple of reasons one is",
"that like hello look at this nearly all",
"of those pixels are white they're nearly",
"all zeros",
"so like what a waste what a waste of",
"time what a waste of computation",
"there's just nothing going on there I'm",
"also this one when you get down to like",
"that 3x3 area two out of the nine pixels",
"are non-white but this one one out of",
"the nine at lone white so they're kind",
"of like there's different amounts of",
"information going into different parts",
"of your convolution so like this it just",
"doesn't make any sense to kind of throw",
"away information like this and they're",
"going to do all this unnecessary",
"computation and have different parts of",
"the convolution having access to",
"different amounts of information so what",
"people generally do nowadays is",
"something really simple which is if you",
"have a let's say a two by two input with",
"these are your pixel values a a B C and",
"D and you want to create a four by four",
"why not just do this a a a a b b b b CC",
"CC d D D D so I've now upscaled from two",
"by two to four by four I haven't done",
"any interesting computation but now on",
"top of that I could just do a Strad one",
"convolution and now I have done some",
"computation right so an up sample this",
"is called nearest neighbor interpolation",
"nearest neighbor interpolation so you",
"can just do that's super fast just",
"that's taken to a nearest neighbor",
"interpolation and then a stride one conf",
"and now you've got some computation",
"which is actually kind of using you know",
"there's no zeros here this is kind of",
"nice because it gets a mixture of A's",
"and B's which is kind of what you would",
"want and so forth another approach is",
"instead of using nearest neighbor",
"interpolation you can use bilinear",
"interpolation which basically means",
"instead of copying a to all those",
"different cells you take a kind of a",
"weighted average",
"if the cells around it so for exam",
"if you were you know looking at what",
"should go here he would kind of go like",
"oh it's about 3/8 2 C's 1d and 2 B's and",
"you could have taken the average not",
"exactly but roughly just a weighted",
"average by linear interpolation you'll",
"find in any you know all over the place",
"it's is pretty standard technique",
"anytime you look at a picture on your",
"computer screen and change its size it's",
"doing bilinear interpolation so you can",
"do that and then a strike one conf so",
"that was what people were using worst",
"what people still tend to use that's as",
"much as are going to teach you this part",
"in part two will actually learn what the",
"first day I library is actually doing",
"behind the scenes which is something",
"called a pixel shuffle also known as sub",
"pixel convolutions it's got not",
"dramatically more complex but complex",
"enough that I won't cover it today",
"there's a same basic idea all of these",
"things is something which is basically",
"letting us do a convolution that ends up",
"with something that's twice the size and",
"so that gives us our up sampling path",
"right so that lets us go from 28 by 28",
"to 54 by 54 and keep on doubling the",
"size so that's good and that was that",
"was it until unit came along that's what",
"people did and it didn't work real well",
"which is not surprising because like in",
"this 28 by 28 feature map how the hell",
"is it going to have enough information",
"to reconstruct a 572 by 572 output space",
"you know that's a really tough ask so",
"you tended to end up with these things",
"that lack fine detail so what dollar for",
"on a burger and a towel did was they",
"said hey let's add a skip connection an",
"identity connection and amazingly enough",
"this was before res Nets existed so this",
"was like a really big leap really",
"impressive",
"and so but rather than adding a skip",
"connection that skipped every two",
"convolutions they added skip connections",
"where these gray lines are in other",
"words they added a skip connection from",
"the same part of the downsampling path",
"to the same-sized bit in the up sampling",
"path and they didn't add that's why you",
"can see the white and the blue next to",
"each other they didn't add they",
"concatenated so basically these are like",
"dense blocks right but the Skip",
"connections are skipping over larger and",
"larger amounts of the architecture so",
"that over here you've literally got",
"nearly the input pixels themselves",
"coming into the computation of these",
"last couple of layers and so that's",
"going to make it super handy through",
"resolving the fine details in these",
"segmentation tasks because you've",
"literally got all of the fine details on",
"the downside you don't have very many",
"layers of computation going on here just",
"for right so you better hope that by",
"that stage you've done all the",
"computation necessary to figure out is",
"this the bicyclist or is this a",
"pedestrian but you can then add on top",
"of that something saying like is this",
"you know is this exact pixel where their",
"nose finishes or is that the start of",
"the tree so that works out really well",
"and that's a unit so this is the unit",
"code from fast AI and the key thing that",
"comes in is the encoder the encoder",
"refers to that part in other words in",
"our case a ResNet 34 in most cases they",
"have this specific older style",
"architecture but like I said replace any",
"older style architecture bits where the",
"ResNet bits and life improves",
"particularly if they're pre trained so",
"that certainly happened for us so we",
"start with our encoder so our layers of",
"our unit is an encoder then batch norm",
"then rally and then middle con which is",
"just con flare comma con flare member",
"Khan",
"is a conf rally veteran on in faster go",
"and so that middle con is these two",
"extra steps here at the bottom okay just",
"doing a little bit of computation you",
"know it's kind of nice to add more",
"layers of computation where you can so",
"encode a batch or Lu and then to",
"convolutions and then we enumerate",
"through these indexes what are these",
"indexes I haven't included the code but",
"these are basically we figure out what",
"is the layer number where each of these",
"stripe to comes occurs and we just store",
"it in an array of indexes so then we can",
"loop through that and we can basically",
"say for each one of those points create",
"a unit block telling us how many",
"upsampling channels that are and how",
"many cross connection these these things",
"here are called cross connections at",
"least that's what I call them so that's",
"really the main works going on in the in",
"the unit block as I said there's quite a",
"few tweaks we do as well as the fact we",
"use a much better encoder we also use",
"some tweaks in all of our app sampling",
"using this pixel shuffle we use another",
"tweak called ICN our and then and then",
"another tweak which I just did in the",
"last week is to not just take the result",
"of the convolutions and pass it across",
"but we actually grab the input pixels",
"and make them another cross connection",
"that's what this last cross is here you",
"can see we're literally appending a res",
"block with the original inputs so you",
"can see I merge layer so really all the",
"works going on a new net block and unit",
"block is it has to store the the",
"activations at each of these",
"downsampling points and the way to do",
"that as we learn in the last lesson is",
"with hooks so we we put hooks into the",
"resinate 34 to store the activations",
"each time there's a strata to cons and",
"so that's you can see here we we grab",
"the hook okay and we grab the result of",
"the stored value in that hook and we",
"literally just go torch doc hat so we",
"concatenate",
"the upsampled convolution with the",
"result of the hook which we Chuck",
"through batch norm and then we do two",
"convolutions to it and actually you know",
"something you could play with at home is",
"pretty obvious here anytime you see two",
"convolutions like this there's an",
"obvious question is what if we used a",
"resident block instead so you could try",
"replacing those two comes with a",
"resinate block you might find you get",
"even better results they're the kind of",
"things I look for when I look at an",
"architecture is like oh two columns in a",
"row probably should be a ResNet block",
"okay so that's that's unit and you know",
"it's amazing to think you know it",
"preceded ResNet to preceded dense net",
"it's been it wasn't even published in a",
"major machine learning venue it was",
"actually published in Mekhi which is a",
"specialized medical image computing",
"conference for years actually you know",
"it was largely unknown outside of the",
"medical imaging community and actually",
"what happened was tackle competitions",
"for segmentation kept on being easily",
"won by people using units and that was",
"the first time I saw it getting noticed",
"outside the medical imaging community",
"and then gradually a few people in the",
"academic machine learning community",
"started noticing and now everybody loves",
"unit which I'm glad because it's just",
"it's just awesome so yeah so identity",
"connections regardless of whether",
"they're a plus style or a concat style",
"we're incredibly useful they can",
"basically get us close to the state of",
"the art on lots of important tasks so I",
"want to use them on another task now and",
"so the next task I want to look at is",
"image restoration so image restoration",
"refers to starting with an image at this",
"time we're not going to create a",
"segmentation mask but we're going to try",
"and create a a better image",
"and there's lots of kind of versions of",
"better there could be different image so",
"the kind of things we can do with this",
"kind of image generation would be take a",
"low res image make it high res take a",
"black-and-white image make a color take",
"an image where something's being cut out",
"of it and trying to replace the cutout",
"thing take a photo and try and turn it",
"into what looks like a line drawing take",
"a photo and try and make it look like a",
"Monet painting these are all examples of",
"kind of image to image generation tasks",
"which you all know how to do after this",
"part of class so in our case we're going",
"to try to do image restoration which is",
"going to start with low resolution poor",
"quality JPEGs with writing written over",
"the top of them and get them to replace",
"them with high resolution good quality",
"pictures in which the the text has been",
"removed two questions okay let's go",
"why do you compat before calling comp to",
"comp one not after because if you did",
"kind of one Conte you know if you did",
"your combs before you concat then",
"there's no way for the channels of the",
"two parts to interact with each other",
"you don't get any you know so remember",
"in a 2d conf it's really 3d right it's",
"moving across two dimensions but in each",
"case it's doing a dot product of all",
"three dimensions of a rank three tensor",
"row by column by Channel so generally",
"speaking we want as much interaction as",
"possible we want to say you know this",
"part of the down sampling path and this",
"part of the up sampling path if you look",
"at the combination of them you find",
"these interesting things so generally",
"you know you you want to have as many",
"interactions going on as possible in",
"each computation that you do",
"is concatenating every layer together in",
"a dense Network when the size of the",
"image feature Maps is changing through",
"the layers that's a great question so if",
"you have a stride to cons you can't keep",
"dense knitting right so that's what",
"actually happens in a dense net is you",
"kind of go like dense block growing",
"dense block growing dense block growing",
"so you getting more and more channels",
"and then you do a stride to cons without",
"a dense block and so now it's kind of",
"gone and then you just do a few more",
"dense blocks and then it's gone so in",
"practice a dense block doesn't actually",
"keep all the information all the way",
"through but just every up into every one",
"of these stride to comes and there's",
"kind of various ways of doing these",
"bottlenecking layers where you're",
"basically saying hey let's let's reset",
"it also helps us keep memory under",
"control because at that point we can",
"decide how many channels we actually",
"want good questions thank you back so in",
"order to create something which can turn",
"crappy images into nice images we need a",
"data set containing nice versions of",
"images and crappy versions of the same",
"images so the easiest way to do that is",
"to start with some nice images and crap",
"fi them and so the way to crap fi them",
"is to create a function called crap fi",
"which contains your krappa fication",
"logic so Mike ratification logic you can",
"pick your own is that I open up my nice",
"image I resize it to be really small 96",
"by 96 pixels with bilinear interpolation",
"I then pick a random number between 10",
"and 70 I draw that number into my image",
"at some random location and then I save",
"that image with a JPEG quality of that",
"random number and a JPEG quality of 10",
"is like absolute rubbish a JPEG quality",
"of 70 is not bad at all okay so I end up",
"where",
"if high quality images low quality",
"images that look something like these",
"and so you can see this one you know",
"there's the image and this is after",
"transformation so that's why it's been",
"flipped and you won't always see the",
"image because we're zooming into them so",
"a lot of the time the image is cropped",
"out",
"so yeah it's trying to figure out how to",
"take this incredibly jpg artifactory",
"thing with with text written over the",
"top and turn it into into this so I'm",
"using the Oxford pets data set again the",
"same one we used in lesson 1 so there's",
"nothing more high qualities and pictures",
"of dogs and cats I think we can all",
"agree with that",
"the krappa fication process can take a",
"while but fast AI has a function called",
"parallel and if you pass parallel a",
"function name and a list of things to",
"run that function on it will run that",
"function on them all in parallel so this",
"actually can run pretty quickly",
"the way you write this function is where",
"you get to do all the interesting stuff",
"in this assignment try and think of an",
"interesting krappa fication which does",
"something that you want to do right so",
"if you want to you know colorize",
"black-and-white images you would replace",
"it with black-and-white if you want",
"something which can you know take like",
"large cutout blocks of image and replace",
"them with kind of hallucinatin image you",
"know add a big black box to these if you",
"want something which can kind of take",
"old families photos scans that have been",
"like folded up and have crinkles in try",
"and find a way of like adding dust",
"prints and crinkles and so forth right",
"and anything that you don't include in",
"crap fi your model won't learn to fix",
"because every time it sees that in your",
"photos the input and output will be the",
"same so it won't consider that to be",
"something worthy of fixing okay",
"so so we now want to create a model",
"which can take an input photo that looks",
"like that and outputs something that",
"looks like that so obviously what we",
"want to do is",
"is a unit because we already know that",
"units can do exactly that kind of thing",
"and we just need to pass the unit that",
"data okay so our data is just literally",
"the file names of each of those from",
"each of those two folders do some",
"transforms data bunch normalize or use",
"imagenet stats because we're going to",
"use a pre trained model why are we using",
"a pre trained model well because like if",
"you're going to get rid of this 46 you",
"need to know what probably was there and",
"to know what probably was there you need",
"to know what this is a picture of back",
"because otherwise how can you possibly",
"know what it ought to look like so you",
"know let's use a pre trained model that",
"knows about these kinds of things so we",
"create our unit with that data the",
"architecture is ResNet 34 these three",
"things are important and interesting and",
"useful but I'm going to leave them to",
"part two okay for now you should always",
"include them when you use a unit for",
"this kind of problem and so now we're",
"going to add this whole thing I'm",
"calling a generator okay it's going to",
"generate this is clarity of modeling",
"they're kind of there's not a really",
"formal definition but it's basically",
"something where the thing we're",
"outputting is like a real object in this",
"case an image it's not just a number so",
"we're going to create a generator",
"learner which is this unit learner and",
"then we can fit we're using MSC loss",
"right so in other words what's the mean",
"squared error between the actual pixel",
"value that it should be in the pixel",
"value that we predicted MSE lost",
"normally expects two vectors in our case",
"we have two images so we have a version",
"called MSC loss flat which simply",
"flattens out those images into a big",
"long vector there's there's never any",
"reason not to use this even if you do",
"have a vector it works fine if you don't",
"have a work vector it'll also work fine",
"so we're already you know down to 0.05",
"mean squared error on the pixel values",
"which is not bad after one minute 35",
"like all things in fast day I pretty",
"much because we're doing transfer",
"learning by default when you create the",
"it'll freeze the the pre-trained part",
"and the pre-trained part of a unit is",
"this putt the downsampling part that's",
"where the resident is so let's unfreeze",
"that and train a little more and look at",
"that so with you know three minutes of",
"four minutes of training we've got",
"something which is basically doing a",
"perfect job of removing numbers it's",
"certainly not doing a good job of up",
"sampling but it's definitely doing a",
"nice you know sometimes when it removes",
"a number it maybe leaves a little bit of",
"JPEG artifact but you're certainly doing",
"something pretty useful and so if all we",
"wanted to do was kind of watermark",
"removal would be finished we're not",
"finished because we actually want this",
"thing to look more like this thing so",
"how we got to do that the problem the",
"reason that we're not making as much",
"progress with that as we'd like is that",
"our loss function doesn't really",
"describe what we want because actually",
"the the mean squared error between the",
"pixels of this and this is actually very",
"small right if you actually think about",
"it most of the pixels are very nearly",
"the right color but we're missing the",
"texture of the pillow and we're missing",
"the eyeballs entirely pretty much right",
"and we're missing the texture of the fur",
"right so we want we want some loss",
"function that does a better job than",
"pixel mean squared error loss of saying",
"like is this a good quality picture of",
"this thing so there's a fairly general",
"way of answering that question and it's",
"something called a generative",
"adversarial Network or can and again",
"tries to solve this problem by using a",
"loss function which actually calls",
"another model and let me describe it to",
"you",
"so we've got our crappy image right and",
"we've already created a generator it's",
"not a great one but it's not terrible",
"right and that's creating predictions",
"like like this we have a high-res image",
"like that and we can compare the",
"high-res image to the prediction with",
"with pixel MSE okay",
"we could also train another model which",
"we would variably call variously call",
"either the discriminator or The Critic",
"they both mean the same thing I'll call",
"it a critic we could try and build a",
"binary classification model that takes",
"all the pairs of the generated image and",
"the real high-res image and tries to",
"classify learn to classify which is",
"which you know so look at some picture",
"and say like hey what do you think is",
"that a high-res cat or is that a",
"generated cat how about this one is that",
"a high-res cat or a generated cat so",
"just a regular standard binary",
"cross-entropy e classified so we know",
"how to do that already so if we had one",
"of those we could now train we could",
"fine tune the generator and rather than",
"using pixel MSE is the loss the loss",
"could be how good are we at fooling the",
"critic so can we create generated images",
"that the critic thinks are real so that",
"would be a very good plan right because",
"if it can do that if if the loss",
"function is am I fooling the critic that",
"then it's going to learn to create",
"images which the critic can't tell",
"whether they're real or fake so we could",
"do that for a while train a few batches",
"but the critic isn't that great the",
"reason the critic is that isn't that",
"great is because it wasn't that hard",
"like these images are really shitty so",
"it's really easy to tell the difference",
"alright so after we train the generator",
"a little bit more using the",
"critic as the loss function the",
"generators going to get really good",
"they're falling the critic so now we're",
"going to stop training the generator and",
"we'll drain the critic some more on",
"these newly generated images so now that",
"the generators better it's now a tougher",
"task for the critic to the side which is",
"real and which is fake so again so we're",
"trained that a little bit more and then",
"once we've done that and the critics now",
"pretty good at recognizing the",
"difference between the better generated",
"images and the originals well we'll go",
"back and we'll fine-tune the generator",
"some more using the better discriminator",
"the better critic as the loss function",
"and so we'll just go ping pong ping pong",
"backwards and forwards that's again well",
"that's our version of again I don't know",
"if anybody's written this before we've",
"we've created a new version of again",
"which is kind of a lot like the original",
"Gans but we have this this neat trick",
"where we pre train the generator and we",
"pre train the critic",
"I mean games have been kind of in the",
"news a lot they're pretty fashionable",
"tall and if you've seen them you may",
"have heard that they're a real pain to",
"Train but it turns out we realized that",
"really most of the pain of training them",
"was at the start if you don't have a pre",
"trained generator and you don't have a",
"pre trained critic then it's basically",
"the blind leading the blind right you're",
"basically like the critics well the",
"generators trying to generate something",
"which falls a critic but the critic",
"doesn't know anything at all so it's",
"basically got nothing to do and then the",
"critics kind of try to decide whether",
"the generated images are real or not and",
"that gets really obvious so it does does",
"it and so they kind of like don't go",
"anywhere for ages and then once they",
"finally start picking up steam they go",
"along pretty quickly so if you can find",
"a way to generate things without using",
"again like means quit there are pixel",
"loss and discriminate things without",
"using a can like predict on that first",
"generator you can make a lot of progress",
"so let's create the critic so to create",
"just a totally standard fast AI binary",
"classification model we need two folders",
"one folders contain",
"high-res images one folder containing",
"generated images we already have the",
"folder with high-res images so we just",
"have to save our generated images so",
"here's a teeny tiny bit of code that",
"does that we're going to create a",
"directory called image gen pop it into a",
"variable called path gen we're good a",
"little function called save Preds that",
"takes a data loader and we're going to",
"grab all of the file names because",
"remember that in an item list the dot",
"items contains the file names if it's an",
"image item list so here's the final file",
"names in that data loaders data set and",
"so now let's go through each batch of",
"the data loader and let's grab a batch",
"of predictions for that batch and then",
"reconstruct akil's true means it's",
"actually going to create fast AI image",
"objects for each of those each thing in",
"the in the batch and so then we'll go",
"through each of those predictions and",
"save them and the name will save it with",
"is the name of the original file but",
"we're going to pop it into our new",
"directory so that's it that's how you",
"save predictions and so you can see I'm",
"kind of increasingly not just using",
"stuff that's already in the first day I",
"library but try to show you how to write",
"stuff yourself right and generally",
"doesn't require heaps of code to do that",
"and so if you come back for part two",
"this is what you know platen matza part",
"two were kind of like here's how you use",
"things inside the library and of course",
"here's how we wrote the library so",
"increasingly writing our own code okay",
"so save those predictions and lend this",
"just to a PIL dot image to open on the",
"first one and yep there it is okay so",
"there's an example of a generated image",
"so now I can train a critic in the usual",
"way it's really annoying to have to",
"restart TripIt a notebook to refresh",
"your reclaim GPU memory so one easy way",
"to handle this is if you just set",
"something that you knew was using a lot",
"of GPU to none like this learner and",
"then just go GC collect that tells",
"Python to do",
"a memory garbage collection and after",
"that you'll generally be fine you'll be",
"able to use all of your GPU remember",
"again if you're using nvidia SMI to",
"actually look at your GPU memory you",
"won't see it clear because plate watch",
"still has a kind of allocated cache but",
"it makes it available so you should find",
"this is how you can avoid restarting",
"your notebook okay",
"so we're going to create a critic it's",
"just an image item list from folder in",
"the totally usual way and the classes",
"will be the image gen and images will do",
"a random split because we want to know",
"how well we're doing with a critic to",
"have a validation set",
"we just label it from folder in the",
"usual way",
"that's and transforms data bunch",
"normalized so it's a totally standard",
"object classifier okay so we've got a",
"totally standard classifier so here's",
"what some of it looks like so here's one",
"from the real images real images",
"generated images generated images okay",
"so that's it's going to try and figure",
"out which class is which okay so we're",
"going to use Brian airy cross HB as",
"usual however we're not going to use a",
"res net here and the reason we'll get",
"into in more detail in part two but",
"basically when you're doing again you",
"need to be particularly careful that the",
"the generator and the critic can't kind",
"of both push in the same direction and",
"like increase the weights out of control",
"so we have to use them in called",
"spectral normalization to make Gans work",
"nowadays well learn about that in part",
"two so anyway if you say Gann critic",
"that will give you first a a will give",
"you a binary classifier suitable begins",
"I strongly suspect we probably can use a",
"resonate here we just have to create a",
"pre trained ResNet with spectral norm",
"hope to do that pretty soon",
"we'll see how we go but as of now this",
"is kind of the best approach there's",
"this thing called game critic again",
"critic uses a slightly different way of",
"averaging the the different parts of the",
"image when it does the loss so anytime",
"you're doing again at the moment you",
"have to wrap your loss function with",
"adaptive loss again we'll look at the",
"details in part two for now just know",
"this is what you have to do and it'll",
"work so other than that slightly odd",
"loss function and that slightly odd",
"architecture everything else is the same",
"we can call that to create our critic",
"because we have this slightly different",
"architecture and slightly different loss",
"function we did a slightly different",
"metric this is the equivalent gain",
"version of accuracy the critics and then",
"we can train it and you can see it's 98%",
"accurate at recognizing that kind of",
"crappy thing from that kind of nice",
"thing but of course we don't see the",
"numbers here anymore right because these",
"are the generated images that generate",
"already knows how to get rid of those",
"numbers that are written on top okay so",
"let's finish up this game now that we",
"have pre trained the generator and",
"pre-trained the critic we now need to",
"get it to kind of ping pong between",
"training a little bit of each and the",
"amount of time you spend on each of",
"those things and the learning rates you",
"use is still a little bit on the fuzzy",
"side so we've created a again learner",
"for you which you just pass in your",
"generator and your critic which we've",
"just just simply loaded here from the",
"ones we just trained and it will go",
"ahead and when you go learned up fit it",
"will do that for you it'll figure out",
"how much time to train the generator and",
"then when to switch to training the",
"discriminator the critic it'll go back",
"on and forward these weights here is",
"that what we actually do is we don't",
"only use the critic as the loss function",
"if we only use the critic as a loss",
"function the game could get very good at",
"creating",
"pictures that look like real pictures",
"but they actually have nothing to do",
"with the original picture the original",
"photo at all so we actually add together",
"the pixel loss and the critic loss and",
"so those two losses are kind of on",
"different scales so we multiply the",
"pixel loss by something between about 50",
"and about 300 again something in that",
"range generally works pretty well",
"something else with cans cans hate",
"momentum when you're training them it",
"kind of doesn't make sense to train them",
"with her mentum because you keep",
"switching between generator and critic",
"so it's kind of tough maybe there are",
"ways to use momentum but I'm not sure",
"anybody's figured it out so this number",
"here when you create an atom optimizer",
"is where the momentum goes so you should",
"set that to zero so anyway if you're",
"doing games use these hyper parameters",
"it should work okay so so that's what",
"can learner does and so then you can go",
"fit and it trains for a while and one of",
"the tough things about ganz is that",
"these loss numbers they're meaningless",
"you can't expect them to go down right",
"because as the generator gets better it",
"gets harder for the discriminator the",
"critic and then as the credit gets",
"better it's harder for the generator so",
"the numbers should stay about the same",
"okay so that's one of the tough things",
"about training ganz is it's kind of hard",
"to know how are they doing so the only",
"way to know how are they doing is to",
"actually take a look at the results from",
"time to time I haven't and so if you put",
"show image equals true here it'll",
"actually print out a sample after every",
"epoch I haven't put that in the notebook",
"because it makes it too big for the repo",
"but you can try that so I've just put",
"the results at the bottom and here it is",
"so pretty beautiful I would say we",
"already knew how to get rid of the",
"numbers but we're now don't really have",
"that kind of",
"back to where it used to be and it's",
"it's definitely sharpening up this",
"little kitty cat quite nicely it's not",
"great always like there's some weird",
"kind of noise going on here certainly a",
"lot better than the horrible original",
"like this is a tough job to turn that",
"into that but there are some really",
"obvious problems like here these things",
"ought to be eyeballs and they're not so",
"why aren't they well our critic doesn't",
"know anything about eyeballs and even if",
"it did it wouldn't know that eyeballs",
"are particularly important you know we",
"care about eyes like when we see a cat",
"with that eyes it's a lot less cute I",
"mean I'm more of a dog person but you",
"know it's it just doesn't know that this",
"is a feature that that matters",
"particularly because the critic remember",
"is not a pre trained network so I kind",
"of suspect that if we replace the critic",
"with a pre trained network that's been",
"pre trained on imagenet but is also",
"compatible with gans it might do a",
"better job here but it's definitely a",
"shortcoming of this approach so I'm",
"going to have a break Oh question first",
"and then we'll have a break and then",
"after the break I will show you how to",
"find the cat's",
"eyeballs again for what kind of problems",
"do you not want to use units",
"well unit so for when the the size of",
"your output you know is is similar to",
"the size of your input and kind of",
"aligned with it",
"there's no point kind of having cross",
"connections if that level of spatial",
"resolution in the output isn't necessary",
"or useful so yeah any kind of generative",
"modeling and you know segmentation is",
"kind of generative modeling right it's",
"it's Jen",
"rating a picture which is a mask of the",
"original objects yeah so probably",
"anything where you want that kind of",
"that kind of resolution of the output to",
"be of the same kind of fidelity as a",
"resolution of the input obviously",
"something like a classifier makes no",
"sense right you you're in a classifier",
"you just want the down sampling path",
"because at the end you just want a",
"single number which is like is it a dog",
"or a cat or what kind of pet is it or",
"whatever great okay so let's get back",
"together at five past eight just before",
"we leave Dan's I just mentioned there's",
"another notebook you might be interested",
"in looking at which is less than 7w",
"again when games started a few years ago",
"people generally use them to kind of",
"create images out of thin air which I",
"personally don't think is a particularly",
"useful or interesting thing to do but",
"it's kind of a good I don't know it's a",
"good research exercise I guess so",
"we implemented this this w again paper",
"which is kind of really the first one to",
"do a somewhat adequate job somewhat",
"easily and so you can see how to do that",
"with the first AI library it's kind of",
"interesting because the data set we use",
"is this else on bedrooms data set which",
"we've provided in our URLs which just as",
"you can see has bedrooms lots and lots",
"and lots of bedrooms and the approach",
"you'll see in the pros here that Silva",
"wrote the the the approach that we use",
"in this case is to just say can we",
"create a bedroom and so what we actually",
"do is that the the input to the",
"generator isn't an image that we clean",
"up we actually feed to the generator",
"random noise and so then the generators",
"task is can you turn random noise into",
"something which the critic can't tell",
"the difference between that output and a",
"real bedroom",
"and so we're not doing any pre training",
"here or any of the stuff that makes this",
"kind of fast and easy so this is a very",
"traditional approach but you can still",
"see you still just go you know again",
"learner and there's actually a double",
"you can version which is you know this",
"kind of older style approach but you",
"just passing the data in the generator",
"and the critic in the usual way",
"and you call fit and you'll see in this",
"case we have show image on you know",
"after epoch one it's not creating great",
"bedrooms or two or three and you can",
"really see that in the early days of",
"these kinds of games it doesn't do a",
"great job of anything but eventually",
"after you know a couple of hours of",
"training producing somewhat like bedroom",
"ish things you know so anyway it's a",
"notebook you can never play with and",
"it's a bit of fun so I was very excited",
"when we got faster I to the point in the",
"last week or so that we had Gans working",
"in a way we're kind of API wise they're",
"far more concise and more flexible than",
"any other library that exists but also",
"kind of disappointed with them they take",
"a long time to train and the outputs sto",
"like so so and so the next step was like",
"well can we get rid of cans entirely so",
"the first step with with that I mean",
"obviously the thing we really want to do",
"is come up with a better loss function",
"we want a loss function that does a good",
"job of saying this is a high-quality",
"image without having to go all the over",
"game trouble and preferably it also",
"doesn't just say it's a high-quality",
"image but it's an image which actually",
"looks like the thing it's meant to so",
"the real trick here comes back to this",
"paper from a couple of years ago",
"perceptual losses of real-time style",
"transfer and super resolution Justin",
"Johnson at our curve this thing they",
"call perceptual losses it's a nice paper",
"but I hate this term because they're",
"nothing particularly perceptual about",
"them I would call them feature losses so",
"in the fast AI library you'll see",
"this referred to as feature losses and",
"it shares something with Ganz which is",
"that after we go through our generator",
"which they call the image transform net",
"and you can see it's got this kind of",
"unit shaped thing they didn't actually",
"use new nets because at the time this",
"came out nobody in the machine learning",
"world knew about units nowadays of",
"course we use units about anyway",
"something unit ish i should mention like",
"in these kind of these architectures",
"where you have a down sampling path",
"followed by table sampling path the down",
"sampling path is very often called the",
"encoder as you saw in our code actually",
"we call that the encoder and the up",
"sampling path is very often called the",
"decoder in generative models more you",
"know generally including generative text",
"models neural translation stuff like",
"that they didn't be called the encoder",
"and the decoder two pieces right so we",
"have this um this generator and we want",
"a loss function that says you know is",
"the thing that it's created like the",
"thing that we want and so the way they",
"do that is they take the prediction",
"remember Y hat is what we normally use",
"for a prediction from a model we take",
"the prediction and we put it through a",
"pre trained image net network so at the",
"time that this came out the pre-training",
"network at work they were using was vgg",
"people still take that's a kind of old",
"now but people still tend to use it",
"because it works fine for this process",
"so they take the prediction and they put",
"it through vgg the pre trained imagenet",
"network it doesn't matter too much which",
"one it is and so normally the output of",
"that would tell you hey is this",
"generated thing you know a dog or a cat",
"or an airplane or a or a fire engine or",
"whatever right but in the process of",
"getting to that final classification it",
"goes through lots of different layers",
"and in this case they've color-coded all",
"the layers with the same grid size in",
"the feature map with the same color so",
"every time we switch colors we're",
"switching grid size so there's a stride",
"to convey or in VG's case they still",
"used to use some Mac spalling layers",
"which kind of similar idea",
"and so what we could do is say hey let's",
"not take the final output of the vgg",
"model on this generated image but let's",
"take home is something in the middle",
"let's take the activations of some layer",
"in the middle",
"so those activations you know it might",
"be a feature map of like 256 channels by",
"28 by 28 say and so those kind of 28 by",
"28 grid cells will kind of roughly",
"semantically say things like hey in this",
"in this part of that 28 by 28 grid is",
"there something that looks kind of furry",
"or is there something that looks kind of",
"shiny or is there something that was",
"kind of circular is there something that",
"kind of looks like an eyeball or",
"whatever so what we do is that we then",
"take the target so that the actual Y",
"value and we put it through the same",
"pre-trained vgg network now we can pull",
"out the activations of the same layer",
"and then we do a mean square error",
"comparison so it'll say like okay in the",
"real image grid cell 1 1 of that 28 by",
"28 feature map you know is is furry and",
"mu and round shaped and in the generated",
"image it's very and blue and not round",
"shaped so it's kind of like an okay",
"match so that ought to go a long way",
"towards fixing our eyeball problem",
"because in this case the feature Maps",
"going to say this eyeballs here it's",
"right here but there isn't here so do a",
"better job of that please make better",
"idols so that's the idea okay and so",
"that's what we call feature losses or",
"Johnson a tower called perceptual losses",
"so so to do that we're going to use the",
"since seven super res notebook and this",
"time the task we're going to do is kind",
"of the same as the previous task but I",
"wrote this notebook a little bit before",
"the game notebook before I came up with",
"the idea of like putting text honours",
"from having a random JPEG quality so the",
"JPEG quality is always 60 there's no",
"text written on top and it's 96 by 96 so",
"and it's before I realized what a great",
"word crap a PHY is so it's called resize",
"so here's our crappy images and our",
"original images kind of a similar task",
"to what we had before so I'm going to",
"try and create a loss function which",
"does this so the first thing i do is i",
"define a base loss function which is",
"basically an account i got to compare",
"the pixels and the features you know and",
"the choices mainly like MSC or l1",
"doesn't matter too much which you choose",
"I tend to like l1 better than MSC",
"actually so I picked l1 right so anytime",
"you see base loss we mean l1 loss that",
"you could use MSE loss as well so let's",
"create a vgg model right so just using",
"the pre-trained model in vgg there's a",
"attribute called dot features which",
"contains the convolutional part of the",
"model so here's the convolutional part",
"of the vgg model because we don't need",
"the head because we only want the",
"intermediate activations so then we'll",
"check that on the GPU",
"we'll put it into eval mode because",
"we're not training it and we'll turn off",
"requires grad because we don't want to",
"update the weights of this model we're",
"just using it for inference right for",
"the loss so then let's enumerate through",
"all the children of that model and find",
"all of the max pooling layers because in",
"this in the vgg model that's where the",
"grid size changes and as you can see",
"from this picture we kind of want to",
"grab features from every time just",
"before the grid size changes so we grab",
"layer I minus 1 that's the layer before",
"it changes",
"so there's our list of layer numbers",
"just before the max Pauling lives and so",
"all of those are values not surprisingly",
"so those are where we want to grab some",
"features from and so we put that in",
"blocks it's just a list of ID's so",
"here's our feature loss plus which is",
"going to implement this idea so",
"basically we when we call the feature",
"loss class we're going to pass it some",
"pre trained model and so that's going to",
"be called M feet that's the model which",
"contains the features which we want to",
"generate for what our feature loss on so",
"we can go ahead and grab all of the",
"layers from that network that we want",
"the losses for that we left or that we",
"want the features for to create the",
"losses so we're going to need to hawk",
"all of those outputs because remember",
"that's how we grab intermediate layers",
"in pi torch is by hooking them so this",
"is going to contain our our hooked",
"outputs so now in the forward of feature",
"loss we're going to call make features",
"passing in the target so this is our",
"actual Y which is just going to call",
"that vgg model and go through all of the",
"stored activations and just grab a copy",
"of them and so we're going to do that",
"both for the target call that out feet",
"and for the input so that's the output",
"of our generator in feet and so now",
"let's calculate the l1 loss between the",
"pixels because we still want the pixel",
"lost a little bit and then let's also go",
"through all of those layers features and",
"get the l1 loss on them right so we're",
"basically going through every one of",
"these end of each block and grabbing the",
"activations and getting the l1 on each",
"one so that's going to end up in this",
"list called feature losses which are",
"then",
"sum them all up okay and you know by the",
"way the reason to do it as a list is",
"because we've got this nice little",
"callback that if you put them into a",
"thing called metrics in your loss",
"function it'll print out all of the",
"separate layer loss amounts for you",
"which is super handy so that's it that's",
"our perceptual loss or feature loss plus",
"and so now we can just go ahead and",
"train a unit in the usual way with our",
"data and pre-trained architecture which",
"is a resonance ID for passing in our",
"loss function which is using our pre",
"trained vgg model and this is that",
"callback I mentioned lost metrics which",
"is going to print out all the different",
"layers losses for us these are two",
"things that we'll learn about in part",
"two of the course but you should use",
"them allow find I just created a little",
"function called do fit that does fit one",
"cycle and then saves the model and then",
"shows the results so as per usual",
"because we're using a pre trained",
"network in our unit we start with frozen",
"layers for the down sampling path train",
"for a while and as you can see we get",
"not only the loss but also the pixel",
"loss and the loss at each of our feature",
"layers and then also something we'll",
"learn about in part two called gram loss",
"which I don't think anybody's used for",
"super ears before as far as I know but",
"as you'll see it turns out great so",
"that's eight minutes so much faster than",
"it can and already as you can see this",
"is our output modeled out pretty good so",
"then we unfreeze and train some more and",
"it's a little bit better and then let's",
"switch up to double the size and so we",
"need to also have the batch size to",
"avoid writing a GPU memory and freeze",
"again and train some more so it's now",
"taking half an hour even better and then",
"unfreeze and train some more so all in",
"all we've done about an hour and 20",
"minutes of training and look at that",
"it's it's it's done it like I mean those",
"it's it knows that eyes are important so",
"it's really made an effort it knows that",
"fur is important so it's really made an",
"effort so it's",
"with something with like JPEG artifacts",
"around the ears and all this mess and",
"like eyes that are just kind of vague",
"light blue things and it just it really",
"created a lot of texture this cat is",
"clearly kind of like looking over the",
"top of one of those little chlorine",
"frames covered in fuzz so it actually",
"recognized that this thing is probably",
"kind of a capita materials it's created",
"a capital e material for us so I mean",
"that's just remarkable",
"so talking of remarkable we can now so I",
"I've never seen outputs like this before",
"without again so I was just so excited",
"when we were able to generate this and",
"so quickly one GPU hour-and-a-half so",
"like if you create your own crap",
"efficacious functions and train this",
"model your build stuff that nobody's",
"built before because like nobody else's",
"that I know of is doing it this way so",
"there are huge opportunities I think so",
"check this out what we can now do is we",
"can now instead of starting with our low",
"res I actually stored another set at",
"size 256 which are called medium res so",
"let's see what happens if we up size a",
"medium res so we're going to grab our",
"medium res data and here is here is our",
"medium res stored photo and so can we",
"improve this so you can see there's",
"still a lot of room for improvement like",
"you see the the lashes here are very",
"pixelated place where there should be",
"hair here is just kind of fuzzy so watch",
"this area as I hit down on my keyboard",
"bump look at that it's done it you know",
"it's taken a medium res image and it's",
"made a totally clear thing here you know",
"the thirds reappeared light look at the",
"eyeball let's go back the eyeball here",
"is just kind of a general blue thing",
"here it's added all the right texture",
"you know so I just think this is super",
"exciting you know",
"here's a model I trained in an hour and",
"a half using standard stuff that you've",
"all learnt about a you NetApp retrain",
"model feature loss function and we've",
"got something which can turn that into",
"that or you know this absolute mess into",
"this and like it's really exciting to",
"think what what could you do with that",
"right so one of the inspirations here",
"has been a guy called Jason integer and",
"Jason was a student in the course last",
"year and what he did very sensibly was",
"decided to focus basically nearly quit",
"his job and work four days a week for",
"really six days a week on studying deep",
"learning and as you should do he created",
"a kind of capstone project and his",
"project was to combine Ganz and feature",
"losses together and his krappa fication",
"approach was to take color pictures and",
"make the black and white so he took the",
"whole of image net created a black and",
"white image net and then trained a model",
"to recolor eyes it and he's put this up",
"as do defy and now he's got these actual",
"old photos from the 19th century",
"that he's turning into color and like",
"what this is doing is incredible like",
"like look at this the model thought oh",
"that's probably some kind of copper",
"kettle so I'll make it like copper",
"colored and oh these pictures are on the",
"wall they're probably let different",
"colors to the wall and maybe that looks",
"a bit like a mirror maybe it would be",
"reflecting stuff outside you know",
"these things might be vegetables",
"vegetables are often red you know let's",
"make them red it's it's extraordinary",
"what it's done and you could totally do",
"this too like you can take our feature",
"loss and our gam loss and combine them",
"so I'm very grateful to Jason because",
"he's helped us build this this lesson",
"has been really nice because we've been",
"able to help him to because he hadn't",
"realize that he can use all this",
"pre-training and stuff and so hopefully",
"you'll see two older phi in the next",
"couple of weeks be even better at the",
"older fication but hopefully you all can",
"now add other kinds of D Kappa fication",
"methods as well so I'm you know I like",
"every course if possible to show",
"something totally new because then every",
"student has the chance to basically",
"build things that had never been built",
"before so this is this is kind of that",
"thing you know but between the much",
"better segmentation results and these",
"much simpler and faster",
"d krappa fication results I think you",
"can build some really cool stuff do you",
"have a question is it possible to use",
"similar ideas to unit and ganz for NLP",
"for example if I want to tag the verbs",
"and nouns in a sentence or create a",
"really good Shakespeare generator yeah",
"pretty much we don't fully know yet it's",
"a pretty new area but there's a lot of",
"opportunities there and we'll be looking",
"at some in in a moment actually",
"so I actually tried training this",
"actually tried testing this on this",
"remember this picture I showed you with",
"a slide last lesson and it's a really",
"rubbishy looking picture and I thought",
"what would happen if we tried running",
"this just through the exact same model",
"and it changed it from that to that so I",
"thought that was a really good example",
"you can see something it didn't do which",
"is this weird discoloration it didn't",
"fix it because I didn't crap a PHY",
"things with weird discoloration right so",
"if you want to create really good image",
"restoration like I say you did really",
"good gratification okay so um here's",
"what we've learned so far right in in",
"the course some of the main things so",
"we've learnt that neural nets consist of",
"Sandwich layers of are fine functions",
"which are basically matrix",
"multiplications slightly more general",
"version and nonlinearities like rel you",
"and we",
"that the results of those calculations",
"are called activations and the things",
"that go into those calculations that we",
"learn accord parameters and that the",
"parameters are initially randomly",
"initialized or we copy them over from a",
"pre-trained model and then we train them",
"with SGD or faster versions and we",
"learned that convolutions are a",
"particular affine function that work",
"great for auto correlated data so things",
"like images and stuff we learn about",
"batch norm drop out data orientation and",
"weight decay as ways of regularizing",
"models and also batch norm helps train",
"models more quickly and then today we've",
"learned about res slash dense blocks",
"we've all of us you learn a lot about",
"image classification regression",
"embeddings categorical and continuous",
"variables collaborative filtering",
"language models and NLP classification",
"and then kind of segmentation unit and",
"gains so go over these things and make",
"sure that you feel comfortable with each",
"of them if you've only watched this",
"series once you definitely won't people",
"normally watch it you know three times",
"or so to really understand the detail so",
"one thing that doesn't that doesn't get",
"here is our it ends so that's the last",
"thing we're gonna do there it is okay so",
"marinades I'm going to introduce a",
"little kind of diagrammatic method here",
"to explain our ends and the diagrammatic",
"method I'll start by showing you a basic",
"neural net with a single hidden layer",
"square means an input so that'll be",
"batch size by number of inputs right so",
"kind of you know batch size by number of",
"inputs an arrow means a layer broadly",
"defined such as matrix product followed",
"by value a circle is activations okay so",
"in this case we have one set of hidden",
"activations and so given that the import",
"was number of inputs this here is a",
"matrix of number of inputs by number of",
"activations so the output will be",
"a batch size by a number of activations",
"it's really important you know how to",
"calculate these shapes right so go learn",
"dot summary lots to see all the shapes",
"so then here's another arrow so that",
"means it's another layer matrix product",
"followed by non-linearity in this case",
"we go into the output so we use soft Max",
"and then triangle means an output okay",
"and so this matrix product will be",
"number of activations by a number of",
"classes so our output is batch size by",
"number classes",
"okay so let's reuse the that key",
"remember triangle Airport circle is",
"activations hidden state we also call",
"that and rectangle is important so let's",
"now imagine that we wanted to create a",
"get a big document split it in two sets",
"of three words at a time and grab each",
"set of three words and then try to",
"predict the third word using the first",
"two words so if we had the data set in",
"place we could grab word one as an",
"import chuck it through an embedding",
"right create some activations pass that",
"through a matrix product and a",
"non-linearity grab the second word put",
"it through an embedding and then we",
"could either add those two things",
"together or concatenate them generally",
"speaking when you see kind of two sets",
"of activations coming together in a",
"diagram you normally have a choice of",
"concatenate or or add and that's going",
"to create the second bunch of",
"activations and then you could put it",
"through one more fully connected layer",
"and softmax to create an output so that",
"would be a totally standard fully",
"connected neural net with one very minor",
"tweak which is concatenated or adding at",
"this point which we could use to try to",
"predict the third word of every from",
"pairs of two words okay so remember",
"arrows represent layer operations",
"and I removed on this one the specifics",
"of what they are because they're always",
"an ephah in function followed by a",
"non-linearity okay let's go further what",
"if we wanted to predict word for using",
"words one and two and three it's",
"basically the same picture as last time",
"except with one extra input and one",
"extra circle but I want to point",
"something out which is each time we go",
"from rectangle to circle we're doing the",
"same thing we're doing an embedding",
"which is just a particular kind of",
"matrix model play where you have a",
"one-pot encoded input each time we go",
"from circle to circle we're basically",
"taking one piece of hidden state run",
"through activations and turning it into",
"another set of activations by saying",
"we're now at the next word and then when",
"we go from circle to triangle we're",
"doing something else again which is",
"we're saying let's convert the hidden",
"state these activations into an output",
"so it would make sense so you can see",
"I've colored each of those errors",
"differently so each of those arrows",
"should probably use the same weight",
"matrix because it's doing the same thing",
"so why would you have a different set of",
"embeddings for each word or a different",
"set of a different matrix to multiply by",
"to go from this hidden state to this",
"hidden state versus this one okay so",
"this is what we're going to build so",
"we're now going to jump into human",
"numbers which is less than seven human",
"numbers and this is a data set that I",
"created which literally just contains",
"all the numbers from one to nine",
"thousand hundred 99 written out in",
"English okay and we're going to try and",
"create a language model that can predict",
"the next word in this document it's just",
"a toy example for this purpose so in",
"this case we only have one document and",
"that one document is the list of numbers",
"so we can use a text list to create an",
"item list with text in for the training",
"and the validation in this case the",
"validation set is the numbers from 8,000",
"onwards and the training set is 1 to",
"8,000",
"we can combine them together turn that",
"into a data bunch so we only have one",
"document so train zero is the document",
"grab its dot txt that's how you grab the",
"contents of a text list and here are the",
"first 80 characters",
"it starts with a special token xx POS",
"anything starting with xx is a special",
"fast a token the OS is the beginning of",
"stream token it basically says this is",
"the start of a document and it's very",
"helpful in NLP to know when documents",
"start so that your models can learn to",
"recognize them",
"the validation set contains 13,000",
"tokens so 13 thousand words or",
"punctuation marks because everything",
"between spaces is a separate token the",
"batch size that we asked for was 64 and",
"then by default it uses something called",
"be PTT of 70 be PTT as we briefly",
"mentioned stands for back prop through",
"time that's the sequence links so for",
"each of our so we're there to our kind",
"of 64 document segments we split it up",
"into lists of 70 words that we look at",
"at one time so what we do is we grab",
"this for the validation set entire",
"string of 13,000 tokens and then we",
"split it into 64 roughly equal sized",
"sections okay people very very very",
"often think I'm saying something",
"different I did not say they are of",
"length 64 they're not they're 64 equally",
"sized roughly segments so we take the",
"first one 64th of the document piece one",
"second 64 space - okay and then for each",
"of those",
"1/64 of the document we then split those",
"into pieces of length 70 so each batch",
"right so let's now say okay for those",
"13,000 tokens how many batches are there",
"well divide by batch size and divide by",
"70",
"there's about 2.9 batches so three",
"there's going to be three batches so",
"let's grab an iterator for a data loader",
"grab one two three batches the X and the",
"y and let's add up the number of",
"elements and we get back slightly less",
"than this because there's a little bit",
"left over at the end that doesn't quite",
"make up a full batch okay so this is the",
"kind of stuff you should play around",
"with a lot lots of shapes and sizes and",
"stuff and iterators as you can see it's",
"95 by 64 I claimed it was going to be 70",
"by 64 that's because our data loader for",
"language models slightly randomizes the",
"PTT just to give you a bit more kind of",
"shuffling get bit more randomization it",
"helps the model and so here you can see",
"the first batch of X yeah remember we've",
"numeric alized all these and here's the",
"first batch of Y and you'll see here",
"this is 2 18 10 11 8 this is 18 10 11 8",
"so this one is offset by 1 from here",
"because that's what we want to do with a",
"language model we want to predict the",
"next word so after two should come up 18",
"and after 18 should come 10 right you",
"can grab the vocab for this data set and",
"a vocab has a text defy so if we call it",
"exactly the same look at the same thing",
"but with text fi that'll just look it up",
"in the vocab so here you can see xx POS",
"8001 where else in the why there's no xx",
"POS is just 8,000 one so after xx POS is",
"8 after 8 these thousand after thousand",
"is 1 okay",
"and so then after we get 8023 comes x 2",
"and look at this we're always looking at",
"column 0 so this is the first batch the",
"first mini batch comes 8024 and then X 3",
"all the way up to 8000 40 right and so",
"then we can go right back to the start",
"but look at batch 1 right so index 1",
"which is better number 2 and now we can",
"continue",
"a slight skip from 8042 8000 46 that's",
"because the last mini batch wasn't quite",
"complete so what this means is that",
"every mini batch so every yeah every",
"mini batch joins up with a previous mini",
"batch you know so you can go straight",
"from x1 0 to X 2 0 to continue 8023 8024",
"right and so he took the same thing for",
"colon comma 1 you'll also see they join",
"up so all the mini batches join up so",
"that's the data we can do show better to",
"see it and here is our model which is",
"doing this right so here is this is just",
"the code copied over right so it content",
"contains one embedding ie the green",
"arrow one hidden to hidden brown arrow",
"layer and one hidden to output right so",
"each colored arrow has a single matrix",
"okay and so that in the forward paths we",
"take our first input X 0 and put it",
"through input to hidden the green arrow",
"okay create our first set of activations",
"which we call H assuming that there is",
"the second word because like sometimes",
"we might be at the end of a batch where",
"there isn't a second word assume there",
"is a second word then we would add to H",
"the result of x1 put through the green",
"arrow",
"remember that's H and then we would say",
"okay our new H is the result of those",
"two add it together put through our",
"hidden to hidden orange arrow and then",
"rel you then batch them and then for the",
"second word do exactly the same thing",
"and then finally blue arrow put it",
"through H oh all right so that's how we",
"convert our diagram to code so now",
"new here at all so now let's to okay and",
"and just you know so we can check that",
"in the learner and we can train it 46",
"percent okay let's take this code and",
"recognize it's pretty awful",
"there's a lot of duplicate code and as",
"coders when we see duplicate code what",
"do we do we refactor so we should",
"reflect to this into a loop so here we",
"are we've refactored it into a loop so",
"now we're going for each X I and X and",
"doing it in the loop guess what",
"that's an hour and in an hour it in is",
"just a refactoring it's not anything new",
"this is now a narrative okay and let's",
"refactor our diagram from this to this",
"this is the same diagram okay but I've",
"just replaced it with my loop does the",
"same thing so here here it is it's got",
"exactly the same unit literally exactly",
"the same just popped a loop here before",
"I start I just have to make sure that",
"I've got some you know a bunch of zeros",
"to add to and of course I get exactly",
"the same result when I train it okay so",
"next thing that you might think then and",
"one nice thing about the loop though is",
"now this will work even if I'm not",
"predicting the fourth word from the",
"previous three but the ninth word to the",
"previous eight it'll work for any",
"arbitrarily length long sequence just",
"nice so let's up the BP TT to 20 since",
"we can now and let's say I'll say okay",
"instead instead of just predicting the",
"length word from the previous n minus",
"one let's try to predict the second word",
"from the first in the third from the",
"second and the fourth from the third and",
"so forth right because previously like",
"look at our loss function previously we",
"were comparing the result of our model",
"to just the last word of the sequence it",
"is very wasteful because there's a lot",
"of words in the sequence so let's",
"compare every word in X to every word in",
"Y so to do that we need to change this",
"so it's not just one triangle at the end",
"of the loop",
"but the triangle is inside this right so",
"that in other words after every loop",
"predict loop predict loop predict so",
"here's this code it's the same as the",
"previous code but now I've created an",
"array and every time I go through the",
"loop I append HOH to the array so now",
"for n inputs I create n outputs so I'm",
"predicting after every word previously I",
"had 46% now I have 40% why is it worse",
"well it's worse because now like when",
"I'm trying to predict the second word I",
"only have one word of state to use okay",
"so like and when I'm looking at the",
"third word I only have two words of",
"state to use so it's a much harder",
"problem for it to solve so the obvious",
"way to fix this then would you know the",
"key problem is here I go H equals torch",
"zeros like I reset my state zero every",
"time I start another be PTT sequence",
"well let's not do that let's keep H",
"right and we can because remember each",
"batch connects to the previous batch",
"it's not shuffled like happens in you",
"know image classification so let's take",
"this exact model and replicate it again",
"but let's move the creation of H into",
"the constructor okay there it is so it's",
"now self dot H okay and so this is now",
"exactly the same code but at the end",
"let's put the new H back into self dot H",
"okay so it's now doing the same thing",
"but it's not throwing away that state",
"and so therefore now we actually get",
"above the original we get all the way up",
"to 54 percent accuracy so this is what a",
"real iron tin looks like they you know",
"you you always want to keep that state",
"right but just keep remembering there's",
"nothing different about an RNA and it's",
"a totally normal fully connected neural",
"net okay it's just that you've got a",
"loop you refactored what you could do",
"though is at the end of your every loop",
"you",
"could not just spit out an output but",
"you could spit it out into another",
"errand in so you have an errand in going",
"into an errand and that's nice because",
"we've now got more layers of computation",
"you would expect that to work better",
"well to get there let's do some more",
"refactoring so let's take this code and",
"replace it with the equivalent built in",
"pipe torch code which is you just say",
"that so n n dot R and n basically says",
"do the loop for me okay we've still got",
"the same embedding we've still got the",
"same output still got the same batch",
"norm we've still got the same",
"initialization of H but we just got rid",
"of the loop so one of the nice things",
"about our a10 is that you can now say",
"how many layers you want so this is the",
"same accuracy of course so here I'm",
"going to do it with two layers but",
"here's the thing when you think about",
"this right think about it without the",
"loop it looks like this right",
"it's like keeps on going and we've got a",
"BP GT of 20 so there's 20 layers of this",
"and we know from that visualizing the",
"lost landscapes paper that deep networks",
"have awful bumpy lost surfaces so when",
"you start creating long timescales and",
"multiple layers these things get",
"impossible to train so there's a few",
"tricks you can do one thing is you can",
"add skip connections of course about",
"what people normally do is instead they",
"put inside instead of just adding these",
"together they actually use a little mini",
"neural net to decide how much of the",
"green arrow to keep and how much of the",
"orange arrow to keep and when you do",
"that you get something that's either",
"called giu or an L STM depending on the",
"details of that little neural net and",
"we'll learn about the details of those",
"neural nets in part 2 they really don't",
"matter though frankly so we can now say",
"let's create a GI u instead it says just",
"like what we had before but it'll handle",
"longer sequences in deeper networks",
"let's use two layers bump and we're up",
"to 75%",
"okay so that's how it ends and the main",
"reason what it's show it to you was to",
"remove the the last remaining piece of",
"magic and this is one of like the least",
"magical things we have in deep learning",
"it's just a refactored fully connected",
"Network",
"so don't let our own ends ever ever put",
"you off and with this approach where you",
"basically have a sequence of n inputs",
"and a sequence of n outputs which we've",
"been using for language modeling you can",
"use that for other tasks right for",
"example the sequence of outputs could be",
"for every word there could be something",
"saying is there something that I is",
"sensitive and I want to anonymize or not",
"you know so like is this private private",
"data or not or it could be a part of",
"speech tag for that word or it could be",
"something saying you know how should",
"that word be formatted or whatever and",
"so these are called sequence labeling",
"tasks and so you can use this same",
"approach for pretty much any sequence",
"labeling task or you can do what I did",
"in the earlier lesson which is once you",
"finish building your language model you",
"can throw away their kind of this h0 bit",
"and instead pop their a standard",
"classification head and then you can now",
"do NLP classification which as you saw",
"earlier what if you stayed at the out",
"results even on long documents so this",
"is a super valuable technique and not",
"remotely metrical okay so that's it",
"right that's that's deep learning or at",
"least you know the kind of the practical",
"pieces from my point of view having",
"watched this one time you won't get it",
"all and I don't recommend that you do",
"watch this so slowly that you get it all",
"the first time but you go back and look",
"at it again take your time and there'll",
"be bits that you go like oh now I see",
"what he's saying and",
"you'll be able to like implement things",
"you couldn't implement before and you'll",
"be able to dig in more than you thought",
"so like definitely go back and do it",
"again and as you do write rack code not",
"just for yourself but put it on github",
"right it doesn't matter if you think",
"it's great code or not you know the fact",
"that you're writing code and sharing it",
"is impressive and the feedback you'll",
"get if you tell people on the forum you",
"know hey I wrote this code it's not",
"great but you know it's my first effort",
"anything you see jump out at you people",
"will say like oh that bit was done well",
"hey but you do know for this bit you",
"could have used this library and",
"Safety's in time you'll learn a lot by",
"interacting with your peers as you've",
"noticed I've started introducing more",
"and more papers now part two will be a",
"lot of papers and so it's a good time to",
"start reading some of the papers that",
"have been introduced in this in this",
"section all the bits that say like",
"derivation and theorems and lemmas you",
"can skip them I do they add almost",
"nothing to your understanding of",
"practical tech learning right",
"but the bits that say like you know why",
"are we solving this problem and what are",
"the results and so forth really",
"interesting and then you know try and",
"write English prose not know English",
"prose that you want to be read by Geoff",
"Hinton and Yann LeCun",
"but English prose that you want to be",
"written read by you as of six months ago",
"right because there's a lot more people",
"in the audience of you as well six",
"months ago then there is of Geoffrey",
"Hinton Danielle Loughran right that's",
"that's the person you best understand",
"you know what they need right go and get",
"help and help others",
"tell us about your success stories but",
"perhaps the most important one is get",
"together with others that people's",
"learning works much better if you've got",
"that social experience so start a book",
"club get involved in meetups create",
"study groups and build things right and",
"again they it doesn't have to be amazing",
"like just build something that do you",
"think the world would be a little bit",
"better if that what existed or you think",
"it would be kind of slightly delightful",
"to your two-year-old to see that thing",
"or you just want to show it to your",
"brother the next time they come around",
"to see what you're doing whatever right",
"like just finish something you know",
"finish something and then try and make",
"it a bit better so for example something",
"I just saw this afternoon is the Elan",
"Elan musk twitch ener ATAR okay so",
"looking at lots of older tweets creating",
"a language model from from Elon Musk and",
"then creating new tweets such as",
"humanity will also have an option to",
"publish on its own journey as an only in",
"civilization it will always like all",
"human being Mars is no longer possible",
"hey I will definitely be the Central",
"Intelligence Agency okay so this is",
"great I love this and I love that Dave",
"Smith wrote and said these are my",
"first-ever commits thanks for teaching a",
"finance guy how to build an app in 8",
"weeks right so you know I think this is",
"awesome and I think like clearly a lot",
"of care and passion is being put into",
"this project you know will it",
"systematically change the future",
"direction of society as a whole maybe",
"not you know but maybe Elon will look at",
"this and think like oh you know like I",
"maybe I need to rethink my method of",
"pros I don't know I think it's I think",
"it's great",
"and so yeah create something put it out",
"there put a bit of yourself into it or",
"get involved in fast AI the first thing",
"I project there's a lot going on you",
"know you can help with documentation and",
"tests which might sound boring but you'd",
"be surprised how incredibly not boring",
"it is to like take a piece of code that",
"hasn't been properly documented and",
"research it and understand it and ask",
"Sylvia and I on the forum what's going",
"on why did you write it this way we'll",
"send you off to the papers that we were",
"implementing you know writing a test",
"requires deeply understanding that part",
"of the machine learning world to really",
"understand how it's meant to work so",
"that's what was interesting stairs",
"Backman has created this nice dev",
"projects index which you can like go",
"onto the forum in the FASTA I dev",
"section and find",
"get to the dev project section and find",
"like give some stuff going on that you",
"might want to get involved in or maybe",
"there's stuff you want to exist you",
"could add your own create a study group",
"you know Deena's already created a study",
"group for San Francisco starting in",
"January this is how easy it is to create",
"a study group right go on the forum find",
"your little time zone subcategory and",
"add a post thing let's create a study",
"group okay but make sure you you know",
"give people like a little Google sheet",
"to sign up some way to actually do",
"something you know a great example is",
"Pierre who's been doing a fantastic job",
"in Brazil of running study groups for",
"the last couple of classes of the course",
"and you know he keeps posting these",
"pictures of people having a good time",
"and learning deep learning together",
"creating wiki's together creating",
"projects together great experience and",
"then come back for part two",
"right where we'll be looking at all of",
"this interesting stuff in particular",
"going deep into the first day our code",
"base to understand how did we build it",
"exactly will actually go through as we",
"were building it we created notebooks of",
"like here where here's where we were at",
"each stage so we're actually going to",
"see the software development process",
"itself we'll talk about the process of",
"doing research how to read academic",
"papers how to turn math into code and",
"then a whole bunch of additional types",
"of models that we haven't seen yet so",
"it'll be kind of like going beyond",
"practical deep learning into actually",
"cutting edge research so we've got five",
"minutes to take some questions we had an",
"AMA going on online and so we're going",
"to have time for a couple of the",
"highest-ranked AMA questions from the",
"community and the first one is by",
"Jeremy's request although it's not the",
"highest ranked what's your typical day",
"like how do you manage your time across",
"so many things that you do yeah I",
"thought that I hear that all the time so",
"I thought I should answer it and I think",
"I got a few votes because I think people",
"who come to our study group",
"are always shocked at how disorganized",
"and incompetent I am and so I often get",
"people saying like Oh",
"Wow I thought you were like this deep",
"learning role model and I'll get to see",
"how to be like you and now I'm not sure",
"what it'd be like you at all",
"so yeah it's for me it's all about just",
"having a good time with it I never",
"really have many plans I just try to",
"finish what I start",
"if you're not having fun with it it's",
"really really hard to continue because",
"there's a lot of frustration in deep",
"learning because it's not like writing a",
"web app where it's like you know",
"authentication check you know back-end",
"service watchdog check okay user",
"credentials check you know like you just",
"you're making progress where else for",
"stuff like this",
"Dan stuff that we've been doing the last",
"couple of weeks it's just like it's not",
"working it's not working it's not",
"working no it also didn't work it also",
"didn't work until oh my god it's amazing",
"it's a cat that's kind of what it is",
"right so you don't get that regular",
"feedback so yeah you know you got to",
"have fun with it",
"and so so Mike yeah my day is kind of",
"you know and the other thing I do I'll",
"say I don't I don't do any meetings",
"I don't do phone calls I don't do",
"coffees I don't watch TV at okay",
"computer games I spend a lot of time",
"with my family a lot of time exercising",
"and a lot of time reading and coding and",
"doing things I like so you know I think",
"the you know the main thing is just",
"finish finish something like properly",
"finish it so when you get to that point",
"where you think 80% of the way through",
"but you haven't quite created to read me",
"yet and the install process is still a",
"bit clunky and you know this is what 99%",
"of github projects look like you'll see",
"the readme says to do you know complete",
"baseline experiments document blah blah",
"blah it's like don't be that person like",
"just do something properly and finish it",
"and maybe get some other people around",
"you to work with you so that you're all",
"doing it together and you know get it",
"done",
"what are the up-and-coming deep learning",
"machine learning things that you are",
"most excited about also you've mentioned",
"last year that you are not a believer in",
"reinforcement learning do you still feel",
"the same way yeah I still feel exactly",
"the same way as I did three years ago",
"when we started this which is it's all",
"about transfer learning its",
"underappreciated its under-researched",
"every time we put transfer learning into",
"anything we make it much better you know",
"our academic paper on transfer learning",
"for NLP has you know helped be one piece",
"of kind of changing the direction of NLP",
"this year it made it all the way to the",
"New York Times just a stupid obvious",
"little thing that we threw together so I",
"remain excited about that I remain",
"unexcited about reinforcement learning",
"for most things I don't see it used by",
"normal people for normal things for",
"nearly anything it's an incredibly",
"inefficient way to solve problems which",
"often solved more simply and more",
"quickly in other ways probably has a",
"maybe a role in the world but a limited",
"one and not in most people's day-to-day",
"work for someone planning to take part -",
"and 2019 what would you recommend doing",
"learning practicing until the part two",
"course starts just code",
"yeah just code all the time I know it's",
"perfectly possible I hear from people",
"who get to this point of the course and",
"they haven't actually written any code",
"yet and if that's you it's okay you know",
"you've just go through and do it again",
"and this time do code and and look at",
"the input the shapes of your inputs and",
"look at your outputs to make sure you",
"know how to grab a mini batch and look",
"at its main standard deviation and plot",
"it and you know there's this so much",
"material that we've covered if you can",
"get to a point where you can you know",
"rebuild those notebooks from scratch",
"without too much cheating when I say",
"from scratch I mean using the first day",
"library not from scratch from scratch",
"you know you're you're be in the top",
"echelon of practitioners because you'll",
"be able to do all of these things",
"yourself and that's really really rare",
"and that'll put you in a great position",
"for part two nine o'clock be honest do",
"one more and where do you see the fast",
"day Iowa library going in the future say",
"in five years well like I said I don't",
"make plans I just I just piss around so",
"I mean our only plan for fast AI you",
"know as an you know organization is to",
"make deep learning accessible as a tool",
"for normal people to use for normal",
"stuff",
"so as long as we need to code we failed",
"at that so the big goal you know cuz",
"ninety-nine point eight percent of the",
"world can't code so the main goal would",
"be to get to a point where it's not a",
"library but it's a piece of software",
"that doesn't require a code it certainly",
"shouldn't require a goddamn lengthy hard",
"working course like this one you know so",
"I want to get rid of the course I want",
"to get rid of the code I want to make it",
"so you can just do useful stuff quickly",
"and and easily so that's that's",
"maybe five years yeah maybe longer all",
"right well I hope to see you all back",
"here for part two",
"131:55": "thank you"
}