{
"all right welcome to lesson 6 where",
"we're going to do a deep dive into",
"computer vision convolutional neural",
"networks what is a convolution and we're",
"also going to learn the final",
"regularization tricks after last lesson",
"learning about weight decay and /lt",
"regularization",
"I want to start by showing you something",
"that I'm really excited about and I've",
"had a small hand and helping to to",
"create for those of you that saw my talk",
"on ted.com you might have noticed this",
"really interesting demo that we did",
"about four years ago showing a way to",
"quickly build models with unlabeled data",
"it's been four years but we're finally",
"at a point where we're we're we're ready",
"to put this out in the world and let",
"people use it and the first people we're",
"going to let use it are you folks so the",
"company is called platform delay I and",
"the reason I'm mentioning it here is",
"that it's going to let you create models",
"on different types of data sets to what",
"you can do now that is to say data sets",
"that you don't have labels for yet we're",
"actually going to help you label them so",
"this is the first time this has been",
"shown before so I'm pretty thrilled",
"about it and let me give you a quick",
"demo when you so if you'd go to platform",
"AI and choose get started you'll be able",
"to create a new project and if you",
"create a new project you can either",
"upload your own images uploading it at",
"500 or so works pretty well you can",
"upload a few thousand but you know to",
"start upload 500 or so they all have to",
"be in a single folder and so we're",
"assuming that you've got a whole bunch",
"of images that you haven't got any",
"labels for or you can start with one of",
"the existing collections if you want to",
"play around so I've started with the",
"cars",
"collection kind of going back to what we",
"did four years ago and so this is what",
"happens when you first go into",
"platformer AI and look at the collection",
"of images you're uploaded a random",
"sample of them will appear on the screen",
"and as you'll recognize probably they",
"are projected from a deep learning space",
"into a 2d space using a pre trained",
"model and for this initial version it's",
"an image net model we're using as things",
"move along",
"we'll be adding more and more pre train",
"models and what I'm going to do is I",
"want to add labels to this data set",
"representing which angle a photo of the",
"car was taken from which is something",
"that actually image that's going to be",
"really bad at isn't it because image net",
"has learnt to recognize the difference",
"between cars versus bicycles and image",
"net knows that the angle you take a",
"photo on actually doesn't matter so we",
"want to try and create labels using the",
"kind of thing that actually image net",
"specifically learn to ignore so the",
"projection that you see we can click",
"these layer buttons at the top to switch",
"to user projection using a different",
"layer of the neural net right and so",
"here's the last layer which is going to",
"be a total waste of time for us because",
"it's really going to be projecting",
"things based on what kind of thing it",
"thinks it is and the first layer is",
"probably going to be a waste of time for",
"us as well because there's very little",
"interesting semantic content there but",
"if I go into the middle in layer 3 we",
"may well be able to find some some some",
"differences there so then what you can",
"do is you can click on the projection",
"button here and you can actually just",
"press up and down rather than just",
"pressing the the arrows at the top to",
"switch between projections or left and",
"right so switch between layers and what",
"you can do is you can basically look",
"around until you notice that there's a",
"projection which is kind of separated",
"out things you're interested in and so",
"this one actually I notice that it's got",
"a whole bunch of",
"cars that are kind of from the top front",
"front right over here okay so if we zoom",
"in a little bit we can double check",
"because like yeah that looks pretty good",
"they're all kind of front right so we",
"can click on here to go to selection",
"mode and we can cut a grab a few and",
"then you should check and so what we're",
"doing here is we're trying to take",
"advantage of the combination of human",
"plus machine the the machine is pretty",
"good at quickly doing calculations but",
"as a human I'm pretty good at looking at",
"a lot of things at once and seeing the",
"odd one out so in this case I'm looking",
"for cars that aren't front right and so",
"by laying the one on in front of me I",
"can do that really quickly it's like",
"okay definitely that one so just click",
"on the ones that you don't want all",
"right it's all good so then you can just",
"go back and so then what you can do is",
"you can either put them into a new",
"category but I can create a new label or",
"you can click on one of the existing",
"ones so before I came I just created a",
"few so here's friend right so I just",
"click on it here there we go okay and so",
"that's the basic idea is that you kind",
"of keep flicking through different",
"layers or projections to try and find",
"groups that represent the things you're",
"interested in and then over time you'll",
"start to realize that there are some",
"things that are a little bit harder so",
"for example I'm having trouble finding",
"sides so what I can do is I can see over",
"here there's a few sides so I can zoom",
"in here and click on a couple of them",
"like this one and this one that one that",
"one okay I mean I'll say find similar",
"and so this is going to basically look",
"in that that projection space and not",
"just at the images that are currently",
"displayed but all of the images that you",
"uploaded and hopefully I might be able",
"to label now a few more side images at",
"that point so it's going through and",
"checking you know all of the images that",
"you uploaded to see if any of them have",
"projections in this space which similar",
"to the ones I've selected and hopefully",
"we'll find a few more of what I'm",
"interested in",
"okay so now if I want to try to find a",
"projection that separates the sides from",
"the front right I can click on each of",
"those two and then over here this button",
"is now called switch to the projection",
"that maximizes the distance between the",
"labels so now what this is going to do",
"is try and find the best projection that",
"separates out those classes and so the",
"goal here is to you know help me",
"visually inspect and quickly find a",
"bunch of things that I can use to label",
"so like they're the kind of the the key",
"features and it's done a good job you",
"can see down here we've now got a whole",
"bunch of sides which I can now grab",
"because I was having a lot of trouble",
"finding them before and it's always",
"worth double-checking it's kind of",
"interesting to see how the neural Nets",
"behave like there seems to be more",
"sports cars in this group than average",
"as well so it's kind of found side",
"angles of sports cars so that's kind of",
"interesting so then I can click all",
"right so I've got those four an arrow",
"clicks side and there we go",
"so once you've done that a few times I",
"find if you've got you know a hundred or",
"so labels you can then click on the",
"train model button and it'll take a",
"couple of minutes and come back and show",
"you your train model and after it's",
"trained which I did it on a smaller",
"number of labels earlier you can then",
"switch this very opacity button and",
"it'll actually kind of fade out the ones",
"that are already predicted pretty well",
"and it'll also give you a estimate as to",
"how accurate it thinks the model is the",
"main reason I mentioned this for you is",
"that so that you can now click the",
"download button and it'll download the",
"predictions which is what we hope will",
"be interesting to most people but what I",
"think will be interesting to you as deep",
"learning students is it'll download your",
"labels so now you can use that labeled",
"subset of data along with the unlabeled",
"set that you haven't labeled yet to see",
"if you can you know see if you can build",
"a better model and platform a I stand",
"for you see if you can use that initial",
"set of data to kind of get going",
"creating models and stuff which you",
"weren't able to label before",
"clearly there are some things that this",
"systems better that than others",
"for things that require you know really",
"zooming in closely and taking a very",
"very close inspection this isn't going",
"to work very well but is really designed",
"for things that the human eye can kind",
"of pick up fairly readily but we'd love",
"to get feedback as well and you can",
"click on the Help button to get feedback",
"give feedback and also there's a",
"platform AI discussion topic in our",
"forum where so are shocked if you can",
"stand up our checks the CEO of the",
"company he'll be there helping out",
"answering questions and so forth so yeah",
"I hope people find that useful it's been",
"many years getting to this point and I'm",
"glad we're we're finally there",
"okay so one of the reasons I wanted to",
"mention this today is that we're going",
"to be doing a big dive into convolutions",
"later in this lesson so I'm going to",
"circle back to this to try and explain a",
"little bit more about how that is",
"working under the hood and give you a",
"kind of a sense of what's what's going",
"on but before we do we have to finish",
"off last week's discussion of",
"regularization and so we were talking",
"about regularization specifically in the",
"context of the tabular learner because",
"the tabular learner",
"this was the forward method sorry this",
"is the init method in the tabular",
"learner and our goal was to understand",
"everything here and we're not quite",
"there yet",
"last week we were looking at the adult",
"data set which is a really simple kind",
"of over simple data set that's just a",
"toy purposes so this lit week let's look",
"at a data set that's much more",
"interesting a kegel competition data set",
"so we know kind of what the the best in",
"the world",
"and you know care girl competition was",
"results tend to be much harder to beat",
"than academic state-of-the-art results",
"tend to be because a lot more people",
"work on cowgirl competitions than most",
"academic data sets so it's a really good",
"challenge to try and do well on a",
"caracal competition data set so this one",
"the rossmann data set is if they've got",
"three thousand drugs",
"in Europe and you're trying to predict",
"how many products they're going to sell",
"in the next couple of weeks so one of",
"the interesting things about this is",
"that the test set for this is from a",
"time period that is more recent than the",
"training set and this is really common",
"right if you want to predict things",
"there's no point predicting things that",
"are in the middle of your training set",
"you want to predict things in the future",
"another interesting thing about it is",
"the evaluation metric they provided is",
"the root mean squared percent error so",
"this is just a normal root mean squared",
"error except we go actual minus",
"prediction divided by actual so in other",
"words it's the percent error that we're",
"taking the root mean squared of so",
"there's a couple of interesting features",
"always interesting to look at the",
"leaderboard",
"so the leaderboard the winner was 0.1",
"the paper that we've roughly replicated",
"was point 105 106 and 10th place out of",
"3,000 was 0.11 ish bit less all right so",
"we're gonna skip over a little bit which",
"is that the data that was provided here",
"was they provided a small number of",
"files but they also let competitors",
"provide additional external data as long",
"as they shared it with all the",
"competitors and so in practice the data",
"set we're going to use contains account",
"remember six or seven tables the way",
"that you join tables and stuff isn't",
"really part of a deep learning course so",
"I'm going to skip over it and instead",
"I'm going to refer you to introduction",
"to machine learning for coders which",
"will take you step-by-step through the",
"data preparation for this we've provided",
"it for you",
"we've provided it for you in Russman",
"data clean so you'll see the whole",
"process there and so you'll need to run",
"through that notebook to create these",
"pickle files that we read here can you",
"see this in the back okay",
"I just want to mention one particularly",
"interesting part of the rossmann data",
"clean",
"notebook which is you'll see there's",
"something that says add date part and I",
"wanted to explain what's going on here",
"I've been mentioning for a while that",
"we're going to look at time series and",
"pretty much everybody who I've spoken to",
"about it has assumed that I'm going to",
"do some kind of recurrent neural network",
"but I'm not interestingly the kind of",
"the main academic group that studies",
"time series is econometrics and but they",
"tend to study one very specific kind of",
"time series which is where the only data",
"you have is a sequence of time points of",
"one thing like that's the only thing you",
"have is one sequence in real life that's",
"almost never the case normally you know",
"if we would have some information about",
"the store that that represents or the",
"people that it represents we'd have",
"metadata we'd have sequences of other",
"things measured at similar time periods",
"or different time periods and so most of",
"the time I find in practice the the",
"state-of-the-art results when it comes",
"to competitions on kind of more",
"real-world data sets",
"don't gender you it's recurrent neural",
"networks but instead they tend to take",
"it's a time piece which in this case it",
"was a date we were given in the data and",
"they add a whole bunch of metadata so in",
"our case for example we've added day of",
"week so we were given a date right we've",
"had a day of week year month week of",
"year day of month day of week day of",
"year and then a bunch of boolean ziz at",
"the month start or end quarter year",
"start or end elapsed time since 1970 so",
"forth if you run this one function at",
"date part and pass it a date it'll add",
"all of these columns to your data set",
"for you and so what that means is that",
"let's take a very reasonable example",
"purchasing behavior probably changes on",
"payday payday might be the fifteenth of",
"the month so if you have a thing here",
"called this is day of month here right",
"then it'll be able to recognize every",
"time something is a fifteen there and",
"associated it with a higher in this case",
"embedding matrix value",
"but so this way it basically the the you",
"know we can't expect a neural net to do",
"all of our feature engineering for us we",
"can expect it to kind of find",
"nonlinearities and interactions and",
"stuff like that but for something like",
"taking a date like this and figuring out",
"that the fifteenth of the month is",
"something when interesting things happen",
"it's much better if we can provide that",
"information for it so this is a really",
"useful function to use and once you've",
"done this you can treat many kinds of",
"time-series problems as regular tabular",
"problems I say many kinds not all you",
"know if there's very complex kind of",
"state involved in a time series such as",
"you know equity trading or something",
"like that this probably won't be the",
"case or this won't be the only thing you",
"need but in this case it'll get us a",
"really good result and it's in practice",
"most of the time I find this works well",
"tabular data is normally in pandas so we",
"just stored them as standard Python",
"pickerel files we can read them in we",
"can take a look at the first five",
"records and so the key thing here is",
"that we're trying to on a particular",
"date for a particular store ID we want",
"to predict the number of sales sales is",
"the dependent variable so the first",
"thing I'm going to show you is something",
"called pre processes you've already",
"learned about transforms transforms are",
"bits of code that run every time",
"something is grabbed from a data set and",
"so it's really good for data",
"augmentation that we'll learn about",
"today which is that it's going to get a",
"different random value every time it's",
"sampled pre processes are like",
"transforms but they're a little bit",
"different which is that they run once",
"before you do any training and really",
"importantly they run once on the",
"training set and then any kind of State",
"or metadata that's created is then",
"shared with the validation and test set",
"let me give you an example when we've",
"been doing image recognition and we've",
"had a set of classes to like all the",
"different pet breeds and they've been",
"turned into numbers the thing that's",
"actually doing that for us",
"is a preprocessor that's being created",
"in the background so that makes sure",
"that the classes for the training set",
"are the same as the classes for the",
"validation and the classes of the test",
"set so we're going to do something very",
"similar here for example if we create a",
"little small subset of a data for",
"playing with this is a really good idea",
"when you start with a new data set so",
"I've just grabbed two thousand IDs at",
"random okay and then I'm just going to",
"grab a little training set in a little",
"test set half and half of those 2,000",
"IDs and it's going to grab five columns",
"okay and then we can just play around",
"with this nice and easy so here's the",
"first few of those from the training set",
"and you can see one of them is called",
"promo interval and it has these strings",
"and sometimes it's missing in pandas",
"missing is na M so the first",
"preprocessor I'll show you is category",
"fee and category does basically the same",
"thing that that classes thing for image",
"recognition does for a dependent",
"variable it's going to take these",
"strings it's going to find all of the",
"possible unique values of it and it's",
"going to create a list of them and then",
"it's going to turn the strings into",
"numbers so if I call it on my training",
"set that'll create categories there and",
"then I call it on my test set passing in",
"testicles true that makes sure it's",
"going to use the same categories that I",
"had before and now when I say dot head",
"it looks exactly the same and that's",
"because pandas has turned this into a",
"categorical variable which internally is",
"storing numbers but externally is",
"showing me the strings but I can look",
"inside promo interval to look at the cat",
"categories this is all standard pandas",
"here to show me a list of all of them",
"what we would call classes in first day",
"a or would be called just categories in",
"pandas and so then if I look at the cat",
"codes you can see here this list here is",
"the numbers that are actually stored",
"minus 1 minus 1 1 minus 1 1 right why am",
"boy one of these minus ones the minus",
"ones represent ni n they",
"missing so pandas uses the special code",
"- one to be mean missing now as you know",
"these are going to end up in an",
"embedding matrix and we can't look up",
"item -1 and an embedding matrix so",
"internally in first AI we add one to all",
"of these another useful preprocessor is",
"fixed missing and so again you can call",
"it on the data frame you can call on the",
"test passing in testicles true and this",
"will create for everything that's",
"missing anything that has a missing",
"value it'll create an additional column",
"with the column name underscore na so",
"competition distance underscore na and",
"it will set it for true for any time",
"that was missing and then what we do is",
"we replace competition distance with the",
"median for those why do we do this well",
"because very commonly the fact that",
"something's missing is of itself",
"interesting like you know it turns out",
"the fact that this is missing helps you",
"predict your outcome alright so we've",
"certainly want to keep that information",
"in a convenient boolean column so that",
"our deep learning model can use it to",
"predict things but then we need",
"competition distance to be a continuous",
"variable so we can use it in the",
"continuous variable part of our model so",
"we can replace it with almost any number",
"right because if it turns out that the",
"missingness is important it can use the",
"interaction of competition distance na",
"and competition distance to make",
"predictions so that's what fixed missing",
"does you don't have to manually call pre",
"processes yourself when you call any",
"kind of item list creating creator you",
"can pass in a list of pre processes",
"which you can create like this ok so if",
"this is saying ok I want to feel missing",
"I want to category Phi I want to",
"normalize so for continuous variables",
"it'll subtract the mean and divide by",
"the standard deviation to help a train",
"more easily and so you just say those",
"are my procs and then you can just pass",
"it in there and that's it and later on",
"you can go data export and it'll save",
"all the metadata for that data bunch so",
"you can later on load it in",
"knowing exactly what your category codes",
"are exactly what median values used for",
"replacing the missing values and exactly",
"what means and standard deviations you",
"normalize by okay so the main thing you",
"have to do if you want to create a data",
"bunch of tabular data is find out or",
"tell it what are your categorical",
"variables and what are your continuous",
"variables and as we discussed last week",
"briefly your categorical variables are",
"not just strings and things but also I",
"include things like day of week and",
"month and day of month even though",
"they're numbers and make them",
"categorical variables because for",
"example day of month I don't think it's",
"going to have a nice smooth curve I",
"think that the fifteenth of the month",
"and the first of the month and the 30th",
"of the month are probably going to have",
"different purchasing behavior to other",
"days of the month and so therefore if I",
"make it a categorical variable it's",
"going to end up creating an embedding",
"matrix and those different days of the",
"month can get different behaviors so",
"you've actually got to think carefully",
"about which things should be categorical",
"variables and on the whole if in doubt",
"and there are not too many levels in",
"your category that's called the",
"cardinality if your cardinality is not",
"too high I would have put it as a",
"categorical variable you can always try",
"an H and see which works best so our",
"final data frame that we're going to",
"pass in is going to be a training set",
"with the categorical variables and the",
"continuous variables and the dependent",
"variable and the date and the date we're",
"just going to use to create a validation",
"set where we go Stickley going to say",
"the validation set is going to be the",
"same number of Records at the end of the",
"time period that the test set is for",
"cattle and so that way we should be able",
"to validate our model nicely ok so now",
"we can create a tabular list so this is",
"our standard data block API that you've",
"seen a few times from a data frame",
"passing all of that information split it",
"into valid vs. train label it with a",
"dependent variable and here's something",
"I don't think you've seen before label",
"this is our dependent variable and as",
"you can see this is this is sales it's",
"not a float",
"it's an n64 if this was a float then",
"first day I would automatically know or",
"guess that you want to do a regression",
"okay but this is not a float it's an int",
"so first I was going to assume you want",
"to do a classification so when we label",
"it we have to tell it that the clasp of",
"the labels we want is a list of floats",
"okay not a list of categories which",
"would otherwise be the default so this",
"is the thing that's going to",
"automatically turn this into a",
"regression problem for us and then we",
"create a data bunch so I wanted to",
"remind you again about dock which is how",
"we find out more information about this",
"stuff in this case all of the labeling",
"functions in the data blocks API will",
"pass on any keywords they don't",
"recognize to the label class so one of",
"the things I've passed in here is log",
"and so that's actually going to end up",
"in float list and so if I go dock float",
"list I can see a summary okay and I can",
"even jump into the full documentation",
"and it shows me here that log is",
"something which if true it's going to",
"take the logarithm of my dependent",
"variable why am i doing that so this is",
"the thing that's actually going to",
"automatically take the log of my way the",
"reason I'm doing that is because as I",
"mentioned before the evaluation metric",
"is root mean squared percentage error",
"and first I'd either fastener iron or PI",
"torch has a root mean squared percentage",
"error loss function built in I don't",
"even know if such a loss function would",
"work super well but if you want to spend",
"the time thinking about it you'll notice",
"that this ratio if you first take the",
"log of Y and Y hat then becomes a",
"difference rather than the ratio so in",
"other words if you take the log of Y",
"then if this becomes root means great",
"error so that's what we're going to do",
"we're going to take the log of Y and",
"then we're just going to use root mean",
"square error which is the default for a",
"regression",
"problems we won't even have to mention",
"it the reason that we have this year is",
"because this is so common right",
"basically anytime you're trying to",
"predict something that's like a",
"population or a dollar amount of sales",
"these kind of things tend to have long",
"tail distributions where you care more",
"about percentage differences and exact",
"differences you know absolute",
"differences so you're very much very",
"likely to want to do things with log",
"equals true and to measure the root mean",
"squared percent error we've learned",
"about the Y range before which is going",
"to use that sigmoid to help us get in",
"the right range because this time the Y",
"values are going to be taken the log of",
"it first we need to make sure that the Y",
"range we want is also the log so I'm",
"going to take the maximum of the sales",
"column I'm going to multiply it by a",
"little bit so that cuz remember how we",
"said it's nice if your range is a bit",
"wider than the range of the data and",
"then we're going to take the log and",
"that's going to be our maximum so then",
"our Y range will be from zero to a bit",
"more than the maximum so now we've got",
"our data bunch we can create a tabular",
"from it and then we have to pass in our",
"architecture and as we briefly discussed",
"for a tabular model our architecture is",
"literally the most basic fully connected",
"network just like we showed in this",
"picture it's an import matrix multiply",
"non-linearity matrix multiply",
"non-linearity matrix model play",
"non-linearity done okay what are the",
"interesting things about this is that",
"this competition is three years old but",
"I'm not aware of any significant",
"advances at least in terms of",
"architecture that would cause me to",
"choose something different to what the",
"third-placed folks did three years ago",
"we're still basically using simple fully",
"connected models for this problem now",
"the intermediate wait may",
"Trix is going to have to go from a 1000",
"activation input to a 500 activation",
"output which means it's going to have to",
"be 500,000 elements in that weight",
"matrix that's an awful lot for a data",
"set with only a few hundred thousand",
"rows so this is going to overfit and we",
"need to make sure it doesn't so one way",
"to make sure it does Bob",
"the way to make sure it doesn't is to",
"use regularization all right not to",
"reduce the number of parameters to use",
"regularization so one way to do that",
"will be to use weight decay which first",
"day I will use automatically and you can",
"vary it to something other than the",
"default if you wish it turns out in this",
"case we're going to want more",
"regularization and so we're going to",
"pass in something called peas this is",
"going to provide dropout and also this",
"one here M prop this is going to provide",
"embedding dropout so let's learn about",
"what is dropout but the short version is",
"dropout is a kind of regularization this",
"is the dropout paper nitish how do you",
"say this through vast Java",
"it was surest Ava's master's thesis",
"under Geoffrey Hinton and this picture",
"from the original paper is a really good",
"picture of what's going on this first",
"picture is a picture of a standard fully",
"connected Network it's a picture of this",
"and what each line shows is a",
"multiplication of an activation times a",
"weight and then when you've got multiple",
"arrows coming in that represents a sum",
"so this activation here is the sum of",
"all of these inputs times all of these",
"activations so that's what a normal",
"neural fully connected neural net looks",
"like for dropout we throw that away",
"we're at random we throw away some",
"percentage of the activations not the",
"weights right not the parameters",
"remember there's only two types of",
"number in a neural net parameters also",
"called weights kind of and activations",
"so we're going to throw away some",
"activation so you can see that when we",
"throw away this activation all of the",
"things that were connected to it are",
"gone too okay for each mini batch we",
"throw away a different subset of",
"activations how many do we throw away we",
"throw them of each one away with a",
"probability P a common value of P is 0.5",
"so what does that mean and you'll see in",
"this case not only have they deleted at",
"random some of these in hidden layers",
"but they've actually deleted some of the",
"inputs as well deleting the inputs is",
"pretty unusual normally we only delete",
"activations in the hidden layers so what",
"is this - well every time I have a mini",
"batch going through I at random throw",
"away some of the activations and then",
"the next mini batch I put them back and",
"I throw away some different ones okay so",
"it means that it's no 1 activation can",
"kind of memorize some part of the input",
"because that's what happens if we over",
"fit right if we over fit some some part",
"of the model is basically learning to",
"recognize a particular image rather than",
"a feature in general or a particular",
"item with dropout it's going to be very",
"hard for it to do that in fact Geoffrey",
"Hinton described one of the kind of part",
"of the thinking behind this as follows",
"he said he noticed every time he went to",
"his bank that all the tellers and staff",
"moved around and he realized the reason",
"for this must be that they're trying to",
"avoid fraud if they keep moving them",
"around",
"nobody can specialize so much in that",
"one thing that they're doing that they",
"can figure out kind of a conspiracy to",
"defraud the bank now of course depends",
"when you ask Hinton at other times he",
"says that the reason for this was",
"because he thought about how spiking",
"neurons work and there's a view he's a",
"neuroscientist by training there's a",
"view that spiking neurons might help",
"regularization and dropout is kind",
"a way of matching this idea of biking",
"neurons",
"I mean it's interesting when you",
"actually ask people where did your idea",
"for some some algorithm come from it",
"basically never comes from math it",
"always comes from intuition and kind of",
"thinking about physical analogies and",
"stuff like that so anyway the truth is a",
"bunch of ideas I guess we're all flowing",
"around and they came up with this idea",
"of dropout but the important thing to",
"know is it worked really really well",
"right and so we can use it in our models",
"to get generalization for free now too",
"much dropout of course is reducing the",
"capacity of your model so it's going to",
"under fit and so you've got to play",
"around with different dropout values for",
"each of your layers to decide so in",
"pretty much every fast AI learner",
"there's a parameter called",
"P's PS which will be the p-value for the",
"dropout for each layer so you can just",
"pass in a list or you can pass it an int",
"and it'll create a list with that value",
"everywhere sometimes it's a little",
"different for CNN for example it",
"actually if you pass in an int it will",
"use that for the last layer and half",
"that value for the earlier layers we",
"basically try to do things or kind of",
"represent best practice but you can",
"always pass in your own list to get",
"exactly the drop out that you want there",
"is an interesting feature of drop out",
"which is that we talk about training",
"time and test time test time we also",
"call inference time training time is",
"when we're actually doing that those",
"wait updates the backpropagation and the",
"training time dropout works the way we",
"just saw at test time we turn off",
"dropout but we're not going to do",
"dropout anymore because we wanted to be",
"as accurate as possible we're not",
"training so we can't cause it to overfit",
"when we're doing inference so we remove",
"dropout but what that means is if",
"previously P was point OV was 0.5 then",
"half the activations were being removed",
"which means when they're all there now",
"our overall activation level is twice",
"what it used to be and so therefore in",
"the paper they suggest",
"multiplying all of your weights at test",
"time by P interestingly you can dig into",
"the PI torch source code and you can",
"find the actual C code where dropout is",
"implemented and here it is and you can",
"see what they're doing is something",
"quite interesting they first of all do a",
"Bernoulli trial so a Bernoulli trial is",
"with probability 1 minus P return the",
"value 1 otherwise return the value 0",
"that's all it means",
"okay so in this case P is the",
"probability of dropout so 1 minus P is a",
"probability that we keep the activation",
"so we end up here with either a 1 or a 0",
"ok and then this is interesting we",
"divide in place remember",
"underscore means in place in play torch",
"we divide in place that 1 or 0 by 1",
"minus P if it's a 0 nothing happens it's",
"still 0 if it's a 1 and P was 0.5 that",
"one now becomes 2 and then finally we",
"multiply in place our input by this",
"noise this dropout mask so in other",
"words we actually don't do in play torch",
"we don't do the change at test time we",
"actually do the change at training time",
"which means that you don't have to do",
"anything special at inference time with",
"play torch it's not a spite watch it's",
"quite a common pattern but it's kind of",
"nice to look inside the pipe torch",
"source code and see you know drop out",
"this incredibly cool incredibly valuable",
"thing is really just these three lines",
"of code which they do in C because I",
"guess it ends up a bit faster when it's",
"all fused together but lots of libraries",
"do it in Python and that works well as",
"well you can even write your own drop",
"out layer and it should give exactly the",
"same results as this that'd be a good",
"exercise to try see if you can create",
"your own drop out layer in Python and",
"see if you can replicate the results",
"that we get with this drop out there",
"so that's drop",
"and so in this case we're going to use a",
"tiny bit of drop out on the first layer",
"and a little bit of drop out on the next",
"layer and then we're going to use",
"special drop out on the embedding layer",
"now why do we do special drop out on the",
"embedding layer so if you look inside",
"the FASTA a source code is our tabular",
"model you'll see that in the section",
"that checks that there's some embeddings",
"we call it embedding and then we",
"concatenate the embeddings into a single",
"matrix and then we call embedding",
"dropout an embedding dropout is simply",
"just a drop out right so it's just an",
"instance of a drop out module this kind",
"of makes sense right for continuous",
"variables that continuous variable is",
"just in one column you wouldn't want to",
"do dropout on that because you're",
"literally deleting the existence of that",
"whole input which is almost certainly",
"not what you want but for an embedding",
"and embedding is just effectively a",
"matrix multiplied by a one hot encoded",
"matrix so it's just another layer so it",
"makes perfect sense to have dropout on",
"the output of the embedding because",
"you're putting drop out on those",
"activations of that layer and so you're",
"basically saying let's delete that",
"random some of the results of that",
"embedding some of those activations so",
"that makes sense the other reason we do",
"it that way is because I did very",
"extensive experiments about a year ago",
"where on this data set I tried lots of",
"different ways of doing clone of",
"everything and you can actually see it",
"here I put it all in a spreadsheet of",
"course Microsoft Excel put them into a",
"pivot table to summarize them all",
"together to find out kind of which",
"different choices and hyper parameters",
"and architectures worked well and worked",
"less well and then I created all these",
"little graphs and these are like little",
"summary training graphs for different",
"combinations of high parameters and",
"architectures and I found that there was",
"one of them which ended up consistently",
"getting a good",
"predictive accuracy the kind of",
"bumpiness of the training was pretty low",
"and you can see on it was just a nice",
"smooth curve and so like this is an",
"example of the kind of experiments that",
"I do that end up in the first day I",
"library right so embedding embedding",
"dropout was one of those things that I",
"just found work really well and",
"basically these the results of these",
"experiments is why it looks like this",
"rather than something else well it's a",
"combination of these experiments but",
"then why did I do these particular",
"experiments well because it was very",
"influenced by what worked well in the",
"that cagoule Prize winners paper but",
"there are quite a few parts of that",
"paper I thought there were some other",
"choices they could have made I wonder",
"why they didn't and I tried them out and",
"found out what actually works and what",
"doesn't work as well and found a few",
"little improvements so that's the kind",
"of experiments that you can play around",
"with as well when you try different",
"models and architectures different",
"dropouts layer numbers number of",
"activations and so forth so I'm having",
"created our learner we can type learned",
"up model to take a look at it and as you",
"would expect in that there is a whole",
"bunch of embeddings each of those",
"abetting matrices tells you well this is",
"the number of levels of the input for",
"each input right and you can match these",
"with with your list cat bars",
"okay so the first one will be store so",
"that's not surprising there are a",
"thousand 116 stores and then the second",
"number of course is the size of the",
"embedding and that's a number that you",
"get to choose and so fast AI has some",
"defaults which actually work really",
"really well nearly all the time so I",
"almost never changed them but when you",
"create your tabular Lerner",
"you can absolutely pass in an embedding",
"size dictionary which Maps variable",
"names to embedding sizes for anything",
"where you want to override the defaults",
"and then we've got our embedding dropout",
"layer and then we've got a batch norm",
"layer with six",
"inputs okay the 16 inputs make sense",
"because we have 16 continuous variables",
"the length of countenance is 16 so this",
"is something for our continuous",
"variables and specifically it's over",
"here the N conte on our continuous",
"variables and BN conte is a batch norm",
"one d what's that",
"well the first short answer is it's one",
"of the things that I experimented with",
"as to having batch normal not in this",
"and I found that it worked really well",
"and then specifically what it is is",
"extremely unclear let me describe it to",
"you it's kind of a bit of regularization",
"it's kind of a bit of training helper",
"it's called batch normalization and it",
"comes from this paper actually before I",
"do this I was want to mention one other",
"really funny thing dropout I mentioned",
"it was a master's thesis not only was it",
"a master's thesis one of the most",
"influential papers of the last ten years",
"it was rejected from the main neural",
"Nets conference what was then called",
"nips",
"now Courtney rips I think this is just",
"it's very interesting because it's just",
"a reminder that you know a our academic",
"community is generally extremely poor at",
"recognizing which things are going to",
"turn out to be important generally",
"people are looking for stuff that are in",
"the field that they're working on and",
"understand so drop out kind of came out",
"of left field it's kind of hard to",
"understand what's going on and so that's",
"kind of interesting and so you know it's",
"a reminder that if you just follow you",
"know as you kind of develop it beyond",
"being just a practitioner into actually",
"doing your own research don't just focus",
"on the stuff everybody's talking about",
"focus on the stuff you think might be",
"interesting because the stuff",
"everybody's talking about generally",
"turns out not to be very interesting the",
"community is very poor at",
"amazing high-impact papers when they",
"come out match normalization on the",
"other hand was immediately recognized as",
"high-impact I definitely remember",
"everybody talking about it in 2015 when",
"it came out and that was because it's so",
"obvious they showed this picture showing",
"the current then state of the art image",
"net model inception this is how long it",
"took them to get you know a pretty good",
"result and then they tried the same",
"thing with this new thing core batch",
"norm and they just did it way way way",
"quickly and so that was enough for",
"pretty much everybody to go wow this is",
"interesting and specifically they said",
"this thing's called batch normalization",
"and it's accelerating training by",
"reducing internal covariant shift so",
"what is internal covariant shift well it",
"doesn't matter because this is one of",
"those things where researchers came up",
"with some intuition and some idea about",
"the scene they wanted to try they did it",
"it worked well they then post hoc added",
"on some mathematical analysis to try and",
"claim where it worked and it turned out",
"they were totally wrong in the last two",
"months there's been two papers so it",
"took three years for people to really",
"figure this out in the last two months",
"there's been two papers that have shown",
"batch normalization doesn't reduce",
"covariate shift at all and even if it",
"did that has nothing to do with why it",
"works so so I think that's a kind of an",
"interesting insight again you know which",
"is like why we should be focusing on",
"being practitioners and experimentalists",
"and developing an intuition right what",
"batch norm does is what you see in this",
"picture here in this paper here are",
"steps or batches right and here is loss",
"and here the red line is what happens",
"when you train without batch norm very",
"very bumpy and here the blue line is",
"what happens when you train with batch",
"norm not very bumpy at all what that",
"means is you can increase your learning",
"rate with batch norm because these big",
"bumps represent times that you're really",
"at risk of your set of weights jumping",
"off into some awful part of the weight",
"space that it can never get out of again",
"so if it's",
"bumpy then you can train at a higher",
"learning rate so that's actually what's",
"going on and here's what it is this is",
"the algorithm and it's really simple",
"the algorithm is going to take a mini",
"batch all right so we have a mini batch",
"and remember this is a layer so the",
"thing coming into it is activations okay",
"so it's a layer and it's going to take",
"in some activations and so that's",
"evasions it's calling X 1 X 2 X 3 and so",
"forth the first thing we do is we find",
"the mean with those activations sum",
"divided by the count let's just the mean",
"and the second thing we do is we find",
"the variance of those activations a",
"difference squared divided by the mean",
"is the variance and then we normalize so",
"then the values minus the mean divided",
"by the standard deviation is the",
"normalized version ok it turns out that",
"B it's actually not that important we",
"used to think it was ok it turns out it",
"not the really important bit is the next",
"bit we take those values and we add a",
"vector of biases they call it beta here",
"and we've seen that before we've used a",
"bias term before okay so we're just",
"going to add a bias term as per usual",
"and then we're going to use another",
"thing that's a lot like a bias term but",
"rather than adding it we're going to",
"multiply by it so there's these",
"parameters gamma and beta which are",
"learnable parameters remember at a",
"neural net there's only two kinds of",
"number activations and parameters these",
"are parameters okay they're things that",
"are learnt with gradient descent this is",
"just a normal bias layer data and this",
"is a model Piketty of bias layer nobody",
"calls it that but that's all it is right",
"it's just like bias but we multiply",
"rather than add that's what batch norm",
"is that's what the layer does so why is",
"that able to achieve this fantastic",
"result I'm not sure anybody has exactly",
"written this down before if they have I",
"apologize for failing to cite Ray",
"because I haven't seen it but let me",
"explain what's actually going on here",
"the value",
"of our predictions y-hat is some",
"function of our various weights there",
"could be millions of them wait 1 million",
"and it's also a function of course of",
"the inputs to our layer this function",
"here is our neuron that function",
"whatever is going on and our neuron",
"there and then our loss let's say it's",
"mean squared error is just our actuals",
"minus our predicted squared okay so",
"let's say we're trying to predict movie",
"review outcomes and they're between 1",
"and 5 okay and we've been trying to",
"train our model and the activations at",
"the very end currently between minus 1",
"and 1 so they're way off where they need",
"to be the scale is off the mean is off",
"so what can we do one thing we could do",
"would be to try and come up with a new",
"set of weights that cause the spread to",
"increase and cause the mean to increase",
"as well but that's going to be really",
"hard to do because remember all these",
"weights interact in very intricate ways",
"right we've got all those nonlinearities",
"and they all combine together so to kind",
"of just move up it's going to require",
"navigating through this complex",
"landscape and we you know we use all",
"these tricks like momentum and atom and",
"stuff like that to help us but it still",
"requires a lot of twiddling around to",
"get there so that's going to take a long",
"time and it's going to be bumpy but what",
"if we did this what if we went times G",
"Plus B we added 2 more parameter vectors",
"or now it's really easy right in order",
"to increase the scale that number has a",
"direct gradient to increase the scale to",
"change the me",
"that number has a direct gradient to",
"change the mean there's no interactions",
"or complexities it's just straight up",
"and down straight in and out and that's",
"what batch Nam does right so batch norm",
"is basically making it easier for it to",
"do this really important thing which is",
"to shift the app puts up and down and in",
"and out and that's why we end up with",
"these results so those details in some",
"ways don't matter terribly the really",
"important thing to know is you",
"definitely want to use it right or if",
"not it's something like it there's",
"various other types of normalization",
"around nowadays but batch norm works",
"great",
"the other main normalization type we use",
"in first AI is something called weight",
"norm which is a much more just in the",
"last few months",
"development okay so that's batch norm",
"and so what we do is we create a batch",
"norm layer for every continuous variable",
"and conte is a number of continuous",
"variables in fast AI n underscore",
"something always means the count of that",
"thing can't always means continuous so",
"then here is where we use it we grab our",
"continuous variables and we throw them",
"through a batch norm layer and so then",
"over here you can see it in a model one",
"interesting thing is this momentum here",
"this is not momentum like in",
"optimization but this is momentum as in",
"exponentially weighted moving average",
"specifically this mean and standard",
"deviation we don't actually use a",
"different mean and standard deviation",
"for every mini batch if we did it would",
"vary so much did it be very hard to",
"train so instead we take an",
"exponentially weighted moving average of",
"the mean and standard deviation okay and",
"if you don't remember what I mean by",
"that look back at last week's lesson to",
"remind yourself about exponentially",
"weighted moving averages which we",
"implemented in excel for the momentum",
"and atom gradient squared terms",
"you can vary the amount of momentum in a",
"batch norm layer by passing a different",
"value to the constructor in plate watch",
"if you use a smaller number it means",
"that the mean and standard deviation",
"will vary less from mini batch to mini",
"batch and that will have less of a",
"regularization effect a larger number",
"will mean the variation will be greater",
"for a mini batch to mini batch that will",
"have more of a regularization effect so",
"as well as this thing of training more",
"nicely because it's parameterised better",
"this momentum term in the mean and",
"standard deviation is the thing that",
"adds is nice regularization piece when",
"you add batch norm you should also be",
"able to use a higher learning rate so",
"that's our model so then you can go ll",
"find you can have a look and then you",
"can go fit you can save it you can plot",
"the losses you can fit a bit more and we",
"end up 0.1 oh three tenths place in the",
"competition was 0.108 so it's looking",
"good all right again take it with a",
"slight grain of salt because what you",
"actually need to do is use the real",
"training set and submit it to cow go but",
"you can see we're very much you know",
"amongst the kind of cutting-edge of",
"models at least as of 2015 and as I say",
"they haven't really been any",
"architectural improvements since then",
"there wasn't batch norm when this was",
"around so the fact we added batch norm",
"means that we should get better results",
"and certainly more quickly and if I",
"remember correctly in their model they",
"had to train at a slow lower learning",
"rate for quite a lot longer as you can",
"see this is about less than 45 minutes",
"of training so that's nice and fast",
"any questions in what proportion would",
"you use dropout versus other",
"regularization errors like weight decay",
"L two norms etc so remember that l2",
"regularization and weight decay a kind",
"of two ways of doing the same thing and",
"we should always use the weight decay",
"version not the l2 regularization",
"version so there's",
"when Takei there's batch norm which kind",
"of has a regularizing effect there's",
"data augmentation which we'll see soon",
"and this drop out so that's normally",
"pretty much always want so that's easy",
"data augmentation we'll see in a moment",
"so then it's really between dropout",
"versus weight okay",
"I have no idea I don't I don't think",
"I've seen anybody to fight a compelling",
"study of how to combine those two things",
"can you always use one instead of the",
"other why why not I don't think anybody",
"has figured that out I think in practice",
"it seems that you generally want a bit",
"of both you pretty much always want some",
"weight decay but you often also want a",
"bit of dropout but honestly I don't know",
"why I've not seen anybody really explain",
"why or how to decide so this is one of",
"these things you have to try out and",
"kind of get a feel for what tends to",
"work for your kinds of problems I think",
"the defaults that we provide in most of",
"our learners should work pretty well in",
"most situations but yeah definitely play",
"around with it okay the next kind of",
"regularization we're going to look at is",
"data augmentation and data augmentation",
"is one of the least well studied types",
"of regularization but it's the kind that",
"I think I'm kind of the most excited",
"about the reason I'm kind of the most",
"about it is that you basically there's",
"basically almost no cost to it you can",
"do data augmentation and get better",
"generalization without it taking longer",
"to train without underfitting",
"to an extent at least so let me explain",
"so what we're going to do now is we're",
"going to come back to a computer vision",
"and we're going to come back to our pets",
"data set again so let's let's load it in",
"all right our pets data set the images",
"were inside the images subfolder I'm",
"going to call get transforms as per",
"usual but when we call get transforms",
"there's a whole long list of things that",
"we can provide and so far we haven't",
"been varying that much at all but in",
"order to really understand data",
"augmentation I'm going to kind of",
"ratchet up all of the defaults so",
"there's a parameter here for what's the",
"probability of an affine transform",
"happening what's the probability of a",
"light lighting transfer happening so I",
"set them both to one so they're all",
"gonna get transformed I'm going to do",
"more rotation more zoom more lighting",
"transforms and more warping what are all",
"those mean well you should check the",
"documentation and to do that by typing",
"doc and there's a doc the brief",
"documentation but the real documentation",
"is in dogs so I'll click on show in",
"Doc's and here it is okay and so this",
"tells you what all those do but",
"generally the most interesting parts of",
"the Doc's tend to be at the top where",
"you kind of get the summaries of what's",
"going on and so here there's something",
"called list of transforms and here you",
"can see every transform has a something",
"showing you lots of different values of",
"it right so here's brightness so make",
"sure you read these and remember these",
"notebooks you can open up and run this",
"code yourself and get this output all of",
"these know all of these HTML",
"documentation documents are",
"auto-generated from the notebooks in the",
"docs underscore source directory in the",
"FASTA a repo okay so you will see the",
"exact same cats",
"if you try this so if I really likes",
"cats so there's a lot of cats in the",
"documentation",
"and I think you know because he's been",
"so awesome at creating great",
"documentation he gets to pick the cats",
"so so for example looking at different",
"values of brightness what I do here is I",
"look to see two things the first is for",
"which of these levels of transformation",
"is it still clear what the picture is a",
"picture of so this is kind of getting to",
"a point where it's pretty unclear this",
"is possibly getting a little unclear the",
"second thing I do is I look at the",
"actual data set that I'm modeling or",
"particularly the data set that I'll be",
"using as validation set and I try to get",
"a sense of what the variation in this",
"case in lighting is so referred like",
"nearly all professionally taking photos",
"I would probably want them all to be",
"about in the middle but if the if the",
"kind of their photos that are taken",
"inside some pretty amateur photographers",
"they are likely to be something they're",
"very overexposed some very underexposed",
"right so you should pick a value of the",
"estate or augmentation for brightness",
"that both allows the image to still be",
"seen clearly and also represents the",
"kind of data that you're going to be",
"using this to model on in practice so",
"you kind of see the same thing for",
"contrast right it'd be unusual to have a",
"data set with such ridiculous contrast",
"where perhaps you do in which case you",
"should use data augmentation up to that",
"level but if you don't then you",
"shouldn't this one called dihedral is",
"just one that does every possible",
"rotation and flip and so obviously most",
"of your pictures are not going to be",
"upside down cats",
"that's so you probably would say hey",
"this doesn't make sense I won't use this",
"for this data set that if you're looking",
"at satellite images of course you would",
"on the other hand flip makes perfect",
"sense so you would include that a lot",
"of things that you can do with fast AI",
"lets you pick a padding mode and this is",
"what padding mode looks like you can",
"pick zeros",
"you can pick border which just",
"replicates or you can pick reflection",
"which as you can see is it's as if the",
"last little few pixels are in a mirror",
"reflections nearly always better by the",
"way I don't know that anybody else has",
"really studied this but we've we we have",
"studied it in some depth haven't",
"actually written a paper about it but",
"just enough for our own purposes to say",
"reflection works best most of the time",
"so that's the default then there's a",
"really cool bunch of perspective warping",
"ones which I'll probably show you by",
"using symmetric warp if you look at the",
"kind of the we've added black borders to",
"this so it's more obvious for what's",
"going on and as you can see what",
"symmetric warp is doing it's as if the",
"camera is being moved above or to the",
"side of the object and literally warping",
"the whole thing like that right and so",
"the cool thing is that as you can see",
"each of these pictures it's as if this",
"cat was being taken kind of from",
"different angles right so they're all",
"kind of optically sensible right and so",
"this is a really great type of data",
"augmentation it's also one which I don't",
"know of any other library that does it",
"or at least certainly one that does it",
"in a way that's both fast and keeps the",
"image crisp as it is in first AI so this",
"is like if you're looking to whit win a",
"cattle competition this is the kind of",
"thing that's going to like get you above",
"the people that aren't using the first",
"area library so having looked at all",
"that we are going to add this have a",
"little get data function that just does",
"the usual game data block stuff but",
"we're going to add padding mode",
"explicitly so that we can turn on",
"padding mode of zeros just so we can see",
"what's going on better fast AI has this",
"handy little function called plot multi",
"which is going to create a three by",
"three grid of plots and each one will",
"contain the result of calling this",
"function which will receive the",
"what coordinates and the axis and so I'm",
"actually going to plot the exact same",
"thing in every box but because this is a",
"training data set it's going to use data",
"augmentation and so you can see the same",
"Dougy using lots of different kinds of",
"data augmentation and so you can see why",
"this is going to work really well",
"because these pictures all look pretty",
"different right but we didn't have to do",
"any extra hand labeling or anything",
"they're like it's like free extra data",
"okay",
"so data augmentation is really really",
"great and one of the big opportunities",
"for research is to figure out ways to do",
"data augmentation in other domains so",
"how can you do data augmentation with",
"text data or genomic data or",
"histopathology data or whatever right",
"almost nobody's looking at that and to",
"me it's one of the biggest opportunities",
"that could let you decrease data",
"requirements by like five to ten x so",
"here's the same thing again but with",
"reflection padding instead of zero",
"padding and you can kind of see like see",
"this doggies legs are actually being",
"reflected at the bottom here so",
"reflection padding tends to create",
"images that are kind of much more",
"naturally reasonable like in the real",
"world you don't get black borders like",
"this so they do seem to work better okay",
"so because we're going to study",
"convolutional neural networks we are",
"going to create a convolutional neural",
"network you know how to create them so",
"I'll go ahead and create one",
"I will fit it for a little bit they will",
"unfreeze it",
"I will then create a larger version of",
"the data set 352 by 352 and fit for a",
"little bit more and I will save it",
"okay so we have a CNN and we're going to",
"try and figure out what's going on",
"and our CNN and the way we're going to",
"try and figure it out is explicitly",
"specifically that we're going to try to",
"learn how to create this picture this is",
"a heat map right this is a picture which",
"shows me what part of the image did the",
"CNN focus on when it was trying to",
"decide what this picture is so we're",
"going to make this heat map from scratch",
"when we so we're kind of at a point now",
"in the course where I'm assuming that if",
"you've got to this point you know when",
"you're still here thank you",
"then you're interested enough that",
"you're prepared to kind of dig into some",
"of these details so we're actually going",
"to learn how to create this heat map",
"without almost any fast AI stuff we're",
"going to use pure kind of tensor",
"arithmetic in PI torch and we're going",
"to try and use that to really understand",
"what's going on so to warn you none of",
"it's rocket science but a lot of its",
"going to look really new so don't expect",
"to get it the first time but expect to",
"like listen jump into the notebook try a",
"few things test things out look",
"particularly at like tensor shapes and",
"inputs and outputs to check your",
"understanding then go back and listen",
"again but and kind of try it a few times",
"because you will get there right it's",
"just that there's going to be a lot of",
"new concepts because we haven't done",
"that much stuff in pure by touch okay so",
"what we're going to do is going to have",
"a seven minute break and then we're",
"going to come back and we're going to",
"learn all about the innards of a CNN so",
"I'll see you at 7:50 so let's learn: 67:33: about convolutional neural networks you",
"know the funny thing is it's pretty",
"unusual to get",
"close to the end of a course and only",
"then look at convolutions but like when",
"you think about it knowing actually how",
"batch norm works or how dropout works or",
"how convolutions work isn't nearly as",
"important as knowing how it all goes",
"together and what to do with them and",
"how to figure out how to do those things",
"better but it's you know we're kind of",
"at a point now where we want to be able",
"to do things like that and although you",
"know where we're adding this",
"functionality directly into the library",
"so you can kind of run a function to do",
"that you know the more you do the more",
"you'll find things that you want to do a",
"little bit differently to how we do them",
"or there'll be something in your domain",
"where you think like oh I could do a",
"slight variation of that so you're kind",
"of getting to a point in your experience",
"now where it helps to know how to do",
"more stuff yourself and that means you",
"need to understand what's really going",
"on behind the scenes so what's really",
"going on behind the scenes is that we",
"are creating a neural network that looks",
"a lot like this right but rather than",
"doing a matrix multiply here and here",
"and here we're actually going to do",
"instead a convolution and a convolution",
"is just a kind of matrix multiply which",
"has some interesting properties you",
"should definitely check out this website",
"certo certo slash Eevee explain visually",
"where we have stolen this beautiful",
"animation it's actually a JavaScript",
"thing that you can actually play around",
"with yourself in order to show you how",
"convolutions work and it's actually",
"showing you a convolution as we move",
"around these little red squares so",
"here's here's a picture a black and",
"white or grayscale picture right and so",
"each 3x3 bit of this picture is this red",
"thing moves around it shows you a",
"different 3x3 part right it shows you",
"over here the",
"of the pixels right so in first ice case",
"our pixel values are between Norton one",
"in this case there between Norton 255",
"right so here are nine pixel values this",
"area is pretty white so they're pretty",
"high numbers okay and so as we move",
"around you can see the nine big numbers",
"change and you can also see their colors",
"change up here is another nine numbers",
"and you can see those in the little X 1",
"X 2 X 1 here 1 2 1 now what you might",
"see going on is as we move this little",
"red block as these numbers change we",
"then multiply them by the corresponding",
"numbers up here and so let's start using",
"some nomenclature the thing up here we",
"are going to call the kernel the",
"convolutional kernel so we're going to",
"take each little 3x3 part of this image",
"and we're going to do an element-wise",
"multiplication of each of the 9 pixels",
"that we are mousing over with each of",
"the 9 items in our kernel and so once we",
"multiply each set together we can then",
"add them all up and that is what's shown",
"on the right as the little bunch of red",
"things move over there you can see",
"there's one red thing that appears over",
"here the reason there's one red thing",
"over here is because each set of 9 after",
"getting through the element-wise",
"multiplication with the kernel get added",
"together to create one output so",
"therefore the size of this image has one",
"pixel less on each edge than the",
"original as you can see see how there's",
"black borders on it that's because at",
"the edge the 3x3 kernel can't quite go",
"any further",
"right so the furthest you can go is to",
"end up with a dot in the middle just off",
"the corner ok so why are we doing this",
"well perhaps you can see what's happened",
"this face has turned into some white",
"parts outlining the horizontal edges how",
"well the how is just by doing this",
"element wise multiplication of each set",
"of 9 pixels with this kernel adding them",
"together and sticking the result in the",
"corresponding but over here why is that",
"creating white spots with the horizontal",
"edges are well let's think about it",
"let's look up here so if we're just in",
"this little bit here right then the",
"spots above it all pretty white so they",
"have high numbers so the bits above it",
"big numbers who are getting multiplied",
"by 1 to 1 so that's going to create a",
"big number and the ones in the middle",
"are all zeros so don't care about that",
"and then the ones underneath are all",
"small numbers because they're all close",
"to 0 so that really doesn't do much at",
"all so therefore that little set there",
"is going to end up with right white okay",
"whereas on the other side right down",
"here you've got light pixels underneath",
"so they're going to get a lot of",
"negative dark pixels on top which are",
"very small so not much happens",
"so therefore over here we're going to",
"end up with very negative so this thing",
"where we take H 3x3 area and element",
"wise multiply them with a kernel and add",
"each of those up together to create one",
"output is called a convolution that's it",
"that's a completion so that might look",
"familiar to you",
"right because what we did back a while",
"ago is we looked at that Zeiler and",
"furgus paper where we saw like each",
"different layer and we visualized what",
"the weights were doing and remember how",
"the first layer was basically like",
"finding diagonal edges and gradient",
"that's because that's what a convolution",
"can do right H of our layers is just a",
"convolution so the first layer can do",
"nothing more than this kind of thing but",
"the nice thing is the next layer could",
"then take the results of this right and",
"it could kind of combine one channel but",
"so one or the output of one",
"convolutional field is called a channel",
"right so it could take one channel that",
"found top edges and another channel that",
"finds left edges and then the layer",
"above that could take those two as input",
"and create something that finds top left",
"corners as we saw when we looked at",
"those earlier and focused visualizations",
"so let's take a look at this from",
"another angle or quite a few other",
"angles and we're going to look at a",
"fantastic post tramatic or Matt Klein",
"Smith who was actually a student in the",
"first year that we did this course and",
"he wrote this as part of his project",
"work back then and what he's going to",
"show here is here is our image it's a",
"three by three image and our kernel is a",
"two by two kernel and what we're going",
"to do is we're going to apply this",
"kernel to the top left 2x2 part of this",
"image and so the pink bit will be",
"correspondingly multiplied by the pink",
"bit the green by the green and so forth",
"and they all get added up together to",
"create this top left in the output so in",
"other words P equals alpha times a beta",
"times be gamma times D Delta times e",
"there it is plus B which is a bias okay",
"so that's fine that's just a normal bias",
"so you can see how basically each of",
"these output pixels is the result of",
"some different linear equation that",
"makes sense and you can see these same",
"four weights are being moved around",
"because this is our convolutional kernel",
"here",
"the way of looking at it from that which",
"is here is a classic neural network view",
"and so P now is result of multiplying",
"every one of these inputs by a weight",
"and then adding them all together except",
"the gray ones I've got to have a value",
"of zero right because remember P was",
"only connected to a B D and E a B D P so",
"in other words remembering that this",
"represents a matrix multiplication",
"therefore we can represent this as a",
"matrix multiplication so here is our",
"list of pixels in our 3x3 image",
"flattened out into a vector and here is",
"a matrix vector multiplication plus bias",
"and then a whole bunch of them we're",
"just going to set to zero all right so",
"you can see here we've got a zero zero",
"zero zero zero which corresponds to zero",
"zero zero zero zero so in other words a",
"convolution is just a matrix",
"multiplication where two things happen",
"some of the entries are set to zero all",
"the time and all of the ones are the",
"same color always have the same weight",
"so when you've got multiple things with",
"the same weight",
"that's called weight time okay so",
"clearly we could implement a convolution",
"using matrix multiplication but we don't",
"because it's slow at swim practice our",
"libraries have specific convolution",
"functions that we use and they're",
"basically doing this which is this which",
"is this equation which is says the same",
"as this matrix multiplication and as we",
"discussed we have to think about padding",
"because if you have a 3 by 3 kernel and",
"a 3 by 3 image",
"then that can only create one pixel of",
"output there's only one place that this",
"3x3 can go so if we want to create more",
"than one pixel of output we have to do",
"something called padding which is to put",
"additional numbers all around the",
"outside so what most libraries do is",
"that they just put a layer of zeros",
"normal layer a bunch of zeros of all",
"around the outside so for 3x3 kernel a",
"single zero on every edge piece here and",
"so once you've pattern it like that you",
"can now move your 3x3 kernel all the way",
"across and give you the same output size",
"that you started with okay now as we",
"mention in fast AI we don't normally",
"necessarily use zero padding where",
"possible we use reflection padding",
"although for these simple convolutions",
"we often use zero padding because it's",
"doesn't matter too much in a big image",
"it doesn't make too much difference okay",
"so that's what a convolution is so a",
"convolutional neural network wouldn't be",
"very interesting if it can only create",
"top edges so we have to take it a little",
"bit further so if we have an input",
"and it might be you know standard kind",
"of red-green-blue picture right then we",
"can create a kernel a 3x3 kernel like so",
"and then we could pass that kernel over",
"all of the different pixels but if you",
"think about it we actually don't have a",
"2d and put anymore we have a 3d input a",
"rank 3 tensor so we probably don't want",
"to use the same kernel values for each",
"of red and green and blue because for",
"example if we're creating a green frog",
"detector we would want more activations",
"on the green then we would on the blue",
"right or if we're trying to find",
"something that could actually find a",
"gradient that goes from green to blue",
"then the different kernels for each",
"channel need to have different values in",
"so therefore we need to create a 3 by 3",
"by 3 kernel okay so this is still our",
"kernel and we're still good a very it",
"across the height and the width but",
"rather than doing an element-wise",
"multiplication of 9 things we're going",
"to do an element-wise multiplication of",
"27 things 3 by 3 by 3 and we're still",
"going to then add them up into a single",
"number so as we pass this cube over this",
"and the kind of like a little bit that's",
"going to be sitting behind it right as",
"we do that part of the convolution it's",
"still going to create just one number",
"because we do an element-wise",
"multiplication of all 27 and add them",
"all together so we can do that across",
"the whole padded single unit padded",
"input and so we started with 1 2 3 4 5",
"by 5 so we're going to end up with an",
"output that's also",
"five by foe right",
"but now input was three channels and our",
"output is only one Channel",
"now we're not going to be able to do",
"very much with just one channel because",
"all we've done now is found the top edge",
"how we're going to find a side edge and",
"a gradient and an area of constant",
"weight well we're going to have to",
"create another kernel and we're going to",
"have to do that convert convolved over",
"the input and that's going to create",
"another 5x5 and then we can just stack",
"those together across this there's",
"another axis and we can do that lots and",
"lots of times and that's going to give",
"us another rank",
"sensor output so that's what happens in",
"practice now I'd in practice we start",
"with an input which is H ok which is H",
"by W by 4 images 3 we pass it through a",
"bunch of convolutional kernels but we",
"compare to pick how many we want and it",
"gives us back an output of and it gives",
"us back an output of height by width by",
"however many kernels we had and so often",
"that might be something like 16 in the",
"first player and so now we've got 16",
"channels they're called sixteen channels",
"representing things like how much left",
"egg edge was on this pixel how much top",
"edge was in this pixel how much blue to",
"red gradient was on this problem this",
"set of 2709 pixels each with RGB and so",
"then you can just do the same thing",
"right you can have another bunch of",
"kernels",
"and that's going to create",
"another output ranked 310 sir again",
"height by width by whatever might still",
"be 16 now what we really like to do is",
"as we get deeper in the network or you",
"actually want to have more and more",
"channels we want to be able to find like",
"a richer and richer set of features so",
"that after a few as we saw in the Siler",
"and Fergus paper by layer four or five",
"we've kind of got eyeball detectors and",
"fur detectors and things right so you",
"really need a lot of channels so in",
"order to avoid our memory going out of",
"control",
"from time to time we create a",
"convolution where we don't step over",
"every single set of 3x3 but instead we",
"skip over two at a time so we would",
"start with a 3x3 centered at 2 comma 2",
"and then we'd jump over to 2 comma 4 2",
"comma 6 2 comma 8 and so forth and",
"that's called a stride to convolution",
"and so what that does is it looks",
"exactly the same right it's still just a",
"bunch of kernels but we're just",
"over to it it's time right we're",
"skipping every alternate input pixel and",
"so the output from that will be H over 2",
"by W over 2 and so when we do that we",
"generally create twice as many kernels",
"so we can now have say 32 activations in",
"each of those spots and so that's what",
"modern convolutional neural networks",
"kind of tend to look like right and so",
"we can actually see that if we go into",
"our pets and we grab our CNN right and",
"we're going to take a look at this",
"particular cat so if we go X comma y",
"equals valid data set some index so it's",
"just grab the 0th we'll go touch show",
"and would print out the value of y",
"apparently this cat is of category Main",
"Hoon so until a week ago I was not at",
"all familiar that there's a cat called a",
"Maine Coon having spent all week with",
"this particular cat I am now deeply",
"familiar with this Maine Coon so we can",
"if we go learn summary remember that our",
"input we asked for was 352 by 352 pixels",
"generally speaking the very first",
"convolution tends to have a stride too",
"so after the first layer its 176 by 176",
"so this is lone dot summary we'll print",
"out for you the output shape up to every",
"layer 176 by 176 and the first set of",
"convolutions is has 64 activations and",
"we can actually see that if we type in",
"learn belt model you can see here it's a",
"2d con input channels and 64 output",
"channels and it's tried of - okay and",
"interestingly it actually starts with a",
"kernel size of 7 by 7 so like nearly all",
"of the convolutions are 3 by 3 so either",
"all 3 by 3",
"right for reasons we'll talk about in",
"part two we often use a larger kernel",
"for the very first one if you here's a",
"larger kernel you have to use more",
"padding so we have to use kernel size",
"int / 2 padding to make sure we don't",
"lose anything anyway so we're now have",
"64 output channels and since it was",
"straight - it's now 176 by 176 and then",
"as we go along you'll see that from time",
"to time we have go from 88 by 88 to 40",
"by 40 by 40 for the grid size so that",
"was a 2d con and then when we do that we",
"generally double the number of channels",
"so we keep going through a few more",
"calms and they've as you can see they've",
"got batch norm and rally that's kind of",
"pretty standard and eventually we do it",
"again now the Strad - cons which again",
"doubles okay we can have about 512 by 11",
"by 11 and that's basically where we",
"finish the main part of the network we",
"end up with 5 12 channels 11 by 11",
"okay so we're actually at a point where",
"we're going to be able to do this heat",
"map now so let's try and work through it",
"before we do I want to show you how you",
"can do your own manual convolutions",
"because it's kind of fun so we're going",
"to start with this picture of a Maine",
"Coon and I've created a convolutional",
"kernel and so as you can see this one",
"has a right edge and a bottom edge with",
"positive numbers and just inside that",
"it's got negative numbers so I'm",
"thinking this should show me",
"bottom-right edges ok so that's my",
"tensor now one complexity is that that",
"3x3 kernel cannot be used for this",
"purpose because I need two more",
"dimensions the first is I need the third",
"dimension to say how to combine",
"the red green and blue so what I do is I",
"say don't expand this is my 3x3 and I",
"pop another three on the start what",
"don't expand does is it says create a 3",
"by 3 by 3 tensor by simply copying this",
"one 3 times I mean honestly it doesn't",
"actually copy it it pretends to have",
"copied it you know but it just basically",
"refers to the same block of memory so it",
"kind of copies it in a memory efficient",
"way so this one here is now 3 copies of",
"that and the reason for that is that I",
"want to treat red and green and blue the",
"same way for this little manual kernel",
"I'm showing you and then we need one",
"more access because rather than actually",
"having a separate kernel like I've kind",
"of printed these as if they were",
"multiple kernels what we actually do is",
"we use a rank 4 tensor and so the very",
"first access is for the every separate",
"kernel that we have so in this case I'm",
"just going to create one kernel so to do",
"a convolution I still have to put this",
"unit access on the front so you can see",
"k dot shape is now 1 comma 3 comma 3",
"comma 3 so it's a 3 by 3 kernel there",
"are three of them and then that's just",
"the one kernel that I have so it kind of",
"takes awhile to get the feel for these",
"higher dimensional tensors because we're",
"not used to writing out before D tensor",
"but like just think of them like this",
"before T tensor is just a bunch of 3d",
"tensors sitting on top of each other",
"ok so this is our um 40 tensor and then",
"you can just call kana 2d passing in",
"some image and so the image I'm going to",
"use is the first part of my validation",
"data set and the kernel there's one more",
"trick which is that in pi torch pretty",
"much everything is expecting to work on",
"a mini-batch not on an individual thing",
"okay so in our case we have to create a",
"a batch of size 1 so our original image",
"is three channels by 352 by 352 hoped by",
"width that's remember paid Watchers",
"channel by height by width I want to",
"create a mini design I need to create a",
"rank 4 tensor where the first axis is 1",
"in other words it's a mini batch of size",
"1 because that's what plate watch",
"expects so there's something you can do",
"in both pi torch and numpy which is you",
"can index into an array or a tensor with",
"a special value none and that creates a",
"new unit access in that point point so T",
"is my image of dimensions 3 by 3 52 by",
"352 T none is a rank 4 tensor a mini",
"batch of one image of 1 by 3 by 3 50 by",
"352 and so now I can go to D and get",
"back ok specifically my Maine Coon ok so",
"that's how you can play around with",
"convolutions yourself so how are we",
"going to do this to create a heat map",
"this is where things get fun remember",
"mentioned was that I basically have like",
"my input red-green-blue and it goes",
"through a bunch of convolutional layers",
"let us write a little line to say a",
"convolutional layer to create",
"activations which have more and more",
"channels and eventually less and less",
"smaller and smaller height by widths",
"until eventually remember we looked at",
"the summary we ended up with something",
"which was 11 by 11 by 512 and there's a",
"hope there's a whole bunch more layers",
"that we skipped over now there are 37",
"classes because remember data dot C is",
"the number of classes we have and we can",
"see that at the end here we end up with",
"37 features in our model so that means",
"that we end up with a probability for",
"every one of the 37 breeds of cat and",
"dog so it's a vector of length 37 that's",
"our final output that we need because",
"that's what we're going to compare",
"implicitly to our one hot encoded matrix",
"which will have a 1 in the location for",
"Maine Coon yeah so somehow we need to",
"get from this 11 by 11 by 512 to this 37",
"and so the way we do it is we actually",
"take the average of every one of these",
"11 by 11 faces we just take the mean so",
"we're going to take the mean of this",
"first face take the mean that gets this",
"one value and then we'll take",
"second of the five twelve faces and take",
"that mean and that'll give us one more",
"value that's a we'll do that for every",
"face and that will give us a five twelve",
"long vector okay and so now all we need",
"to do is pop that through a single",
"matrix multiply of five twelve by thirty",
"seven and that's going to give us an",
"output vector of length thirty-seven",
"okay so this step here where we take the",
"average of each face is called average",
"pooling so let's go back to our model",
"and take a look there it is here is our",
"final five twelve and here is we will",
"talk about what a concat pooling is in",
"part two for now we'll just focus on",
"that this is a fast AI specialty",
"everybody else just does this average",
"pool average pool duty with an output",
"size of one so here it is output average",
"pool 2d with an output size of one and",
"then again there's a bit of a special",
"faster I think that we actually have two",
"layers here but normally people then",
"just have the one linear layer with the",
"input of 512 and the output of 37 okay",
"so what that means is that this little",
"box over here where we want a one for",
"Maine Coon we've got to have a box over",
"here which needs to have a high value in",
"that place so that the lots of below so",
"if we're going to have a high value",
"there the only way to get it is with",
"this matrix multiplication is that it's",
"going to represent a simple weighted",
"linear combination of all of the 512",
"values here so if we're going to be able",
"to say I'm pretty confident this is a",
"Maine Coon just by taking the weighted",
"sum of a",
"of inputs those inputs are going to have",
"to represent features like how fluffy is",
"it what color is it snows how long as",
"its legs how point here it's ears you",
"know all the kinds of things that can be",
"used because for the other thing which",
"figures out is this a bulldog it's going",
"to use exactly the same kind of 512",
"inputs with a different set of weights",
"because that's all a matrix",
"multiplication is right it's just a",
"bunch of weighted sums a different",
"weighted sum for each output okay so",
"therefore we know that this you know",
"potentially dozens or even hundreds of",
"layers of convolutions must have",
"eventually come up with an 11 by 11 face",
"for each of these features saying in",
"this little bit here how much is that",
"part of the image like a pointy ear how",
"much is it fluffy how much is it like a",
"long leg how much is it like a very red",
"nodes right so that's what all of those",
"things must represent so each face is",
"what we call each of these represents a",
"different feature okay so the outputs of",
"these we can think of as different",
"features so what we really want to know",
"then is not so much what's the average",
"across the 11 by 11 to get this set of",
"outputs but what we really want to know",
"is what's in each of these 11 by 11",
"spots so what if instead of averaging",
"across the 11 by 11 let's instead",
"average across the 512 if we average",
"across the 512 that's going to give us a",
"single 11 by 11 matrix and each item",
"each each grid point in that 11 by 11",
"matrix will be the average of how",
"activated was that area when it came to",
"figuring out that this was a Maine Coon",
"how many",
"signs of Maine Coon ish Ness was there",
"in that part of the 11 by 11 grid and so",
"that's actually what we do to create our",
"heat map so I think maybe the easiest",
"way is to kind of work backwards here's",
"our heat map and it comes from something",
"called",
"average activations and it's just a",
"little bit of matplotlib and faster a",
"faster I to show the image and then",
"matplotlib to take the heat map which we",
"passed in which was called average",
"activations hm for heat map alpha 0.6",
"means make it a bit transparent and",
"matplotlib extent means expand it from",
"11 by 11 to 352 by 352 he is by linear",
"interpolations it's not all blocky and",
"use a different color map to kind of",
"highlight things that's just the",
"matplotlib is not important the key",
"thing here is that average activations",
"is the 11 by 11 matrix we wanted here it",
"is average activations touch shape is 11",
"by 11 so to get there we took the mean",
"of activations across dimension 0 which",
"is what I just said in PI torch the",
"channel dimension is the first dimension",
"so the main across dimension 0 took us",
"from something of size 512 by 11 by 11",
"as promised",
"to something of 11 by 11 so therefore",
"activations axe contains the activations",
"we're averaging where did they come from",
"they came from something called a hook",
"so a hook is a really cool more advanced",
"PI torch feature that lets you as the",
"name suggests hook into the PI torch",
"machinery itself and run any arbitrary",
"Python code you want to it's a really",
"amazing and nifty thing because you know",
"normally when we do a forward pass",
"through a PI torch module it gives us",
"this set of outputs but we know that in",
"the process it's calculated these",
"so why would I what I would like to do",
"is I would like to hook into that",
"forward pass and tell PI torch hey when",
"you calculate this can you store it for",
"me please okay so what is this this is",
"the output of the convolutional part of",
"the model so the convolutional part of",
"the model which is everything before the",
"average pool is basically all of that",
"but and so thinking back to transfer",
"learning right you remember with",
"transfer learning we actually cut off",
"everything after the convolutional part",
"of the model and replaced it with our",
"own little bit right so with fast AI the",
"original convolutional part of the model",
"is always going to be the first thing in",
"the model and specifically it's always",
"going to be called assuming so in this",
"case I'm taking my model and I'm just",
"going to call it M right so you can see",
"M is this big thing but always at least",
"in first day I always m0 will be the",
"convolutional part of the model so in",
"this case we created a let's go back and",
"see we created a resin at 34 so the the",
"main part of the resin at 34 though the",
"pre-trained bit we hold on to is in m0",
"and so this is basically it this is a",
"printout of the rezident 84 and at the",
"end of it there is the 512 activations",
"so what in other words what we want to",
"do is we want to grab em 0 and we want",
"to hook its output so this is a really",
"useful thing to be able to do so far C",
"is actually created something to do it",
"for you which is literally you say hook",
"output and you pass in the PI torch",
"module that you want to hook the output",
"of and so most of the most likely the",
"thing you want to hook is the",
"convolutional part of the model",
"and that's always going to be M 0 or",
"learn model zero so we give that hawk a",
"name don't worry about this part we'll",
"learn about it next week so having",
"hooked the output we now need to",
"actually do the forward pass all right",
"and so remember in PI torch to actually",
"get it to calculate something which is",
"called doing the forward pass you just",
"act as if the model is a function right",
"so we just pass in our X our X",
"mini-batch so we already had a Maine",
"Coon image called X right but we can't",
"quite pass that into our model it has to",
"be normalized and turned into a mini",
"batch and put on to the GPU so first AI",
"has a thing called a data bunch which we",
"have in data and you can always say data",
"dot one item to create a mini batch with",
"one thing in it ok and as an exercise at",
"home you could try to create a mini",
"batch without using data dot one item so",
"make sure that you kind of learn how to",
"normalize and stuff yourself if you want",
"to but this is how you can create a mini",
"batch with just one thing in it and then",
"I can pop that onto the GPU by saying",
"drop CUDA that's what I passed in my",
"model and so the predictions I get out",
"actually don't care about right because",
"the predictions is the predictions is",
"this thing which is not what I want",
"right so I'm not actually going to do",
"anything with the predictions the thing",
"I care about is the hook that it is",
"created now one thing to be aware of is",
"that when you hook something in playa",
"torch that means every single time you",
"run that model assuming you're hooking",
"outputs it's storing those outputs and",
"so you want to remove the hook when",
"you've got what you want because",
"otherwise if you use the model again",
"it's going to keep hooking more and more",
"outputs which will be slow and memory",
"intensive so we've created this thing",
"Python calls that a context manager",
"you can use any hook as a context",
"manager at the end of that with block",
"it'll remove the hook okay so we've got",
"and so now pi torch walks so fast a",
"eyehooks",
"always give you something called or at",
"least the output hooks always give you",
"something called dot stored which is",
"where it stores away the thing you asked",
"you to hook and so that's where the",
"activations now uh okay so we did a",
"forward pass after hooking the output of",
"the convolutional section of the model",
"we grabbed what it stored we check the",
"shape it was 512 by 11 by 11 as we",
"predicted we then took the mean of the",
"channel axis to get an 11 by 11 tensor",
"and then if we look at that that's our",
"picture so there's a lot to unpack right",
"lot to unpack but if you take your time",
"going through these two sections the",
"convolution kernel section and the",
"heatmap section of this notebook like",
"running those lines of code and changing",
"them around a little bit and remember",
"the most important thing to look at is",
"shape you might have noticed when I'm",
"showing you these notebooks so very",
"often print out the shape and when you",
"look at this shape you want to be",
"looking at how many axes are there",
"that's the rank of the tensor and how",
"many things are there in each axis and",
"try and think why right try going back",
"to the printout of the summary try going",
"back to the actual list of the layers",
"and try and go back and think about the",
"actual picture we drew and think about",
"what's actually going on okay so that's",
"a lot of technical content so what I'm",
"going to do now is switch from technical",
"content to something much more important",
"unless we have some questions first okay",
"because in the next lesson in the next",
"lesson we're going to be looking at",
"generative models both text and image",
"generative models and generative models",
"are where you can create a new piece",
"of text or a new image or a new video or",
"a new sound and as you probably are",
"aware this is the area that deep",
"learning has developed the most in in",
"the last 12 months and we're now at a",
"point where we can generate realistic",
"looking videos images audio and to some",
"extent even text and so there are many",
"things in in this journey which have",
"ethical considerations but perhaps this",
"area of generative modeling is one of",
"the largest ones so before I got into it",
"I wanted to specifically touch on ethics",
"and data science",
"most of the stuff I'm showing you",
"actually comes from Rachel and Rachel",
"has a really cool TEDx San Francisco",
"talk that you can check out on YouTube",
"and a more extensive analysis of ethical",
"principles and bias principles in AI",
"which you can find at this talk here and",
"she has a playlist that you can check",
"out we've already touched on an example",
"of bias which was his gender shades",
"study where if you remember for example",
"lighter male skin people on IBM's main",
"computer vision system 99.7% accurate",
"and darker females are some hundreds of",
"times less accurate in terms of error so",
"like extraordinary differences and so",
"it's interesting to kind of like okay",
"it's it's first more important to be",
"aware that not only can this happen",
"technically that this can happen on a",
"massive companies rolled out publicly",
"available highly marketed system that",
"hundreds of quality control people have",
"studied and lots of people are using it",
"it's it's out there in the world they",
"all look kind of crazy right",
"so it's interesting to think about why",
"and so one of the reasons why is that",
"the data we feed these things but we",
"tend to use be included a lot of these",
"datasets kind of unthinkingly right but",
"like imagenet which is the basis of like",
"a lot of the computer vision stuff we do",
"is over half American and Great Britain",
"right like when it comes to the",
"countries that actually have most of the",
"population in the world I can't even see",
"them here they're somewhere in these",
"impossibly thin lines because remember",
"these datasets are being created almost",
"exclusively by people in u.s. Great",
"Britain and nowadays increasingly also",
"China so there's a lot of bias in the",
"content we're creating because of a bias",
"in the kind of people that are creating",
"that content even when in theory it's",
"being created in a very kind of neutral",
"way but you can't argue with the data",
"right it's it's obviously not neutral at",
"all and so when you have biased data",
"creating biased algorithms you then need",
"to say like or what are we doing with",
"that so we've been spend a lot of time",
"talking about image recognition so a",
"couple of years ago this company deep",
"Lin advertised their image recognition",
"system which can be used to do mass",
"surveillance on large crowds of people",
"find any person passing through who is a",
"person of interest in theory and so",
"putting aside even the question of like",
"is it a good idea to have such a system",
"you kind of think is it a good idea to",
"have such a system where certain kinds",
"of people are 300 times more likely to",
"be misidentified and then thinking about",
"it so this is now starting to happen in",
"America these systems are being rolled",
"out and so there are now systems in",
"America that will identify a person of",
"interest in a video and send a ping to",
"the local police",
"and so these systems are extremely",
"inaccurate and extremely biased and what",
"happens that of course is if you're in a",
"predominantly black neighborhood where",
"the probability of successfully",
"recognizing you is much lower and you're",
"much more likely to be surrounded by",
"black people and so suddenly all of",
"these black people are popping up as",
"persons of interest or in a video of a",
"person of interest all the people in the",
"video are all recognized as in the",
"vicinity as a person of interest you",
"suddenly get all these pings going off",
"the local police department causing the",
"police to run down there and therefore",
"likely to lead to a larger number of",
"arrests which is then likely to feed",
"back into the data being used to develop",
"the systems so this is happening right",
"now",
"and so like thankfully a very small",
"number of people are actually bothering",
"to look into these things I mean",
"ridiculously small but at least it's",
"better than nothing and so for example",
"then one of the best ways that people",
"get publicity is to do kind of funny",
"experiments like let's try the mug shot",
"image recognition system that's being",
"widely used and trade against the",
"members of Congress and find out that",
"there are 28 members of Congress who",
"would have been identified by this",
"system obviously incorrectly oh I didn't",
"know that okay members have black",
"members of Congress not at all surprised",
"to hear that Thank You Rachel we see",
"this kind of bias and a lot of the",
"systems we use I'm not just image",
"recognition but text translation when",
"you convert she as a doctor he is a",
"nurse into Turkish you quite correctly",
"get a gender in specific pronoun because",
"that's what Turkish uses you could then",
"take that and feed it back into Turkish",
"with your gender in specific pronoun and",
"you will now get he as a doctor she is",
"in this so the bias again this is in a",
"massively widely rolled out carefully",
"studied system and it's not like even",
"these kind of things like a little",
"one-off things then get fixed quickly",
"these issues have been identified in",
"Google Translate for a very long time",
"and they're still there and they don't",
"get fixed so the the kind of results of",
"this are in my opinion quite terrifying",
"because what's happening is that in many",
"countries including America where I'm",
"speaking from now algorithms are",
"increasingly being used for all kinds of",
"Public Policy judicial and so forth",
"surfaces for example there's a system",
"called compass which is very widely used",
"to decide who's going to jail and it",
"does that in a couple of ways",
"it tells judges what sentencing",
"guidelines they should use for",
"particular cases and it tells them also",
"which people the system says should be",
"let out on bail but here's the thing",
"white people that keeps on saying let",
"this person out even though they end up",
"reoffending and vice versa it's",
"systematically like out by double",
"compared to what it should be in terms",
"of getting it wrong with white people",
"versus black people so this is like kind",
"of horrifying because I mean amongst",
"other things the data that it's using in",
"this system is literally asking people",
"questions about things like did any of",
"your parents ever go to jail or do any",
"of your friends do drugs like they're",
"asking questions about other people who",
"they have no control over so not only",
"are these systems biased very",
"systematically biased but they're also",
"are being done on the basis of data",
"which is totally out of your control so",
"this is kind of DeJoria it seems that oh",
"yeah are your parents divorced is",
"another question that's being used to",
"decide whether you go to jail or not",
"okay so when we raise these issues kind",
"of on Twitter or in talks or whatever",
"there's always a few people always white",
"men a few people who will always say",
"like",
"that's just the way the world is that's",
"just reflecting what the data shows but",
"when you actually look at it it's not",
"right it's actually systematically",
"erroneous and systematically erroneous",
"against people of color minorities the",
"people who are less involved in creating",
"the systems that these products are",
"based on sometimes this can go a really",
"long way",
"so for example in Myanmar there was a",
"genocide of Thuringia people and that",
"genocide was very heavily created by",
"Facebook not because anybody at Facebook",
"wanted it I mean heavens no I know a lot",
"of people at Facebook I have a lot of",
"friends at Facebook they're really",
"trying to do the right thing right",
"they're really trying to create a",
"product that people like but not in a",
"thought for enough way because when you",
"roll out something we're literally in",
"Myanmar a country that most people",
"didn't have most but maybe half of",
"people didn't have electricity until",
"very recently and you say hey you can",
"all have free internet as long as it's",
"just Facebook I think carefully about",
"what you're doing right and then you use",
"algorithms to feed people the stuff they",
"will click on and of course what people",
"click on is stuff which is controversial",
"stuff that makes their blood boil so",
"when they actually started asking the",
"generals in the Myanmar army that were",
"literally throwing babies onto bonfires",
"they were saying we know that these are",
"not humans we know that they are animals",
"because we read the news we read the",
"internet but and because this is the",
"that this is the stories that the",
"algorithms are pushing but and the",
"algorithms are pushing the stories",
"because the algorithms are good they",
"know how to create eyeballs how to get",
"people watching and how can I get people",
"clicking and again putting it Facebook",
"said let's cause a massive genocide in",
"Myanmar they said let's maximize the",
"engagement of people in this new market",
"on our platform",
"so they very successfully maximized",
"engagement yes please it's just it's",
"important to note people warned",
"executives of Facebook how the platform",
"was being used to incite violence as far",
"back as 2013 2014 2015 and 2015 someone",
"even warned executives that Facebook",
"could be used in Myanmar in the same way",
"that the radio broadcast were used in",
"Rwanda during the Rwandan genocide and",
"as of 2015 Facebook only had four for",
"contractors who spoke Burmese working",
"for them they really did not put many",
"resources into the issue at all even",
"though they were getting very very",
"alarming warnings about it so I mean why",
"does this happen right the part of the",
"issue is that ethics is complicated and",
"you will not find Rachel or I telling",
"you how to do ethics you know how do you",
"fix this we don't know we can just give",
"you kind of things to think about all",
"right another part of a problem we keep",
"hearing is it's not my problem I'm just",
"a researcher I am just a techie I'm just",
"building a data set I'm not part of a",
"problem I'm part of this foundation",
"that's far enough away that I can",
"imagine that I'm not part of this right",
"but you know if you're creating image",
"net and you want it to be successful you",
"want lots of people to use it you want",
"lots of people to build products on it",
"lots people to do research on top of it",
"if you're trying to create something",
"that people are using you want them to",
"use then please try to make it something",
"that won't cause massive amounts of harm",
"and doesn't have massive amounts of bias",
"and it can actually come back and bite",
"you in the ass right",
"the Volkswagen engineer who ended up",
"actually encoding the thing that made",
"them systematically cheat on their",
"diesel emissions tests on their",
"pollution tests ended up in jail not",
"because it was their decision to cheat",
"on the tests but because their manager",
"told them to write their code",
"and they wrote the code and therefore",
"they were at the ones that ended up",
"being criminally responsible and they",
"were the ones that were jailed right so",
"if you do in some way a shitty thing",
"that ends up causing trouble that can",
"absolutely come back around and get you",
"in trouble as well sometimes it can",
"cause huge amounts of trouble",
"so if we go back to World War two right",
"then this was one of the first great",
"opportunities for IBM to show off their",
"amazing amazing tabulating system and",
"they had a huge client in Nazi Germany",
"and Nazi Germany used this amazing new",
"tabulating system to encode all of the",
"different types of Jews that they had in",
"the country and all the different types",
"of problem people so Jews were eight",
"gypsies were 12",
"then different outcomes were coded",
"executions were for death in a gas",
"chamber was six a Swiss judge ruled that",
"IBM was actively involved facilitating",
"the commission of these crimes against",
"humanity right so there are absolutely",
"plenty of examples of people building",
"data processing technology that are",
"directly causing deaths sometimes",
"millions of deaths right so we don't",
"want to be one of those people and so",
"you might have thought oh you know I'm",
"just creating some data processing",
"software and somebody else is thinking",
"I'm just the sales person and somebody",
"else is thinking",
"I'm just the biz dev person opening new",
"markets but it all comes together right",
"so we need to care and so one of the",
"things we need to care about is getting",
"humans back in the loop right and so",
"when we pull humans out of the loop is",
"one of the first times that trouble",
"happens I don't know if you remember I",
"remember this very clearly when I first",
"heard that Facebook was firing the human",
"editors that were responsible for",
"basically curating the news that ended",
"up on the Facebook pages and I got to",
"say",
"a time I thought that's a recipe for",
"disaster because I've seen again and",
"again that humans can be the person in",
"the loop that can realize this isn't",
"right you know it's very hard to create",
"an algorithm that can recognize this",
"isn't right or else humans are very good",
"at that",
"and we saw that's what happened right",
"after Facebook fired two human editors",
"the nature of stories on Facebook",
"dramatically changed that and you",
"started seeing this proliferation of",
"conspiracy theories and the kind of the",
"algorithms went crazy with recommending",
"more and more controversial topics and",
"of course that changed people's",
"consumption behavior causing them to one",
"more and more controversial topics so",
"we're one of the really interesting",
"places this comes in and Cathy O'Neil",
"who's got a great book called reference",
"of math destruction thank you Rachel",
"and many others have pointed out is that",
"what happens to algorithms is that they",
"end up impacting people for example",
"compass sentencing guidelines go to a",
"judge now you can say the algorithm is",
"very good we I mean it in compass this",
"case it isn't it actually turned out to",
"be about as bad as random because it's a",
"black box and all that but even if it",
"was very good",
"you could then say well you know the",
"judge is getting the algorithm otherwise",
"they're just be getting a person people",
"also give bad advice",
"so what humans respond differently to",
"algorithms it's very common particularly",
"for a human that is not very familiar",
"with the technology themselves like a",
"judge just see like oh that's what the",
"computer says the computer looked it up",
"and it figured this out right it's",
"extremely difficult to get a",
"non-technical audience to look at a",
"computer recommendation and come up with",
"a nuanced decision-making process so",
"what we see is that algorithms are often",
"put into place with no appeals process",
"they're often used to massively scale up",
"decision making systems because they're",
"cheap",
"and then the people that are using the",
"Atlas of those algorithms tend to give",
"them more credence than they deserve",
"because very often they're being used by",
"people that don't have the technical",
"competence to judge them themselves so",
"great example right was here's an",
"example of somebody who lost their",
"health care and they lost their health",
"care because of an error in a new",
"algorithm that was systematically",
"failing to recognize that there are many",
"people that need help with was it",
"Alzheimer's cerebral palsy and diabetes",
"thanks Rachel",
"and so this system which had this this",
"era that was later discovered was",
"cutting off these people from the home",
"care that they needed so that cerebal",
"palsey victims loan longer had the care",
"they needed so their life was destroyed",
"basically and so when the person that",
"created that algorithm with the error",
"was asked about this and one",
"specifically said should they have found",
"a better way to communicate the system",
"the strengths the failures and so forth",
"he said yeah I should probably also dust",
"under my bed that was there that was the",
"level of interest they had and this is",
"extremely common I hear this all the",
"time and it's much easier to kind of see",
"it from afar and say okay after the",
"problems happened I can see that that's",
"a really shitty thing to say but it can",
"be very difficult when you're kind of in",
"the middle of it I just want to say one",
"more thing about that example and that's",
"that this was a case where it was",
"separate there was someone who created",
"the algorithm then I think different",
"people implemented the software and this",
"is a note in use in over half of the 50",
"states and then there was also the",
"particular policy decisions made by that",
"state and so there this is one of those",
"situations where nobody felt responsible",
"because the algorithm creators like oh",
"no it's the policy decisions of the",
"state that were bad you know and the",
"state can be like oh no it's the ones",
"who implemented the software and so",
"everyone's just kind of pointing fingers",
"and not taking responsibility and you",
"know in some ways maybe it's unfair but",
"I would argue the person who is",
"creating the data set and the person who",
"is implementing the algorithm is the",
"person best placed to get out there and",
"say hey here are the things you need to",
"be careful of and make sure that they",
"are part of the implementation process",
"so we've also seen this with YouTube",
"right it's kind of similar to what",
"happened with Facebook and we're now",
"seeing with heard examples of students",
"watching the faster I courses who say",
"hey Jeremy and Rachel watching the first",
"day our courses really enjoyed them and",
"at the end of one of them the YouTube",
"autoplay fed me across to a conspiracy",
"theory and what happens is that once the",
"system decides that you like the",
"conspiracy theories it's going to just",
"feed you more and more and then what",
"happens is that please come on just",
"briefly you don't you don't even have to",
"like conspiracy theories the goal is to",
"get as many people hooked on conspiracy",
"theories as possible as what the",
"algorithms trying to do kind of whether",
"or not you've expressed interest right",
"and so the interesting thing again is I",
"know plenty of people involved in",
"YouTube's recommendation systems none of",
"them are wanting to promote conspiracy",
"theories but people click on them right",
"and people share them and what tends to",
"happen is also people that are into",
"conspiracy theories consume a lot more",
"YouTube media so it actually is very",
"good at finding a market that watches a",
"lot of hours of YouTube and then it",
"makes that market watch even more so",
"this is an example of a feedback loop",
"and the New York Times as net is now",
"describing YouTube is perhaps the most",
"powerful radicalizing instrument of the",
"21st century I can tell you my friends",
"that worked on the YouTube",
"recommendation system did not think they",
"were creating the most powerful",
"radicalizing instrument of the 21st",
"century and to be honest most of them",
"today when I talk to them still think",
"they're not they think it's all",
"you know not all of them but a lot of",
"them now are at the point where they",
"just feel like they're the victims here",
"people are unfairly you know they don't",
"get it they don't understand what we're",
"trying to do it's very very difficult",
"right out there in the heart of it so",
"you've got to be thinking from rad at",
"the start what are the possible",
"unintended consequences of what you're",
"working on and as the technical people",
"involved how can you get out in front",
"and make sure that people are aware of",
"them and I just also need to say that in",
"particular many of these conspiracy",
"theories are promoting white supremacy",
"they're you know kind of far-right after",
"no nationalism anti-science and i think",
"you know maybe five or ten years ago I",
"would have thought conspiracy theories",
"are more a more fringe thing but we're",
"seeing the kind of huge societal impact",
"it can have for many people to believe",
"these know and you know partly it's you",
"see them on YouTube all the time it",
"starts to feel a lot more normal right",
"so one of the things that people are",
"doing to try to say like how to fix this",
"problem is to explicitly get involved in",
"talking to the people who might or will",
"be impacted by the kind of decision",
"making processes that you're enabling so",
"for example there was a really cool",
"thing recently where literally",
"statisticians and data scientists got",
"together with people who had been inside",
"the criminal system ie had gone through",
"the the bail and sentencing process of",
"criminals themselves and talking to the",
"lawyers who worked with them and put",
"them together with the data scientists",
"and actually kind of put together a",
"timeline of how exactly does it work and",
"where exactly the other places that",
"there are inputs and how do people",
"respond to them and who's involved this",
"is really cool right this is the only",
"way for you as a kind of a data product",
"developer to actually know how your data",
"products going to be working a really",
"great example of a somebody who did a",
"great job here was Evan s dollar at",
"Meetup who said hey a lot of men are",
"going to our tech meetups and if we use",
"a recommendation system naively it's",
"going to recommend more tech meetups to",
"man which is going to cause more men to",
"go to them and then when women do try to",
"go they'll be like oh my god there's so",
"many men here we're just going to cause",
"more men to go to the tech meetups yeah",
"yeah so showing recommendations to men",
"and therefore not showing them to women",
"yes yeah",
"so so what Evan and made-up decided was",
"to make an explicit product decision",
"that this would not even be representing",
"the actual true preferences of people it",
"would be creating a runaway feedback",
"loop so let's explicitly stop it right",
"before it happens and and not recommend",
"less made ups to women and tech meetups",
"women and more tech meetups",
"come in and so I think that's that's",
"just it's really cool it's like it's",
"saying we don't have to be slaves to the",
"algorithm we actually get to decide",
"another thing that people can do to help",
"is regulation and normally when we kind",
"of talk about regulation there's a",
"natural reaction of like how do you",
"regulate these things that's ridiculous",
"you can't regulate AI but actually when",
"you look at it again and again and this",
"fantastic paper core data sheets for",
"data sets has lots of examples of this",
"there are many many examples of",
"industries where people thought they",
"couldn't be regulated people thought",
"that's just how it was like cars people",
"died in cars all the time because they",
"literally had sharp metal knobs on",
"dashboards steering columns weren't",
"collapsible and all of the discussion in",
"the community was that's just how cars",
"are and when people died in cars it's",
"because of the people but then",
"eventually the regulations did come in",
"and today driving is dramatically safer",
"like dozens and dozens of times safer",
"than it was before",
"right so often there are things we can",
"do through policy so to summarize we are",
"part of the point three to 0.5% of the",
"world that knows how to code all right",
"we have a school that very few other",
"people do not only that we now know how",
"to code deep learning algorithms which",
"is like the most powerful kind of code I",
"know so I'm hoping that we can",
"explicitly think about like at least not",
"making the world worse and perhaps",
"explicitly making it better right and so",
"why is this interesting to you as an",
"audience in particular and that's",
"because fast AI in particular is trying",
"to make it easy for domain experts to use",
"deep learning and so this picture of the",
"goats here is an example of one of our",
"international fellows from a previous",
"course who is a goat dairy farmer and",
"told us that they were going to use deep",
"learning on their remote Canadian Island",
"to help study other disease in goats",
"that and to me this is a great example",
"of like a domain experts problem which",
"nobody else even knows about let alone",
"know that as a computer vision problem",
"that can be solved with deep learning so",
"in your field whatever it is you",
"probably know a lot more now about the",
"opportunities in your field to make it a",
"hell of a lot better than it was before",
"you're probably to come up with all",
"kinds of cool product ideas right maybe",
"be able to startup or create a new",
"product group in your company or",
"whatever but also let us be thinking",
"about what that's going to mean in",
"practice and think about where can you",
"put humans in the loop right where can",
"you put those pressure release valves",
"who are the people you can talk to who",
"could be impacted who could help you",
"understand right and get the kind of",
"humanities folks involved to understand",
"history and psychology and sociology and",
"so forth so that's our plea to you if",
"you've got this far you're definitely at",
"a point now where you're ready to you",
"know make a serious impact on the world",
"so I hope we can make sure that that's a",
"positive impact see you next week"
}